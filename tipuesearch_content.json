{"pages":[{"url":"https://blog.john-pfeiffer.com/john-likes/","text":"https://github.com/getpelican/pelican http://oncrashreboot.com/elegant-best-pelican-theme-features https://github.com/getpelican/pelican-plugins/tree/master/tipue_search https://github.com/getpelican/pelican-plugins/tree/master/sitemap https://github.com/getpelican/pelican-plugins/tree/master/post_stats http://listjs.com https://play.golang.org/ The Go Programming Language by Alan Donovan and Brian Kernighan Building Microservices by Sam Newman Release It!: Design and Deploy Production-Ready Software by Michael Nygard Clean Code by Robert C. \"Uncle Bob\" Martin The Pragmatic Programmer: From Journeyman to Master by Andy Hunt and Dave Thomas Foundations of Programming - Building Better Software by Karl Seguin The C Programming Language (2nd Edition) by Brian Kernighan and Dennis Ritchie Effective Java Programming Language Guide by Joshua Bloch Racing the Beam by Nick Montfort and Ian Bogost Steve Jobs by Walter Isaacson Founders at Work: Stories of Startups' Early Days by Jessica Livingston Peopleware: Productive Projects and Teams by Tom DeMarco, Timothy Lister The Mythical Man-Month: Essays on Software Engineering by Frederick P. Brooks Jr. Design Patterns: Elements of Reusable Object-Oriented Software by Erich Gamma, Ralph Johnson, John Vlissides, Richard Helm Applied Cryptography by Bruce Schneier The Algorithm Design Manual, 2nd Edition by Steven Skiena Testing Computer Software, 2nd Edition by Cem Kaner, Jack Falk, Hung Q. Nguyen http://www.essrl.wustl.edu/~jao/itrg/shannon.pdf (Claude Shannon on Communication) http://research.microsoft.com/en-us/um/people/lamport/pubs/time-clocks.pdf (Leslie Lamport on Time) http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf (Leslie Lamport on Paxos) http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf (Leslie Lamport on Paxos Simplified) http://queue.acm.org/detail.cfm?id=1142065 (Werner Vogels about AWS) http://www.se-radio.net/2010/03/episode-157-hadoop-with-philip-zeyliger http://www.se-radio.net/2010/09/episode-167-the-history-of-junit-and-the-future-of-testing-with-kent-beck http://www.se-radio.net/2010/11/episode-169-memory-grid-architecture-with-nati-shalom http://www.se-radio.net/2014/11/episode-215-gang-of-four-20-years-later/ http://www.se-radio.net/2015/02/episode-221-jez-humble-on-continuous-delivery/ http://www.se-radio.net/2015/04/episode-224-sven-johann-and-eberhard-wolff-on-technical-debt/ http://www.se-radio.net/2015/05/se-radio-episode-226-eric-evans-on-domain-driven-design-at-10-years/ http://www.se-radio.net/2015/05/the-cap-theorem-then-and-now/ http://www.se-radio.net/2015/11/se-radio-episode-241-kyle-kingsbury-on-consensus-in-distributed-systems/ http://www.se-radio.net/2016/01/se-radio-show-246-john-wilkes-on-borg-and-kubernetes/ http://thisweekinstartups.com/thisweekin-startups/advice-biggest-mistakes-founders-startups-jerry-colonna http://thisweekinstartups.com/thisweekin-startups/steve-jurvetson-dfj-tesla-spacex Follow with an atom feed link: https://blog.john-pfeiffer.com/feeds/john-pfeiffer.atom.xml","tags":"pages","title":"John Likes"},{"url":"https://blog.john-pfeiffer.com/alpine-linux-introduction-tutorial/","text":"Alpine Linux is a minimalist secure linux distro. In security terms less \"footprint\" often means less vectors of attack and less complexity to analyze for vulnerabilities Alpine Linux is becoming a preferred base OS for many foundational official Docker Images (python, php, ruby, nginx, redis, haproxy,) since downloading Docker Images (aka Deploying Docker Containers) can saturate the network at scale. https://hub.docker.com/_/alpine/ http://www.pcworld.com/article/3031765/is-docker-ditching-ubuntu-linux-confusion-reigns.html https://news.ycombinator.com/item?id=10998667 https://en.wikipedia.org/wiki/Alpine_Linux Basics Most of the very basic commands are similar to other linux distros like Debian/Ubuntu/Redhat, but of course there are differences ;) /bin/sh busybox cat /etc/passwd curl 127.0.0.1 ls -l /usr/bin /usr/sbin | more more busybox Package Management apk --help apk update apk search iptables apk add iptables http://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management Networking hostname cat /etc/resolv.conf ifconfig netstat -anp apk add iptables traceroute Compiling prepare musl compiler on alpine linux WARNING: below did not work docker pull alpine docker run -it --rm alpine /bin/sh apk update apk add musl-dev wget tar gzip gcc make wget http://www.musl-libc.org/releases/musl-1.1.15.tar.gz tar xf musl-1.1.15.tar.gz cd musl-1.1.15 ./configure make make install Now we've installed the musl compiler? /usr/local/musl seemed terribly empty of binaries http://dominik.honnef.co/posts/2015/06/go-musl/ hi.c #include <stdio.h> int main () { printf ( \"hi\" ); } compiling apk add file gcc -static hi.c file a.out chmod +x a.out ./a.out hi The default is a.out: ELF 64-bit LSB executable, x86-64, version 1 (SYSV), statically linked, not stripped Note that this was just gcc not musl-gcc :( http://www.musl-libc.org/how.html docker run -it --rm alpine /bin/sh apk update apk add alpine-sdk But maybe because it's already an alpine linux container the gcc already uses musl instead of libc and does not need the musl-gcc wrapper? compile go on alpine linux Just use the golang image based on alpine linux ;) https://github.com/docker-library/docs/tree/master/golang docker pull golang:alpine docker run -it --rm golang:alpine /bin/sh Now that we've got the container running we can use the shell vi intro.go package main import \"fmt\" func main () { fmt . Println ( \"hi\" ) } go run intro . go hi ls -l go build intro.go ls -l ./intro hi","tags":"linux","title":"Alpine Linux Introduction Tutorial"},{"url":"https://blog.john-pfeiffer.com/debian-packages-for-deployment/","text":"What is a Debian Package? A debian package is a way to distribute and install a collection of files (aka software) onto a system (i.e. Debian or Ubuntu). While a piece of software might depend on other debian packages (e.g. libraries) usually a single .deb file represents some sort of module that serves a single purpose. Once a debian package is built any client (dpkg or apt which also uses dpkg ;) can use it to install the software. Why use a Debian Package? When you're developing on your own box you can pretty much get away with anything A complex and large scale production environment typically has a lot of costs (both operationally and in not becoming a bottleneck to dev velocity). Any opportunity to increase determinism and reduce risk is welcome. Deploying source code directly from git version control doesn't always scale well (streaming tons of small files, dedicated read only service user direct to production, etc.) nor does it create enough determinism with dependency management. As deployments become more frequent and the Continuous Integration becomes more complex it is really important to embrace the \"build once\" principle so that a single artifact (hopefully with all of its dependencies) can pass through the guantlet of integration testing and canary/incremental rollout. So now that you're convinced \"Artifacts\" are the way to go lets just skip .exe, .msi, .jar, .etc and go straight to... The Debian Package is a \"battle tested\" format with lots of features (dependency requirements, preinst scripts, postinst scripts, etc.) but if there is a bug in a specific .deb file it is not always practical to get the full source code and rebuild the whole thing (especially considering static bindings and specific compilation environment/parameters). One example people give is an erroneous pre install or post install script that is preventing either installation or removal. The example below is more on just simply changing the control file \"Description:\" How to unpack a debian package, modify the control file, and repack it To unpack, modify, and repack a debian package: wget https://example.com/example.deb --output-document /tmp/example.deb docker run --rm --it --volume /tmp:/tmp ubuntu:14.04 /bin/bash apt-get update apt-get install vim cd /tmp mkdir emptydir dpkg-deb -R example.deb /tmp/emptydir ls -ahl /tmp/emptydir ls -ahl /tmp/emptydir/DEBIAN vi /tmp/emptydir/DEBIAN/control dpkg-deb -b emptydir /tmp/example-fixed.deb On your host /tmp should now contain example.deb and example-fixed.deb more info https://www.debian.org/doc/manuals/debian-faq/ch-pkg_basics.en.html http://unix.stackexchange.com/questions/138188/easily-unpack-deb-edit-postinst-and-repack-deb http://manpages.ubuntu.com/manpages/hardy/man1/dpkg-deb.1.html","tags":"build-CI-CD-devops","title":"Debian Packages for Deployment"},{"url":"https://blog.john-pfeiffer.com/python-packaging-pip-wheels/","text":"Installing packages with python Pip is the standard way to install python packages sudo pip install packagename searches https://pypi.python.org and finds the latest version of the package, full docs https://pip.pypa.io/en/latest/ If you get the package name wrong you will have installed something completely different. sudo pip search packagename find packages similar to the name you provided (from pypi or whatever provider you are using) https://pip.pypa.io/en/stable/reference/pip_search/ sudo pip freeze displays what is installed Virtualenv for sanity and isolation A common mistake is to use the global pip installation of the Operating System to store all of the installed dependencies. As soon as you have conflicting version requirements this breaks. As soon as you have multiple applications/services installing globally it becomes unmanageable. Virtualenv creates a virtual environment (basically injecting a PATH into the environment) for python binaries and package installation. Pinning Versions and Guaranteed Sourcing One common mistake is to not pin version numbers and depend on https://pypi.python.org Without pinning version numbers for your dependencies (listed one per line in requirements.txt) you will receive a nasty surprise when the maintainers make a breaking change and you get a newer version unexpectedly. Since python is a dynamic language you may receive the worst kind of surprise in production (hopefully nothing as bad as data corruption or security issues). When you do pin the version number BUT still depend on https://pypi.python.org to provide the file then the project maintainers may remove the version you are pinned to (causing your builds to fail - though some for some cowboys this will cause production deployments to fail). Pin the version of your dependencies, provide the dependencies locally or through a system under your control (your own pypi server or s3 bucket) , explicit is better than implicit. Wheels are better Python Packaging Wheels are (awesome), it's the beginning of trying to make python installations more deterministic and pip less dynamic at install time. http://wheel.readthedocs.org/en/latest/ An output directory of the wheels of a project are known by convention as a \"wheelhouse\". As a side effect the wheel directory, \"/tmp/wheelhouse\" in the example, contains installable copies of the exact versions of your application's dependencies. By installing from those cached wheels you can recreate that environment quickly and with no surprises. When you install using pip it looks for a \"wheel file\" (*.whl which is the newer zip compressed format, goodbye .egg) of the correct name for your (virtual) environment (e.g. py2 or py3 or x86 linux). This wheel file saves time and bugs from installing a package/.egg from source (usually that time is spent compiling C code for the python library). sudo pip install wheel; cd projectsource; python setup.py bdist_wheel; ls -l ./dist pip wheel --find-links /root/wheelhouse --wheel-dir=/root/wheelhouse -r requirements.txt https://pip.pypa.io/en/stable/reference/pip_wheel/ http://pip-python3.readthedocs.org/en/latest/reference/pip_wheel.html#build-system-interface installing using a wheel file pip install somepackage-version-py2.py3.whl pip freeze error: invalid command 'bdist_wheel' https://pypi.python.org/pypi/docutils#downloads only provided a py3 wheel (facepalm) Downloading the source .tar.gz and running python setup.py bdist_wheel resulted in: error: invalid command 'bdist_wheel' Reading the internet provided no comprehensible answers (lots of \"setuptools does not match your version of pip or wheel or whatever\") The following hacking seems to have provided a solution: python --version pip --version cd /tmp wget https://pypi.python.org/packages/source/d/docutils/docutils-0.12.tar.gz#md5=4622263b62c5c771c03502afa3157768 tar xf docutils-0.12.tar.gz cd docutils-0.12 virtualenv venv source venv/bin/activate python --version pip --version pip install wheel pip freeze python setup.py install pip freeze pip wheel . ls /tmp/docutils-0.12/wheelhouse docutils-0.12-py2-none-any.whl","tags":"build-CI-CD-devops","title":"Python packaging pip wheels"},{"url":"https://blog.john-pfeiffer.com/static-site-with-bitbucket-and-shippable-and-pelican/","text":"Running Software Costs Money One of the most overlooked costs in running a service is operations. While Research and Development (aka coding) is often cited as the largest expense (software developer salaries! https://www.quora.com/What-are-the-average-operating-costs-of-SaaS-companies ), and 80% (or more) of (successful) software's life is maintenance ( http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3610582/ ), you have to run the darn thing all the time. A Free and Efficient Static Web Site What is one cost effective (free!) and efficient solution to running a static site? Bitbucket also has a free static site capability (as long as the DNS is USERNAME.bitbucket.org), so no server/hosting required Bitbucket have free private repositories The Bitbucket static site repository can be private (only the html exposed will be visible to anonymous users) Shippable have a free plan with 1 container that will do your builds (fine by me, Docker is fast!) Pelican converts markdown into .html and you can still use javascript for fancy things http://docs.getpelican.com/ The basic process is to be triggered by a git push to the private repository of new/updated source markdown, use pelican to process it into .html, and then publish (git push) the new/updated .html to the static site repository. One reason to use two seperate repositories instead of only one repository is that if you make a commit to your markdown source repository that will trigger a CI run which will push the updated .html files to the repository which would be detected and maybe trigger an infinite loop. Or at the least interleave your source code changes with generated output changes in the commit logs. =] An alternative is using multiple branches but you'd better hope nobody ever deletes your source branch by accident. Another alternative is to include an IF statement in your shippable code to not push if the diff/md5 of the source files (or maybe check against the output .html?) still match. I say keep it super simple ;) Bitbucket setup Create The Source and Target Repositories Create a new private repository (for your markdown), consider prefixing the name with source or something (good names makes for good maintenance) Make sure you have cloned the pelican project and setup a basic static site: https://blog.john-pfeiffer.com/how-to-set-up-a-pelican-static-blog-site/ Inside your .gitignore you will probably want to exclude .pyc and ./output and any other pelican created artifacts. Inside your repository at the root level you will need a shippable.yaml file: language : python python : - \"2.7\" install : - pip install pelican Markdown beautifulsoup4 script : - rm - rf ./ output - rm - rf ./ cache - rm - rf ./ plugins /* - mv ./ pelican - project /* . - ls - ahl ./ content - pelican ./ content - o ./ output - s ./ publishconf . py after_script : - ls - ahl - ls - ahl ./ output This assumes that the pelican-project is a subdirectory in the repository using the best practice of leaving the top level of a repository for build and test artifacts and isolating the source code into a subdirectory. Create another private repository for your public html. It MUST be named USERNAME.bitbucket.org to make use of the bitbucket static site capabilities. (yes, the name must include those dots/domain name of the service) https://confluence.atlassian.com/bitbucket/publishing-a-website-on-bitbucket-cloud-221449776.html Get OAuth Access Generate a Consumer OAuth2 Token with https://confluence.atlassian.com/bitbucket/oauth-on-bitbucket-cloud-238027431.html#OAuthonBitbucketCloud-Createaconsumer Use curl to verify your token (this is how Shippable will get a 1 hour expiring access token to work on the target output repository) curl https://bitbucket.org/site/oauth2/access_token -d grant_type=client_credentials -u yourkeyhere:yoursecrethere http://stackoverflow.com/questions/24965307/how-to-manipulate-bitbucket-repository-with-token Shippable setup Enable the integration with Bitbucket: http://docs.shippable.com/#step-0-prerequisite Right from the beginning Shippable tries to ask which source repository provider (either GitHub or Bitbucket) you will be using. Use Shippable's OAuth implementation (Account Integration) to pick which Bitbucket repository Home -> CI (dropdown) -> Bitbucket (hopefully you have a different avatar between Bitbucket and GitHub) Press \"Sync\" if you have a newly created repository that is not listed yet Add the Bitbucket OAuth2 Key and Secret as a Shippable secure environment variable in the format KEY:SECRET Go to https://app.shippable.com/projects/1234d2ea1895ca4474661234/settings and look for the Encrypt section Fill it in with something like OAUTH_USER=yourkeyhere:yoursecrethere http://shippable-docs-20.readthedocs.org/en/latest/config.html#secure-environment-variables Copy the output to your shippable.yml file Putting it all together Update the source repository top level shippable.yaml file: language : python python : - \"2.7\" install : - pip install pelican Markdown beautifulsoup4 env : - secure : yourencryptedkeyandsecret == script : - rm - f token . json - rm - rf ./ output - rm - rf ./ cache - rm - rf ./ plugins /* - mv ./ pelican - project /* . - ls - ahl - ls - ahl ./ content - pelican ./ content - o ./ output - s ./ publishconf . py - ls - ahl ./ output - curl https : // bitbucket . org / site / oauth2 / access_token - d grant_type = client_credentials - u $ OAUTH_USER >> token . json - BBTOKEN = $ ( cat token . json | python - c 'import sys, json; print json.load(sys.stdin)[\"access_token\"]' ) - git clone \"https://x-token-auth:$BBTOKEN==@bitbucket.org/USERNAME/USERNAME.bitbucket.org\" - ls - ahl USERNAME . bitbucket . org / - rm - rf USERNAME . bitbucket . org /* - cd USERNAME . bitbucket . org - mv ../ output /* . - ls - ahl . - git config user . email \"me@example.com\" - git config user . name \"John Pfeiffer\" - git add - f . - git commit - m \"build $BUILD_NUMBER commit $COMMIT\" - git push - fq origin master > / dev / null - rm - f ../ token . json after_script : - ls - ahl Adding the bitbucket oauth2 consumer key and secret (separated by a colon) as an encrypted environment variable using curl to generate a temporary access_token and extracting it into a local environment variable cloning with the access)token and removing the previous contents and replacing them with the newly generated output leveraging the CI variables to indicate on the output html repository what markdown source commits triggered this build Reviewing the output The full output of the run is available at something like https://app.shippable.com/builds/1234dec1d00e020c0011234 This is really helpful for debugging (especially seeing how many seconds each step took) Possible improvements: use a python application best practice of documenting dependencies with a requirements.txt file at the top level putting all of the commands into a script like publish-in-ci.sh so that it could be run locally in a dev environment add the Dockerfile used for local development into the source repository to consolidate and simplify development in one place Misc One thing that is interesting about this is that using OAuth tokens through a service is merely wrapping all of the manual steps I have in a previous blog post into a nice SaaS wrapper =) https://blog.john-pfeiffer.com/publish-a-pelican-blog-using-a-bitbucket-post-webhook/","tags":"build-CI-CD-devops","title":"Static site with Bitbucket and Shippable and Pelican"},{"url":"https://blog.john-pfeiffer.com/go-programming-intro-with-vs-code-and-arrays-slices-functions-and-testing/","text":"Introducing the Go Programming Language (aka golang) basics, interactive sandbox with http://play.golang.org Prerequisites Tooling is often essential to productivity. Download and install the go language compiler and tools vim and Docker Or alternatively just avoid the IDE and Environment and use vim and a docker container ;) https://hub.docker.com/_/golang/ Installing and the Environment https://golang.org/dl/ and cd /opt; tar xf go.tar.gz Instead of the ephemeral export PATH=$PATH:/usr/local/go/bin I prefer the persistent ~/.profile (or for all users /etc/profile though clearly /opt indicates a single user system ;) # Go Programming export GOROOT=/opt/go export PATH= $ PATH : $ GOROOT /bin export GOPATH=/opt/goprojects export PATH= $ PATH : $ GOPATH /bin source ~/.profile The last step gets you going without needing to reload your shell, full docs at https://golang.org/doc/install WARNING be careful how you name your executables as the $GOPATH/bin will contain the names of the projects as binaries (so don't create a project or binary named bash!) Download and install an IDE If you are used to larger projects then an IDE is quite helpful for colorization, auto completion, (right click or f12) goto definition, rename, build on save, auto formatting, etc. Suprisingly one of the most popular and effective golang IDE combinations is: https://code.visualstudio.com/Docs/?dv=linux64_deb dpkg -i vscode-amd64.deb To install https://marketplace.visualstudio.com/items?itemName=lukehoban.Go aka https://github.com/Microsoft/vscode-go you actually: open Visual Studio Code Control + P (Launches VS Code Quick Open) ext install Go (capital G for \"lukehoban\") restart when prompted File -> Preferences -> Color Theme (Light Visual Studio ;) At this point you probably need to reboot to get VSCode to recognize the GOPATH correctly In your shell where you setup the GOPATH run the following to get all of the analysis tools that \"Go for Visual Studio Code\" uses, though it's a lot easier to just use the IDE in the bottom right corner \"Analysis Tools Missing\" To take advantage of those tools (like gofmt on save), in your workspace (GOPATH) there will be a .vscode directory with settings.json https://code.visualstudio.com/docs/customization/userandworkspace // Place your settings in this file to overwrite default and user settings. { \"go.buildOnSave\": true, \"go.lintOnSave\": true, \"go.vetOnSave\": true, \"go.buildFlags\": [], \"go.lintFlags\": [], \"go.vetFlags\": [], \"go.coverOnSave\": false, \"go.useCodeSnippetsOnFunctionSuggest\": false, \"go.formatOnSave\": true, \"go.formatTool\": \"goreturns\" } Just in case restart VSCode to recognize the updated settings Unfortunately it is not quite simple to execute the code directly in VSCode https://github.com/Microsoft/vscode-go/issues/21 Install or Build or Run Because Go is a static language there is a compilation (and linking) phase where the source code is transformed into a binary. Manual CLI compilation and installation and execution The traditional command line method is: cd $ GOPATH /path-to-your-project/PROJECTNAME go install $ GOPATH /bin/PROJECTNAME https://golang.org/doc/code.html#Command http://dave.cheney.net/2014/01/21/using-go-test-build-and-install A concrete example: Compile with: cd /opt/goprojects/src/github.com/johnpfeiffer/intro/ ; go install Execute with: /opt/goprojects/bin/intro Debugging with Delve go get github.com/derekparker/delve/cmd/dlv cd /opt/goprojects/src go install /derekparker/delve/cmd/dlv The first time I attempted to do it manually: ls -ahl /opt/goprojects/bin/ cd /opt/goprojects/src/github.com/johnpfeiffer/YOURPROJECT /opt/goprojects/bin/dlv debug --headless --listen=:2345 --log Now in the VSCode IDE open the project folder and create your helloworld.go source file and Control + S to save (and auto gofmt) and then press F5 and it will connect to the Delve Debugger and display the output Delve Debugging and running your application with F5 is automatic once installed correctly The first time you run \"Continue\" with F5 on a file it will prompt you to setup your launch.json (and the IDE will open the default template for you) Use the IDE to go back to your source .go file and press F5 again, this time since the .vscode subdirectory was created and the default delve launch.json file was created, it will just start in debug mode with the Debug Console output at the bottom Coding and Compiling For VSCode IDE keyboard shortcuts: https://code.visualstudio.com/docs/customization/keybindings Comments Types Strings Slices For Loops The main package it where the execution begins (aka \"main\" in c https://en.wikipedia.org/wiki/Entry_point ) Comments are either single line with double slashes or block comments https://golang.org/doc/effective_go.html#commentary intro.go package main import \"fmt\" /* https://golang.org/doc/effective_go.html#mixed-caps https://golang.org/ref/spec#Constants */ const alphabetMax int = 26 func main () { // while j := 0 for j < 4 { fmt . Println ( j ) j += 2 } for i := 0 ; i < 4 ; i += 2 { fmt . Println ( i ) } for { fmt . Println ( \"break or return exits an infinite loop\" ) break } // arrays are contiguous memory, fixed size and type // initialized to capacity 5 with values inserted, alternatively just initialized to empty with: var a [5]string a := [ 5 ] string { \"a\" , \"b\" , \"c\" , \"d\" , \"e\" } // prefer Slices which are Reference Objects that wrap the underlying arrays // https://blog.golang.org/go-slices-usage-and-internals s := [] string for i := 0 ; i < len ( a ); i ++ { fmt . Println ( i , a [ i ]) } // cleaner way of iterating over key and value for k , v := range a { fmt . Println ( k , v ) } fmt . Println ( s ) // [] s = a [:] fmt . Println ( s ) // [a b c d e] s = a [ 2 :] fmt . Println ( s ) // [c d e] } arrays are contiguous memory and 4 bytes is normal // int is usually the 4 byte int32 https://golang.org/ref/spec#Numeric_types b := [ 2 ] int { 1 , 2 } // dereference the addresses that are holding the values 1 and 2 fmt . Printf ( \"%d %d\\n\" , & b [ 0 ], & b [ 1 ]) // rune is also int32 c := [ 2 ] rune { 'a' , 'ä' } fmt . Printf ( \"%d %d\\n\" , & c [ 0 ], & c [ 1 ]) fizzbuzz and switch https://golang.org/ref/spec#Switch_statements for i := 1 ; i < 16 ; i ++ { // usually static case values and \"switch i {\" , note it will NOT fall through by default switch { case i % 3 == 0 && i % 5 == 0 : fmt . Println ( \"fizzbuzz\" ) case i % 3 == 0 : fmt . Println ( \"fizz\" ) case i % 5 == 0 : fmt . Println ( \"buzz\" ) default : fmt . Println ( i ) } } time import \"time\" // https://golang.org/pkg/time/#Now now := time . Now () fmt . Println ( \"local:\" , now ) fmt . Println ( now . UnixNano () / 1000000 , \"ms\" ) fmt . Println ( \"in UTC:\" , now . UTC (). Format ( time . UnixDate )) Packages and String Reverse When you modularize your code into packages then multiple programs can make use of DRY https://en.wikipedia.org/wiki/Don%27t_repeat_yourself main package main import ( \"fmt\" \"github.com/johnpfeiffer/mystringutil\" ) func main () { fmt . Printf ( stringutil . Reverse ( \"!oG ,olleH\" )) } package mystringutil with Reverse // Package mystringutil contains utility functions for working with strings. \"go build\" package mystringutil // Reverse returns its argument string reversed rune-wise left to right. func Reverse ( s string ) string { r := [] rune ( s ) for i , j := 0 , len ( r ) - 1 ; i < len ( r ) / 2 ; i , j = i + 1 , j - 1 { r [ i ], r [ j ] = r [ j ], r [ i ] } return string ( r ) } palindrome and string conversion integer to ascii Besides the main function for executing you will obviously create re-usable packages which will contain functions. Here is the source code for a simple \"is this string a palindrome\" and \"is this integer a palindrome\" programs: package main import ( \"fmt\" \"strconv\" ) func main () { fmt . Println ( isPalindrome ( \"a\" )) fmt . Println ( isPalindrome ( \"ala\" )) fmt . Println ( isPalindrome ( \"noon\" )) fmt . Println ( isPalindrome ( \"ab\" )) fmt . Println ( isPalindrome ( \"racecar\" )) fmt . Println ( isPalindrome ( \"abfooba\" )) // slower and uses extra memory fmt . Println ( isPalindrome ( strconv . Itoa ( 1991 ))) fmt . Println ( isPalindrome ( strconv . Itoa ( 1981 ))) // takes advantage of math (mindblown) number := 9 reversed := reverseInteger ( number ) fmt . Println ( number , reversed ) fmt . Println ( number , \"is a palindrome: \" , number == reversed ) number = 123 reversed = reverseInteger ( number ) fmt . Println ( number , reversed ) fmt . Println ( number , \"is a palindrome: \" , number == reversed ) number = 121 reversed = reverseInteger ( number ) fmt . Println ( number , reversed ) fmt . Println ( number , \"is a palindrome: \" , number == reversed ) } func isPalindrome ( word string ) bool { for index , value := range word { oppositeIndex := len ( word ) - index - 1 opposite := rune ( word [ oppositeIndex ]) fmt . Printf ( \"%d %T %c compared to %d %c \\n\" , index , value , value , oppositeIndex , opposite ) if index >= oppositeIndex { break } if value != opposite { return false } } return true } func reverseInteger ( x int ) int { reversed := 0 for ; x > 0 ; x /= 10 { remainder := x % 10 reversed = ( reversed * 10 ) + remainder fmt . Println ( remainder , x , reversed ) } return reversed } Random Integers // https://golang.org/pkg/crypto/rand/ // https://golang.org/pkg/math/big/ var max big . Int var myRandom * big . Int var err error max . SetUint64 ( 10 ) myRandom , err = rand . Int ( rand . Reader , & max ) fmt . Println ( * myRandom ) fmt . Println ( err ) Binary Search Using main and print is the poor man's Unit Testing ;) package main import ( \"bytes\" \"fmt\" ) //todo pass by ref? func binarySearch ( a [] int , target , low , mid , high int ) int { fmt . Println ( \"low =\" , low , \"mid =\" , mid , \"high =\" , high ) if a [ mid ] == target { return mid } if mid >= high || mid <= low { return - 1 } if a [ mid ] < target { low = mid + 1 } if a [ mid ] > target { high = mid } mid = (( high - low ) / 2 ) + low return binarySearch ( a , target , low , mid , high ) } func main () { a := [] int { 1 , 4 , 6 , 8 } fmt . Println ( a ) targets := [] int { 1 , 4 , 6 , 8 , 0 , 3 , 5 , 7 , 9 } for _ , t := range targets { result := binarySearch ( a , t , 0 , len ( a ) / 2 , len ( a )) fmt . Println ( \"found\" , t , \"at location\" , result ) } } Efficient String append and replacement http://stackoverflow.com/questions/1760757/how-to-efficiently-concatenate-strings-in-go func myReplace ( source string ) string { var b bytes . Buffer for _ , c := range source { if c == ' ' { b . WriteString ( \"%20\" ) } else { b . WriteString ( string ( c )) } } return b . String () } Testing with Go https://golang.org/pkg/testing/ http://nathanleclaire.com/blog/2015/10/10/interfaces-and-composition-for-effective-unit-testing-in-golang/ https://cloud.google.com/appengine/docs/go/tools/localunittesting/#Go_Introducing_the_Go_testing_package A simple web server package main import ( \"fmt\" \"net/http\" ) func myHandler ( w http . ResponseWriter , r * http . Request ) { fmt . Fprintf ( w , \"hi\" ) } func main () { http . HandleFunc ( \"/\" , myHandler ) http . ListenAndServe ( \":8080\" , nil ) } Verify with curl localhost:8080 Deploying a Go Web Application to Google AppEngine First create an app.yaml file in your package application : MyApplicationName version : 1 runtime : go api_version : go1 handlers : - url : /.* script : _go_app Second adapt your source code to the Google App Engine entrypoint: package myapplicationname // package main import ( \"fmt\" \"net/http\" ) func myHandler ( w http . ResponseWriter , r * http . Request ) { fmt . Fprintf ( w , \"hi\" ) } // The App Engine PaaS provides its own main() that handles the Listening and Serving ;) //func main() { func init () { http . HandleFunc ( \"/\" , myHandler ) // http.ListenAndServe(\":8080\", nil) } Assuming you have created the project with https://console.cloud.google.com/ and received a unique application id... A prerequisite is to use the SDK if you want to test it locally: https://cloud.google.com/appengine/downloads#Google_App_Engine_SDK_for_Go unzip go_appengine_sdk...zip /opt/go_appengine/goapp serve /path-to-project/MyProjectFolder/ INFO 2016-06-02 06:30:27,493 devappserver2.py:769] Skipping SDK update check. INFO 2016-06-02 06:30:27,527 api_server.py:205] Starting API server at: http://localhost:38837 INFO 2016-06-02 06:30:27,530 dispatcher.py:197] Starting module \"default\" running at: http://localhost:8080 INFO 2016-06-02 06:30:27,531 admin_server.py:116] Starting admin server at: http://localhost:8000 To deploy to Google App Engine /opt/go_appengine/appcfg.py -A MyApplicationID update ./MyProjectFolder/ View the version deployed and stats with https://console.cloud.google.com/appengine/versions?project=MyApplicationId curl http://MyApplicationId.appspot.com/ A gotcha is future updates deployed need the app.yaml version to increment AND either use the Web UI to set the new \"default\" or... /opt/go_appengine/appcfg.py -A MyApplicationId set_default_version /MyProjectFolder HandlerFunc and Anonymous Functions and Closure The decorator pattern Anonymous functions an package main import ( \"fmt\" \"net/http\" ) /* using an anonymous function and closure to wrap the HandlerFunc https://golang.org/pkg/net/http/#HandlerFunc https://medium.com/@matryer/the-http-handlerfunc-wrapper-technique-in-golang-c60bf76e6124 */ func makeHandler ( name string ) http . HandlerFunc { return func ( w http . ResponseWriter , r * http . Request ) { // https://golang.org/src/net/http/request.go fmt . Println ( \"serving: \" , r . URL . Path ) fmt . Fprintf ( w , \"<h1>%s</h1>\" , name ) } } func main () { fmt . Println ( \"starting...\" ) indexHandler := makeHandler ( \"Index\" ) myHandler := makeHandler ( \"John\" ) // https://golang.org/pkg/net/http/#HandleFunc , string, func(ResponseWriter, *Request) http . HandleFunc ( \"/\" , indexHandler ) http . HandleFunc ( \"/john\" , myHandler ) http . ListenAndServe ( \":8080\" , nil ) } More Info https://tour.golang.org/basics/ https://blog.joshsoftware.com/2014/03/12/learn-to-build-and-deploy-simple-go-web-apps-part-one/","tags":"programming","title":"Go Programming Intro with VS Code and Arrays Slices Functions and Testing"},{"url":"https://blog.john-pfeiffer.com/build-automation-using-packer-to-build-an-ami-use-immutable-not-chef/","text":"Why build automation? Software has always been about automation and leveraging the computer's capacity for precision and repetition. Somehow though, software is sometimes still deployed using a series of often poorly documented steps (to physical hardware even!). I've been there, it ain't pretty. (badpokerface) The second time you need to build a server running service(s) you may be under time pressure. (Murphy's law says you might be building it again because the first one which was business critical blew up unexpectedly.) Building things by hand is possibly the most expensive way to generate impossible to reproduce bugs and job security for the personality challenged. (Almost everyone agrees that technology employees are expensive and so by extension their time is constantly being wasted by everything they do). As virtualization (and linux!) took over the world there was an explosion of virtual machines that needed to be deployed and an evolution of a fairly standard virtual harwdare layer. (x86 cpu and Intel NIC anyone?) Suddenly you couldn't hire enough antisocial people to run around with floppies and scratching cds while shoving them into servers. Why not chef? Chef, Puppet, and Ansible are the well known configuration management and build/deployment automation tools. Automated configuration management which tries to keep a remote server in a specific state seems like a good recipe for things going wrong I've used chef successfully quite a few times and the main things that make it a specialized tool that I prefer not to use: It's really easy to do chef wrong: nested roles and recipes that keep exploding exponentially with circular dependencies which make you think software development starts looking easy again. Community cookbooks are written to allow deployment on every architecture ever created (Debian, Ubuntu, RedHat, Windows, SPARC, etc.) which makes them challenging to read and debug, almost impossible to customize to do what you actually want. The ruby based DSL isn't bad but it's pretty annoying to constantly make syntax errors (which unless you're all TDD rambo and use Kitchen you'll find during the never ending waiting many minutes for a deployment to fail) It's difficult to debug the non intuitive \"compilation phase\" and \"execution phase\" way chef does its dependency tree magic, and the \"shoot yourself in the foot\" is compounded with the apparently edge case necessary compile time run executions The \"best practices\" have changed 3 or 4 times (write your own custom cookbooks, leverage the community cookbooks, write a custom wrapper for the community cookbook, don't ever use set_unless even though it still exists, etc.) and the 6 layers of variable overrides makes it hard to keep track of what the actual output of a script will be (don't worry, they have pages of documentation explaining it) The recommended \"chef client server architecture\" does not scale to really large numbers well and creates administration overhead and a lot of authorization complexity - and my preferred method with \"chef solo\" still requires an annoying amount of bootstrap setup on the target machines. Polling not only creates network congestion but worse creates windows of uncertainty about deployment state and the possibility of nodes silently dropping out https://docs.chef.io/chef_client.html Chef tends to encourage the pattern of long lived mutable servers (with their therefore necessary expensive and obnoxious biological caretakers) https://docs.chef.io/resource_common.html#lazy-evaluation https://docs.chef.io/resource_common.html#run-in-compile-phase http://erik.hollensbe.org/2013/03/16/the-chef-resource-run-queue/ So is there a simpler way to just reliably, reproducibly, build a box? Packer to the rescue Packer is from the same people who brought you Vagrant https://www.vagrantup.com/ , that really easy way to set up a virtual machine... https://blog.john-pfeiffer.com/using-vagrant-to-deploy-instances-on-aws It is very straightforward to read and actually you can still leverage chef (unless you realize that a series of shell commands is all you wanted anyways...) This leads to the better path of \"immutable servers\" http://martinfowler.com/bliki/ImmutableServer.html packer --version my_example_box.json { \"variables\" : { \"aws_access_key\" : \"\" , \"aws_secret_key\" : \"\" }, \"builders\" : [{ \"type\" : \"amazon-ebs\" , \"access_key\" : \"{{user `aws_access_key`}}\" , \"secret_key\" : \"{{user `aws_secret_key`}}\" , \"region\" : \"us-east-1\" , \"source_ami\" : \"ami-de0d9eb7\" , \"instance_type\" : \"t1.micro\" , \"ssh_username\" : \"ubuntu\" , \"ami_name\" : \"packer-example {{timestamp}}\" }] } packer validate mybox.json packer build mybox.json packer build -debug mybox.json This will prompt for the enter key to continue at each step Once it's done it will terminate the EC2 instance for you (it only runs as long as it takes to build the machine and then burn the Amazon Machine Image). us-east-1: ami-19601234 Unfortunately it is not machine readable json output so you have to do some bash-fu to extract just the id Also unfortunately there is no way to tell packer to not terminate so you can troubleshoot, the workarounds are the -debug which is essentially \"interactive\" or adding sleep commands my_advanced_box.json { \"_comment\" : \"This is a comment\" , \"variables\" : { \"my_secret\" : \"{{env `MY_SECRET`}}\" , }, \"builders\" : [{ \"type\" : \"amazon-ebs\" , \"region\" : \"us-east-1\" , \"source_ami\" : \"ami-de0d9eb7\" , \"instance_type\" : \"t1.micro\" , \"ssh_username\" : \"ubuntu\" , \"ami_name\" : \"packer-example {{timestamp}}\" \"subnet_id\" : \"subnet-f0be1234\" , \"security_group_id\" : \"sg-9bf51234\" , \"associate_public_ip_address\" : true , \"ssh_keypair_name\" : \"my-packer\" , \"ssh_private_key_file\" : \"./my-packer.pem\" }], \"provisioners\" : [{ \"type\" : \"file\" , \"source\" : \"./debs/\" , \"destination\" : \"/tmp\" }, { \"type\" : \"shell\" , \"inline\" : [ \"/sbin/ip a\" , \"curl -s http://checkip.amazonaws.com\" , \"ls -ahl /tmp\" , \"echo {{user `my_secret`}} > /tmp/{{isotime \\\"2006-01-02-030405\\\"}}--my-secret.txt\" , \"sudo dpkg -i --force-confnew /tmp/*.deb\" , \"machine_state_validation.sh\" ] }] } The access credentials could instead be environment variables: AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY Post instantiation validation is a really handy safeguard as statistically something always goes wrong somewhere and it's far cheaper to find out with a quick test versus a system that loses data. Packer and DigitalOcean DigitalOcean is a relatively new player (compared to Linode and even AWS) but they provide a very fast and easy to use way of building boxes (a snapshot can be used like an AMI to spin up multiple instances). { \"_comment\": \"https://www.packer.io/docs/builders/digitalocean.html\", \"variables\": { \"digitalocean_api_token\": \" {{ env ` DIGITALOCEAN_API_TOKEN ` }} \", \"newuser_name\": \" {{ env ` NEWUSER_NAME ` }} \", \"newuser_password\": \" {{ env ` NEWUSER_PASSWORD ` }} \" }, \"builders\": [{ \"type\": \"digitalocean\", \"api_token\": \" {{ user ` digitalocean_api_token ` }} \", \"size\": \"512mb\", \"region\": \"lon1\", \"image\": \"ubuntu-16-04-x64\", \"droplet_name\": \"built-from-packer- {{ timestamp }} \", \"snapshot_name\": \"built-from-packer- {{ timestamp }} \" }], \"provisioners\": [ { \"type\": \"shell\", \"inline\": [ \"ip a\", \"curl -s http://checkip.amazonaws.com\", \"apt-get update\", \"sudo apt-get install -y vim curl wget byobu ntp\", \"timedatectl set-timezone Etc/UTC\", \"cat /etc/timezone\", \"date\", \"useradd -s /bin/bash -m {{ user ` newuser_name ` }} \", \"usermod -a -G admin {{ user ` newuser_name ` }} \", \"echo ' {{ user ` newuser_name ` }} : {{ user ` newuser_password ` }} '|chpasswd\", \"cat /etc/passwd\", \"sed -i 's/Port 22/Port 2222/g' /etc/ssh/sshd_config\", \"sed -i 's/PermitRootLogin yes/PermitRootLogin no/g' /etc/ssh/sshd_config\", \"echo 'PasswordAuthentication no' >> /etc/ssh/sshd_config\", \"cat /etc/ssh/sshd_config\", \"mkdir -p /home/ {{ user ` newuser_name ` }} /.ssh\", \"mkdir -p /opt/www/html\" ] }, { \"type\": \"file\", \"source\": \"authorized_keys\", \"destination\": \"/home/ {{ user ` newuser_name ` }} /.ssh/authorized_keys\" } ] } This is a simple example that automates some of the security best practices of a non standard username, non standard ssh port, no ssh root login, no ssh password based login, etc. NEWUSER_NAME=yourusername NEWUSER_PASSWORD=yourpassword DIGITALOCEAN_API_TOKEN=012345yourtoken /opt/packer build packer.json Why not docker containers? Actually I prefer docker containers as the artifact and deployment vehicle for services but it's not the only hammer in your toolbelt. And you have to setup the Docker hosts somehow, right? (Unless you've already uploaded your soul into the matrix and are using Googazon's PaaS and never have to sully your container delicate fingers with a crude virtual machine again). more info https://www.packer.io/docs/installation.html https://www.packer.io/docs/builders/amazon-ebs.html https://www.packer.io/intro/getting-started/build-image.html https://www.packer.io/docs/templates/configuration-templates.html https://www.packer.io/docs/templates/user-variables.html","tags":"build-CI-CD-devops","title":"Build Automation using packer to build an AMI use immutable not chef"},{"url":"https://blog.john-pfeiffer.com/nginx-with-docker/","text":"nginx overview http://nginx.org/en/ is one of the most popular performant web servers in the world (and it's pretty handy as a reverse proxy or load balancer too!). http://nginx.org/en/docs/http/load_balancing.html https://github.com/nginx/nginx is written in c (very performant but often needs to be compiled , especially with any of the extra 3rd party modules https://www.nginx.com/resources/wiki/modules/ ). A recent update means \"...optionally load separate shared object files at runtime as modules\" https://www.nginx.com/blog/dynamic-modules-nginx-1-9-11/ docker pull nginx With Docker most of the time is spent in preparation, configuration, and testing. The advantage is that less time is wasted on compiling, packaging, etc. (for all those still eeking out another 1% in efficiency via esoteric flags and bundling all sorts of custom modules - good luck!) One quick way to attempt to leverage nginx as a front end for your projects is using containers with Docker https://hub.docker.com/_/nginx/ sudo su docker pull nginx:alpine docker images This will grab the latest image based on the very small alpine linux https://en.wikipedia.org/wiki/Alpine_Linux , around 13 MB vs 191 MB for the traditional nginx:latest which is based on debian jessie https://en.wikipedia.org/wiki/Debian https://hub.docker.com/r/library/nginx/tags/ contains what other versions of nginx are provided by the vendor as Docker Images, the default build/image has quite a few modules http://nginx.org/en/docs/ all future references to docker commands will assume you are root or typing sudo first docker nginx interactive container docker run --rm --publish 127.0.0.1:80:80 nginx starts an ephemeral container that binds the container port 80 to the local Host port 80 (binding to 127.0.0.1 prevents any other access except from the Host) , note by not explicitly sharing port 443 it is not connected/available docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3fccf15a3c24 nginx \"nginx -g 'daemon off\" 3 seconds ago Up 2 seconds 127.0.0.1:80->80/tcp, 443/tcp pensive_elion netstat -antp Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:80 0.0.0.0:* LISTEN 5826/docker-proxy curl 127.0.0.1:80 <h1>Welcome to nginx!</h1> is part of the default nginx home page, (success) You will notice the access logs are being output to the console (where the docker container is running). 192.168.1.100 - - [01/Jan/1970:01:00:59 +0000] \"GET / HTTP/1.1\" 200 612 \"-\" \"curl/7.38.0\" \"-\" Control + C will terminate the container running nginx manually in the interactive docker container docker run --rm -i -t --publish 127.0.0.1:80:80 nginx /bin/bash Starting as root inside the container root @557 ac197e2c1 : / # which nginx / usr / sbin / nginx nginx version and modules root @557 ac197e2c1 : / # nginx - V nginx version : nginx / 1.9.7 built by gcc 4.9.2 ( Debian 4.9.2 - 10 ) built with OpenSSL 1.0.1 k 8 Jan 2015 TLS SNI support enabled configure arguments : -- prefix =/ etc / nginx -- sbin - path =/ usr / sbin / nginx -- conf - path =/ etc / nginx / nginx . conf -- error - log - path =/ var / log / nginx / error . log -- http - log - path =/ var / log / nginx / access . log -- pid - path =/ var / run / nginx . pid -- lock - path =/ var / run / nginx . lock -- http - client - body - temp - path =/ var / cache / nginx / client_temp -- http - proxy - temp - path =/ var / cache / nginx / proxy_temp -- http - fastcgi - temp - path =/ var / cache / nginx / fastcgi_temp -- http - uwsgi - temp - path =/ var / cache / nginx / uwsgi_temp -- http - scgi - temp - path =/ var / cache / nginx / scgi_temp -- user = nginx -- group = nginx -- with - http_ssl_module -- with - http_realip_module -- with - http_addition_module -- with - http_sub_module -- with - http_dav_module -- with - http_flv_module -- with - http_mp4_module -- with - http_gunzip_module -- with - http_gzip_static_module -- with - http_random_index_module -- with - http_secure_link_module -- with - http_stub_status_module -- with - http_auth_request_module -- with - threads -- with - stream -- with - stream_ssl_module -- with - mail -- with - mail_ssl_module -- with - file - aio -- with - http_v2_module -- with - cc - opt = ' - g - O2 - fstack - protector - strong - Wformat - Werror = format - security - Wp , - D_FORTIFY_SOURCE = 2 ' -- with - ld - opt = ' - Wl , - z , relro - Wl , -- as - needed ' -- with - ipv6 Test your config file nginx -t -c /etc/nginx/nginx.conf -g \"daemon off;\" nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful run nginx directly /usr/sbin/nginx -c /etc/nginx/nginx.conf -g \"pid /var/run/nginx.pid; worker_processes 2;\" from https://www.nginx.com/resources/wiki/start/topics/tutorials/commandline/ nginx.conf While there are quite a few ways to configure nginx one choice to make with Docker is to either docker run --name some-nginx -v /some/nginx.conf:/etc/nginx/nginx.conf:ro -d nginx run docker as a daemon with a specified Container Name and override the container nginx.conf file with /some/nginx.conf from the hose Use a Dockerfile (based on the upstream nginx Docker image) to copy your own configuration file and other custom bits in and build your own custom Docker image (a highly recommended way of not being completely dependent on an upstream provider - especially if you push your Docker Image to your own private registry afterwards) e.g. https://hub.docker.com/r/jwilder/nginx-proxy/~/dockerfile/ which also has \"Foreman in Go lang\" and makes use of expecting the Host to provide the SSL certificates https://www.nginx.com/resources/admin-guide/nginx-web-server/ https://www.nginx.com/resources/wiki/start/topics/examples/full/ docker run --rm --publish 127.0.0.1:80:80 nginx /bin/bash -c \"nginx -t -c /etc/nginx/nginx.conf\" nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful Build your own nginx Dockerfile If you require 3rd party modules then you will have to build nginx from source, e.g. https://github.com/openresty/headers-more-nginx-module#installation That being the case you'll probably want to have a build process with 2 Docker files: the first Dockerfile will contain build-essentials, gcc, make, etc. so that you can build the binary from source (with any 3rd party modules) http://nginx.org/en/docs/configure.html The second docker image would be your \"production container\" where you copy in the custom nginx binary, install the dependencies (i.e. openssl), and setup the default config. Converting an existing nginx 3rd party module to a dynamic module Since NGINX 1.9.11 supports dynamic modules and attempts to maintain API compatibility then it is possible to sometimes convert a module into a dynamic module (i.e. build the shared library object) ./configure --add-dynamic-module=/opt/source/ngx_my_module/ make -f objs/Makefile modules Look for .so files in the objs directory after compilation or the modules subdirectory during installation https://www.nginx.com/resources/wiki/extending/converting/ Configuring nginx There are entire books about how to configure nginx so I will just jot down some basics for myself. /etc/nginx/nginx.conf user nginx; worker_processes 4; pid /run/nginx.pid; events { worker_connections 1024; } http { server { location / { root /var/www; } } # include /etc/nginx/conf.d/*.conf; } Run the service as the www-data user and define 4 worker processes define an HTTP server that listens on port 80 by default the root location will return the contents of /var/www a best practice is to use multiple configuration files in the conf.d directory (as a really long complex configuration in a single is difficult to maintain) BUT we must comment it out as in the installation there can be a default.conf that overrides our nginx.conf sudo docker run -it --rm --publish 0.0.0.0:80:80 --volume /tmp/nginx.conf:/etc/nginx/nginx.conf:ro nginx:alpine /bin/sh debug the image interactively by overriding the default CMD with a shell (remember that Alpine is limited so apk update and apk add SOMENAME sudo docker run --rm --publish 0.0.0.0:80:80 --volume /tmp/nginx.conf:/etc/nginx/nginx.conf:ro nginx:alpine /bin/sh -c \"nginx -t -c /etc/nginx/nginx.conf\" alternative method to just test your configuration file sudo docker run --rm --publish 0.0.0.0:80:80 --volume /tmp/nginx.conf:/etc/nginx/nginx.conf:ro --volume /tmp/www:/var/www:ro nginx:alpine assumes you have a config file and some index file defined /var/www/index.html <html><body> hi </body></html> That's it, now you have nginx serving static files! (curl localhost OR use a browser and visit localhost or http://hostfqdn) of course /tmp is an insecure location so please store production nginx configuration files and web content from a secure directory in the docker host filesystem http://nginx.org/en/docs/beginners_guide.html http://nginx.org/en/docs/ngx_core_module.html#worker_processes nginx with ssl It is hard to imagine running a production (or even test or dev server that should mirror production) without SSL since all traffic could be intercepted or hijacked. It takes a little more work but clearly it is an important step in running a service that others will use. /etc/nginx/nginx.conf user nobody; worker_processes 4; pid /run/nginx.pid; events { worker_connections 1024; } http { server { listen 443 ssl; ssl_certificate /etc/nginx/server.crt; ssl_certificate_key /etc/nginx/server.key; location / { root /var/www; } } } openssl self signed certificates openssl req -subj '/CN=example.com/O=My Company Name LTD./C=US' -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /tmp/server.key -out /tmp/server.crt mkdir -p /tmp/www echo \" <html><body> hi </body></html> \" > /tmp/www/index.html dockerized nginx with ssl sudo docker run --rm --publish 0.0.0.0:443:443 --volume /tmp/nginx.conf:/etc/nginx/nginx.conf:ro --volume /tmp/server.crt:/etc/nginx/server.crt --volume /tmp/server.key:/etc/nginx/server.key --volume /tmp/www:/var/www:ro nginx:alpine curl --insecure https://localhost firefox https://localhost http://nginx.org/en/docs/http/configuring_https_servers.html nginx with ssl and http/2 https://en.wikipedia.org/wiki/HTTP/2 is a new and more capable and performant standard for the venerable HTTP protocol. It has widespread vendor support so you can use most modern servers (e.g. nginx) and most modern browsers (e.g. chrome) and get the benefits immediately. (With of course all sorts of fallbacks for legacy clients) /etc/nginx/nginx.conf user nobody ; worker_processes 4 ; pid / run / nginx .pid ; events { worker_connections 1024 ; } http { server { listen 80 ; location / { return 301 https :// $ host $ request_uri ; } } server { listen 443 ssl http2 default_server ; ssl_certificate / etc / nginx / server . crt ; ssl_certificate_key / etc / nginx / server . key ; location / { root / var / www ; } } } Assuming the previous nginx with ssl steps of creating a certificate and content, be aware that browsers permanently CACHE the 301 redirect so use Private Browsing mode otherwise you will never see a different result for localhost =[ sudo docker run --rm --publish 0.0.0.0:80:80 --publish 0.0.0.0:443:443 --volume /tmp/nginx.conf:/etc/nginx/nginx.conf:ro --volume /tmp/server.crt:/etc/nginx/server.crt --volume /tmp/server.key:/etc/nginx/server.key --volume /tmp/www:/var/www:ro nginx:alpine Verifying HTTP/2 curl curl localhost <html> <head><title> 301 Moved Permanently </title></head> <body bgcolor= \"white\" > <center><h1> 301 Moved Permanently </h1></center> <hr><center> nginx/1.9.12 </center> </body> </html> curl --insecure --location localhost <html><body> hi </body></html> openssl s_client openssl s_client -connect localhost:443 -nextprotoneg '' CONNECTED(00000003) Protocols advertised by server: h2, http/1.1 chrome browser Browse directly to https://localhost/ with the chrome extension installed: https://chrome.google.com/webstore/detail/http2-and-spdy-indicator/mpbpobfflnpcgagjijhmgnchggcjblin/related?hl=en The blue lightning symbol on the far far right (next to the \"hamburger\") indicates HTTP/2 is working. browsers permanently CACHE the 301 redirect so connecting to http://localhost will forever see a different result for localhost =[ Browse directly to https://localhost/ (If using a self signed certificate accept any warnings about insecure SSL) Chrome/Chromium Settings -> More tools -> Developer tools (aka Control + Shift + I) Click on the Network section Control + Shift + F5 to reload (or click on the arrow circling up) Right click on the Name colum in the result so that you can add results for column heading, \"Protocol\" Control + Shift + F5 to reload (or click on the arrow circling up) Protocol: h2 https://www.nginx.com/blog/nginx-1-9-5/ https://blog.cloudflare.com/tools-for-debugging-testing-and-using-http-2/ http://tech.finn.no/2015/09/25/setup-nginx-with-http2-for-local-development/ nginx with php-fpm Investigate php-fpm in Docker This hack is fun but proves unnecessary when using Docker Compose later... docker pull php:5.5-fpm-alpine https://hub.docker.com/_/php/ :::bash ifconfig | grep Bc docker run -it --rm --publish 0.0.0.0:9000:9000 php:5.5-fpm-alpine /bin/sh route -n php-fpm --version sed -i 's/listen = 127.0.0.1:9000/listen = 0.0.0.0:9000/g' /usr/local/etc/php-fpm.d/www.conf php-fpm --fpm-config /usr/local/etc/php-fpm.conf ifconfig and route -n are to discover the docker host IP Address via the default gateway 172.17.0.1 contains include=etc/php-fpm.d/*.conf /usr/local/etc/php-fpm.d/www.conf http://php.net/manual/en/install.fpm.configuration.php Configure nginx vi /etc/nginx/nginx.conf worker_processes 4 ; pid / run / nginx .pid ; events { worker_connections 1024 ; } http { server { location / { root / var / www / html ; try_files $ uri $ uri / index . php ; } location ~ \\ .php $ { root / var / www / html ; try_files $ uri $ uri / ; fastcgi_index index . php ; # depends on Docker linking or hostfile for fpm to resolve fastcgi_pass fpm : 9000 ; fastcgi_param SCRIPT_FILENAME $ document_root $ fastcgi_script_name ; # https :// web . nvd . nist . gov / view / vuln / detail ? vulnId = CVE -2016-5385 fastcgi_param HTTP_PROXY \"\" ; include fastcgi_params ; } } } a basic nginx config that forwards .php requests to fpm on port 9000 docker run -it --rm --publish 0.0.0.0:80:80 --volume /tmp/nginx.conf:/etc/nginx/nginx.conf:ro --volume /tmp/www/html:/var/www/html:ro nginx:alpine /bin/sh route -n This manual command proves unnecessary with Docker Compose but can be useful to debug (i.e. run the php-fpm container first, then run this one since it depends on fpm) Some Test Content Create the following files to test the various cases... vi /tmp/www/html/foo.html <html><body> hi </body></html> vi /tmp/www/html/index.php <?php print \"<html><body>hello</body></html>\" ; ?> vi /tmp/www/html/bar.php <?php print \"bar\" ; ?> docker-compose.yml # https://docs.docker.com/compose/compose-file/ nginx: image: nginx:alpine ports: - \"80:80\" volumes: - /tmp/nginx.conf:/etc/nginx/nginx.conf:ro - /tmp/www/html:/var/www/html links: - fpm fpm: image: php:5.5-fpm-alpine ports: - \"9000:9000\" volumes: - /tmp/www/html:/var/www/html # http://stackoverflow.com/questions/29905953/how-to-correctly-link-php-fpm-and-nginx-docker-containers-together # http://stackoverflow.com/questions/35388590/issue-with-docker-compose-container-command-not-found docker-compose up to start nginx and php-fpm docker-compose up docker-compose rm -f curl localhost:80/foo.html curl localhost:80/bar.php curl localhost:80/index.php Another tiny step forward in tying together a lot of moving pieces =]","tags":"virtualization","title":"nginx with Docker"},{"url":"https://blog.john-pfeiffer.com/meeting-bjarne-stroustrup-creator-of-c-plus-plus-in-the-atlassian-dev-den/","text":"Bjarne Stroustrup visits Atlassian One of the things I looked forward to when joining Atlassian was being a part of a culture that celebrated software development. Today I had the opportunity to meet Bjarne Stroustrup http://www.stroustrup.com/ , creator of C++. (yey) To put this in perspective, C++ is one of the most popular programming languages of all time and is essentially like meeting the inventor of the hammer. (mindblown) To summarize, he was one of the most intelligent, mindful, humble, and quietly passionate people I have ever met. Truly an inspiration to any engineer. I have taken a tiny amount of free license due to my faulty memory and for readability. Getting the book signed I had dug out my old The C++ Programming Language (3rd edition from 1997) which filled me with nostalgia of how amazing it was to read a text book that was just full of succinct coding amazingness. So many textbooks were full of fluff, confusion, and downright goofiness but that book opened my mind to the difference it makes to have the original inventor writing it (Kernighan and Ritchie ;) Lucky me, I was there about 15 minutes early and while they got the Audio/Video set up I bravely politely asked him to sign it. (yes, success!) There was one other co worker there who was helping him prepare (and apparently also a huge fan, I took her picture with him) and I managed to ask some small talk questions (go me!) Bjarne seemed distant at first and his driver/handler (former Chief Technology Officer somewhere important) was maybe a bit apprehensive but I suspect the fan-ness of it all relaxed a bit after we proved we weren't pyschos... \"This book was really amazing.\" He responded, \"Well this one is a bit old, you should use a newer edition\". I quickly replied, \"I don't actually use this in my day to day work so I keep it at home. It's a bit heavy to carry around all the time, I read a lot of things on my phone (easier to get the latest version of the books).\" I must have hit a topic of interest for him as he came over and I had flipped open the book to some sample code. He ruminated, \"Well, actually, I prefer this kind of reading with a book. The formatting and print... you have to work hard to make sure the examples don't get split across pages. And you have two pages.\" (Gesturing with his hands the extra width) I agreed with him, \"Yes, it is certainly a lot easier to read the diagrams and code examples in a physical book. The phone screen is really small. I guess the book is a perfected technology that's been around 1000 years.\" \"You know Atlassian isn't really a C++ shop, more Java with some Javascript... and Python products teams here in SF. Actually it's even a really argumentative culture sometimes around technology, though with you here I'm sure they'll all be quiet and respectful.\" He actually smiled at that, \"Back in AT&T we would ask: was there blood on the floor? That was how serious the discussions got. Not really any blood on the floor, just an expression.\" \"What do you enjoy most from your travels/touring?\" Such a wise response: \"Meeting the people, learning about what they're thinking and what new ideas they might have\". Looking around the mostly empty room with 1 minute before the event starts, \"Well I don't know where everyone else is, I'm sure there's lots of interest but the SF office is the Marketing and Business headquarters with only a few Development/Product teams\". He responded, \"Yesterday Facebook was standing room only.\" I looked over at the monitors, \"Well I'm sure there'll be attendance from the other offices too\". \"What other locations does Atlassian have?\" \"Austin, and of course Sydney has 800 Java Developers, and Poland, and Vietnam, it's a pretty globally diverse company\". I also warned him about the air conditioning intermittently coming on and being loud and often too cold. \"I was baking hot yesterday so I guess if it's cold in here today I guess it will average out\". (With his curious funny little smile) And with the microphone on and an international audience he was introduced and here were the questions and answers... Current challenges for programming languages Since hardware improvements are through parallelization (unless quantum entanglements gets solved soon) developers must adapt, but concurrency is hard. (And no global variables, right?) \"Functional programming has good ideas but I did not want everything to be a recursive function\" Distributed systems are a new perspective problem: for a fair trade, an exchange in New York offering the same price to Sydney and San Francisco becomes difficult (the speed of light is still a limit). Anyone heard of the leap second? There are at least 3 ways it is being dealt with: Hold on for a really long time and then jump forward all at once. Jump forward immediately. Break the leap second into a lot of intervals and merge it in. But if it's a distributed system how can everyone agree? And that's just one example. Trends in programming? Silicon Valley is an echo chamber but you probably know more than I do. Mobile is very hot but it is a very small screen, and not everything (like a textbook) can be read on a small phone screen or even a tablet screen. A second favorite language? Bjarne was incredibly diplomatic about pointing out that any language he said he preferred might be tweeted and start a \"language war\"; language comparisons are useless without the context of the constraints of the problem domain. For instance he had written C++ to be a performant language with the assumption of unix shell as the \"other language\". But what about Rust \"Zero Cost Abstractions\"? \"It's not good to pay for what you don't use.\" He admitted that since he coined the phrase \"Abstraction without overhead\" that indeed he admired the goal of Rust (getting close to the hardware for maximum performance) but hadn't been following their recent developments. For garbage collection... you shouldn't litter. http://blog.rust-lang.org/2015/05/11/traits.html Languages are about solving problems so I am very familiar with the strengths and weaknesses of C++ and I know how to make it do quite a lot. But if I use another language then I am an amateur, not familiar with the idioms, and perhaps I will not use it so well. Any features you regret putting in or leaving out of C++? No, everything in there has a use and ... they have to stay or it would upset thousands of people. I wish it had better syntax, starting from C, well even they realized it was a mistake but it was already done so what are you going to do. Anything you put in a language, especially a mistake, will stay with you. Maybe you can put something off to release a product and know you will get back to it in an iteration or two. But make a mistake with a language and you live with it for a couple of decades. I do wish I had gotten some things right earlier. Templates. They solved the problem really well, general enough to do more than I imagined (that's the criteria for sucess). Nobody else at the time knew how to solve it but I wish the interface was better. Now, with the upcoming \"Concepts\", it will be better, but I wish I had done it right the first time. You have to understand, at the time of creation C was constrained to 64K, and C++ to 256K, so there are simply things you cannot do then. A complicated question about the N4047 modules proposal It was a good proposal. Isolating code from other portions to make compilation faster. C++ is maintained by people who actually pay 1280 to be on the committee... they don't make any money by doing it. Daveed Vandevoorde had to continue working and doing this proposal was something on the side. If it is not in my top 20 then it's not an important problem. So a lot of good proposals do not make my top 20. More recently teams in Microsoft, Google, GCC, and Clang are looking at this and some will even have implementations before it is released. It should be including in release 17 and you may see a compilation speedup between %50 and 50x , maybe more usually 4x or 8x faster. Language features are complicated as you have to coordinate between all of the major tool chains (you certainly can't have one of them do it one way and another differently). How many languages? When asked about whether a developer should focus on one language or learn many Bjarne had a very nuanced answer: When he started programming it was not impossible to learn 25 languages. It is very valuable to learn the different ways of thinking and solving problems by programming in a different language. Now though, languages are more complex with a lot of libraries, idioms, and toolchain to learn. So learning two languages, like the example before of 1 performant and 1 more general, would not be enough since that would not provide any comparison of why. Maybe 4, so that you could compare two general and two performant languages and understand how/why they did things differently. My Question I decided to mull over and try and ask a live question (with an audience this time). \"Given your comment about tooling before, how valuable do you find developer tools to a programming language (like an IDE)? Also, what are any major gaps you see in current tools?\" \"Good question. I don't really use tools that much since I work on smaller examples that might need to be ported in a few places. On larger projects and code bases tooling is very useful as you can't keep all of it in your head anymore. I'm maybe not the best person to ask, you probably work on large code more than I. But just being able to click on a macro or identifier and jump to it in the code is very useful.\" \"I don't use a debugger very often. Other people might but I don't. If you are using a debugger it's maybe a bad sign about your code. But I would like to see a debugger that could follow concurrency. That would really help.\" \"Tests are really important, so tools that make it easy to run tests, and to write tests quickly, that would be very useful.\" What is a professional? If someone, your employer or your manager, asks you to do something that is shabby you should just resign. It doesn't happen very often but there are ethics. I wish the Universities, that teach \"Computer Science\" and not programming, would spend some time on caching or real world coding and problems. They graduate being able to describe the Halting Problem but with magic constants throughout their code. It would be nice if there was a more universal way to explain/teach being \"professional\" in software. C++ in Education Bjarne was asked and clearly had some strong feelings on the subject of why C++ wasn't taught in universities. \"Perhaps it's because C++ doesn't have a marketing budget\" he quipped. More seriously he went on to point out \"he had very successfully taught college freshman programming in C++ so it's quite possible. There isn't enough time to teach all of C first so you don't start with that: for instance pointers are chapter 17, not chapter 2.\" More info An interesting interview he gave for comparison: http://www.stroustrup.com/CVu263interview.pdf","tags":"programming","title":"Meeting Bjarne Stroustrup, creator of C plus plus, in the Atlassian Dev Den"},{"url":"https://blog.john-pfeiffer.com/pragmatic-testing-from-makefile-to-ci-with-docker/","text":"A colleague recently suggested \"Hey, why don't you run those tests from outside of the target server under test?\" And I thought to myself, \"Hmm.... why are we doing it that way? Was I just dumb when I did this the first time? The answer is the journey of successfully testing a successful product and the pragmatic choices made along the way. I believe that engineering requires compromises because without achieving short term progress we would have never reached our bigger, long term goals. Humble beginnings with Makefile Our initial project was to deliver an OVA (open virtualization archive) that packaged all of the services (and dependencies) into an easy to deploy and maintain virtual appliance. Will, as the project lead, had already spent some time setting up the build environment using vStudio and an Ubuntu ISO but one of our first issues after generating an OVA was to try and determine if it was \"good\". Could it even be deployed? Did it have the services installed correctly? I leveraged the Makefile that we were already using for the build to create a \"test\": deploy the latest OVA artifact and check that it was basically sound. This was a good start as, at the very least, we could quickly determine and bisect where we were producing bad builds. Adding some tests, but where? Once the initial spike (\"tracer bullet\") was done I began adding in selenium python based tests to verify the Web UI interface. Since our Continuous Integration system was basically bash and Makefile (which was also our build system) I opted to run the tests from inside of each deployed Virtual Machine. This allowed for isolation of the test execution from the build process and for each test run. While not ideal for product acceptance testing it provided a basic safety net that allowed us to know if a breaking change occurred upstream and was the first automated verification (followed by lots of manual testing) of the exact OVA we were shipping to Customers in the Beta. Bamboo Continuous Integration for visibility and the team A further improvement was to improve visibility of the test results. In parallel, after the team discussion during \"HipCon\" and with motivation from Don and Sam, I setup a private Bamboo installation in our VM area and helped get Integration tests setup for our upstream backend code. Once again I stuck with the tried and true \"tests inside of the target virtual machine\" pattern as the Bamboo server was only using \"local agents\" and I was concerned about trying to maintain a clean environment and the resources required as the number of test plans scaled. Additionally I migrated all of the previous test plans from the \"build factory\" into Bamboo which really helped with failed test visibility and tracking over time. The unit tests continued to run in a different SaaS version of Bamboo so I avoided scope creep and left things that were working alone. Migrating to a managed service One of the most asked questions whenever a new person joined was \"Why are the tests spread across so many different servers/services\"? I had to answer that question so many times! The answer basically boiled down to \"the testing grew organically as different people in the team solved the problem they faced\". Unsatisfactory? \"Be the change you seek\" didn't seem to get anyone else to solve the problem for me ;) Using the opportunity presented by a service outage issue I pushed forward a plan I had to migrate all of the test plans into a newly provisioned Bamboo server run by Build Engineering. (awthanks) There was an awesome team effort by a lot of people to make that happen (made even more challenging by doing it when the source service was out). (awesome) This solved quite a few problems: The Unit, Integration, and Product Acceptance tests were all finally combined under one roof... one UI to rule them all! New people joining the team needing access to Bamboo: since I hadn't linked it to any directory/authentication I was adding users manually - we were finally able to leverage an Atlassian backend Directory system I had been managing upgrades and maintenance of the Bamboo software (not fun and not my core expertise) I began to worry about the resource consumption (since this was running on hardware that also provided for our Build Factory) - no longer a worry with lots of Remote Agents and Elastic Agents (all provided by Build Engineering) Bonuses: More plugins and capabilities and knowledge from the Bamboo server and Build Engineering expertise Plan Templates to keep test plans in version control and macros for common functionality A successful spike of using Docker for unit testing With so much going on during the migration I avoided changing the testing paradigm, so tests continued to execute inside of each VM/EC2 Instance deployed. (shrug) Adopting Docker and refactoring Remember that question at the beginning? When something goes wrong, while it makes sense to separate \"fixing it\" from \"improving it\" I'm a big fan of taking advantage of having the hood open to go the extra mile and leave the campground cleaner =) So some of the selenium based tests were failing and it occurred to me that some of how we were changing our dependency infrastructure at the operating system level could be the cause. After some unproductive poking around I tried to reproduce and isolate the issue by running the tests from outside of the VM. Aha! In that moment I realized that this was actually the desired (original intention, honest!) way to run the blackbox product acceptance tests. So I pulled up my sleeves and tried out the \"hot fancy new silver bullet technology that solves every problem\". Why Docker? Docker encourages design of modular, deterministic and defined, single purpose components that are easy to reuse and compose into larger services. Not only are (Docker) Containers fast, one of the biggest advantages of Containers is the ability to reduce complexity. Docker can turn an application/service, it's dependencies, and even the OS level requirements into a single blackbox package (that you can still inspect inside if you really want to). So I built a Docker Image containing python selenium and http://phantomjs.org/ (a headless javascript based browser) and other dependencies. Sure enough I the tests passed when leveraging the previous Docker spike to run my new docker container. (success) Refactoring the bamboo plan (since it was leveraging Plan Templates and the Groovy DSL macros) didn't take too long and with other stakeholders PR/approval we're moving full speed ahead towards the \"ideal\" solution. (It only took about 2 years). Dogfooding the whole way Something I should mention that has been an invaluable companion throughout the course of building the product: a dogfooding server. \"Eating our own dog food\" is a wonderful way to experience the exact pain you are inflicting on your users. From the very begininng Will setup and we maintained a dogfood server which received every beta upgrade (and a few upgrades that never reached the customers), amazingly it's still alive and full of data! Not only did I learn about bugs that would affect our oldest and most loyal users (who kept with us and kept upgrading), I also felt the User Experience pain of how long upgrades took, mysterious incomprehensible errors messages, and \"partial upgrades\". All of these learnings, along with being Developer on Support and assisting on support tickets, kept me honest and humble and allowed me to improve the product just as much as any fancy testing automation framework. What does this all mean? It's easy to draw up how things should work according to best practice. It's even easier if it's work that someone else has to do and there aren't any deadlines. Success comes in stages. Overengineering and premature optimization cost way more in opportunity cost and thrown away work than doing things the \"wrong way\". This story could be massaged to fit a parable of \"Lean and Agile\" but it's really just common sense about transparently understanding the cost/value tradeof of the work, solving the current needs, and moving forward onto something better (by keeping informed of new solutions) when the opportunity shows up.","tags":"build-CI-CD-devops","title":"Pragmatic testing, from Makefile to CI with Docker"},{"url":"https://blog.john-pfeiffer.com/haproxy-in-docker/","text":"Not only are Containers fast, one of the biggest advantages of Containers is the ability to reduce complexity. Docker can turn an application/service, it's dependencies, and even the OS level requirements into a single blackbox package (that you can still inspect inside if you really want to). One thing I really like is less code. Seriously. Configuration over coding (whenever I don't need customization) means far less maintenance and bugs. Here's a trivial example of how I can leverage the HAProxy Docker image/container to load balance two web servers. (aka \"reverse proxy\" http://en.wikipedia.org/wiki/Reverse_proxy ) client -> all other sites | reverse proxy (haproxy) / \\ BackendA BackendB There are new problems that go along with the benefits of any new technology, see the complicated networking/port coordination Prerequisites sudo docker pull haproxy:1.5 Some backend web servers mkdir -p /tmp/BackendA echo \"foo\" > /tmp/BackendA/foo.txt cd /tmp/BackendA python -m SimpleHTTPServer 8000 & mkdir -p /tmp/BackendB echo \"bar\" > /tmp/BackendB/bar.txt cd /tmp/BackendB python -m SimpleHTTPServer 8001 & Clearly a trivial example (more likely two remote hosts in logical/geographic disparate areas if aiming for High Availability, or at least on different hosts to scale with more resources) /opt/mydata/haproxy.cfg global debug defaults log global mode http timeout connect 5000 timeout client 5000 timeout server 5000 listen http_proxy :8443 mode tcp balance roundrobin server srv1 docker:8000 check server srv2 docker:8001 check start haproxy.sh #!/bin/bash HOSTIP = ` ip addr show | grep docker0 | grep global | awk '{print $2}' | cut -d / -f1 ` sudo docker run -p 8443:8443 --add-host = docker: ${ HOSTIP } --rm -it -v /opt/mydata/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro --name myhaproxy haproxy:1.5 Now that the docker container's /etc/hosts file has the Host IP Address injected (with the name \"docker\") the haproxy config file probably makes more sense running the container the host port 8443 mapped to the container port 8443 injecting into the container /etc/hosts the Host IP Address as \"docker\" ephemeral container (automatic cleanup on termination) interactive tty readonly mapping of the /opt/mydata/haproxy.cfg file on the host to /usr/local/etc/haproxy/haproxy.cfg name the container myhaproxy (each container name must be unique) the container is using the haproxy version 1.5 Docker Image ./start-haproxy.sh curl localhost:8443 :::html <!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\"> Directory listing for / Directory listing for / bar.txt curl localhost:8443 :::html <!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\"> Directory listing for / Directory listing for / foo.txt HAProxy Stats sudo docker rm -f myhaproxy /opt/mydata/haproxy.cfg global debug defaults log global mode http timeout connect 5000 timeout client 5000 timeout server 5000 listen http_proxy :8443 mode tcp balance roundrobin server srv1 docker:8000 check server srv2 docker:8001 check # optional section to enable statistics for haproxy protected by basic auth (replace with your own user and password) listen stats :1936 stats enable stats uri / stats realm HAProxyStatistics stats auth user:password ./start-haproxy.sh nginx forward proxy client -> forward proxy (nginx) -> all other sites docker pull nginx:alpine nginx.conf worker_processes 1 ; events { worker_connections 1024 ; } http { include mime . types ; default_type application / octet - stream ; sendfile on ; keepalive_timeout 65 ; gzip on ; server { listen 8080 ; location / { resolver 8 . 8 . 8 . 8 ; proxy_pass http :// $ http_host $ uri $ is_args $ args ; } error_page 500 502 503 504 / 50x .html ; location = / 50x .html { root html ; } } } docker run -it --rm -p 8080 :8080 -v / tmp / nginx .conf :/ etc / nginx / nginx .conf :ro --name mynginx nginx :alpine bind to port 8080 on the host and run an ephemeral container based on the alpine linux with nginx image that uses the /tmp/nginx config http_proxy=127.0.0.1:8080 curl example.com set the linux operating system proxy environment just for this one curl command and see the dockerized nginx forward proxy log show: 172.17.0.1 - - [06/May/2016:22:37:22 +0000] \"GET HTTP://example.org/ HTTP/1.1\" 200 1270 \"-\" \"curl/7.35.0\" Configure your browser (firefox) to use 127.0.0.1:8080 as the proxy for all protocols and watch the log statements fly by when you test http://example.com NOTE: this does not support HTTPS http://forum.nginx.org/read.php?2,15124,15256#msg-15256 Attempting https://example.com will return \"CONNECT example.com:443 HTTP/1.1\" 400 173 \"-\" \"-\" and the browser will show \"Server not found\" Warning do not use http_proxy=http://127.0.0.1:8080 as that will fail =[ You can permanently set the environment proxy with export http_proxy=127.0.0.1:8080 HAProxy as a limited outbound proxy sudo docker pull haproxy:1.6-alpine Now that you have the 10MB haproxy image... Note there is a limitation to haproxy in that it always assumes a syslog facility (no direct logging to stdout or files) https://github.com/dockerfile/haproxy/issues/3 http://www.gnu.org/software/libc/manual/html_node/Overview-of-Syslog.html /opt/mydata/haproxy.cfg This is the more readable config style which separates the frontend from backend and this is haproxy 1.6 The logging probably does not work =( global debug log /dev/log local0 info defaults mode http timeout connect 5s timeout client 5s timeout server 5s frontend myfrontend bind *:8443 default_backend mybackend backend mybackend server s1 example.com:443 ssl verify none starting the haproxy docker container docker run -p 8443:8443 --rm -it -v /home/admin/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro -v /dev/log:/dev/log --name myhaproxy haproxy:1.6-alpine Control + C to quit more info https://registry.hub.docker.com/_/haproxy/ https://cbonte.github.io/haproxy-dconv/configuration-1.5.html https://cbonte.github.io/haproxy-dconv/configuration-1.6.html http://www.haproxy.org/git?p=haproxy-1.6.git;a=tree;f=examples http://docs.docker.com/reference/commandline/cli/#adding-entries-to-a-container-hosts-file","tags":"virtualization","title":"HAProxy in Docker"},{"url":"https://blog.john-pfeiffer.com/caching-data-and-common-gotchas-and-an-intro-to-redis-memcached-and-varnish/","text":"Caching is when you use a copy of a data set rather than using the source. Caching often involves a \"Key Value Lookup\": A request is received and the service checks the cache using a Key The cache does not contain the Key The service generates the result from the originating data source (i.e. database) The service then stores the result in the cache with the Key as the index (and the result as the Value) A request is received and the service checks the cache using a Key The cache does contain the Key The service retrieves the Value from the cache and returns the result A more concrete example would be to cache a User object by Email, so that whenever a request came in for a particular Users details the cache would contain their Name, Address, and Phone Number. https://en.wikipedia.org/wiki/Associative_array Why Cache? A tradeoff of memory for cpu (or latency or some other business cost). accessing the data from source is too slow the data actually comes from multiple sources (complex and expensive to retrieve) to reduce load on the service originating data to reduce contention (i.e. reads and writes) for a client-server architecture, caching on the client reduces the number of required connections to a server server side caching can protect backend resources and improve throughput and performance comp Questions to ask when caching Is the complexity of caching worth the performance gain? (simpler is often better) Does my cache need to be consistent? (meaning the cache and data source return identical results) Can my cache be \"eventually consistent\"? (meaning a wrong answer for some specified period of time is ok) Am I caching at a high level? (meaning aggregating a lot of work/responses from lower level systems) Am I caching at a low level? (meaning inside of my Data Access Object pattern I'm protecting a single simple resource, i.e. a MySQL table, from being accessed too often) How unique are my Keys in my cache (i.e. if multiple users can have the same identifier it would be very bad to return the wrong session to the wrong user) How to Cache Cache on Write Also known as \"cache on write through\" Whenever new data is written a cache must also be updated. Cache on Read Also known as \"cache on read through\" Whenever a query is made first the cache is checked. If there is a \"cache miss\" then the data source is queried and the cache is updated and the result is returned. If there is a \"cache hit\" and the data is in the cache then it is returned (and potentially a cache key expiration updated as this cache hit improved the cache efficiency). Common Gotchas Caching is challenging because of the need for data consistency, parallel requests, and race conditions. One good way to think about it is a banking system with money: if two people both try and empty an account at an ATM at the same time how will your caching system handle it? Cache on write gotchas One implementation flaw is to update the cache first; if the update to the data source fails then some requests may have been given incorrect data. Another flaw would be to not have a \"transaction\" defined around both the update of the data and then the update of the cache since if either fails future requests will receive inconsistent results. There may be a design mismatch as since data is only cached on write, if reads are occurring mostly on data written a long time ago they will be expired/pushed out and you will have poor cache efficiency. While \"cache on write\" is a sometimes band-aid for NoSQL \"eventual consistency\" when it fails (i.e. all applications should expect that a cache will not exist or have a cache miss) the result may be data inconsistency. One workaround is \"check and set\" (or \"compare and set\") where the cache will auto-invalidate if two conflicting entries are attempted. http://neopythonic.blogspot.com/2011/08/compare-and-set-in-memcache.html Expiration: cache full of stale junk A naive implementation of caching will store every result in the cache forever. While this seems like a good idea (\"The cache application/service will just evict unused items based some algorithm\") it is essentially forcing your cache to be full of potentially low value information on the hope that someone else will solve the problem. Since some caching tools/framework do not set a default Time To Live or Expiration and in that case all of your data may quickly fill up the cache (not a bad thing per se), but then it will use whatever default or global \"eviction policy\" that is defined. Applying business logic and empirical data to pick sane expiration values might not only improve cache performance but may protect your service from security issues or bugs due to serving really stale data. e.g. for security reasons, caching a session \"forever\" is a bad idea as an attacker may get access to an old client cache or token and be able to impersonate a legimate user Set a TTL or Expiration whenever possible that matches your domain (i.e. for a session 1 day or 1 week). If the Time to Live is too short then the cache may have very poor efficiency (items expire before they can generate even one cache hit) Cold Cache and the Thundering Herd If the cache is \"cold\", i.e. has not been populated, then all queries will go directly to the source If the source is not prepared for the \"thundering herd\" of requests (that were usually handled by the cache) then the source may become overloaded and bad things will happen It is therefore best practice to \"warm the cache\" by seeding data from the source into the cache before significant load events Tools for caching Much like encryption it is probably a good idea to use a time tested product over writing your own implementation. Redis Examples http://redis.io/commands Interactive Redis Prompt redis-cli keys * Non Interactive Redis Commands redis-cli KEYS *:* redis-cli KEYS session:1:* redis-cli hgetall session:1:web redis-cli hgetall session:1:web:presence redis-cli KEYS \"session:3:*\" | xargs redis-cli DEL # then upgrade --restart redis-cli KEYS session:1:* | grep session:1:web-48 # user_session.py , when the hardcoded max of 10 simultaneous sessions is hit no more can be created session:1:web-48679:rooms session:1:web-48679:presence session:1:web-48679:message_ids session:1:web-48679 redis-cli zrange sessions:1 0 9 1) \"1:web\" 2) \"1:web-48679\" # remove a session manually? redis-cli zrem sessions:1 1:web-48679 redis-cli del session:1:web-48679:rooms redis-cli del session:1:web-48679:presence redis-cli del session:1:web-48679:message_ids redis-cli del session:1:web-48679 Installing Redis http://redis.io/topics/quickstart http://packages.ubuntu.com/trusty/redis-server (sudo apt-get install redis-server) redis-cli -h example.com ping PONG , aka verify a remote server connectivity Redis Clients http://redis.io/clients#python pip install redis https://pypi.python.org/pypi/redis import redis r = redis . StrictRedis ( host = 'localhost' , port = 6379 ) r . flushall () Memcached http://memcached.org https://en.wikipedia.org/wiki/Memcached Varnish https://www.varnish-cache.org/about REST web caching","tags":"programming","title":"Caching data and common gotchas and an intro to redis memcached and varnish"},{"url":"https://blog.john-pfeiffer.com/subunit-and-subunit2junitxml-to-get-junitxml-test-result-output-from-unittest/","text":"Test results from differing systems or multiple test runs need a common format. JUnit XML is almost a de facto standard for test results given almost all major Continuous Integration products support it. https://confluence.atlassian.com/display/BAMBOO/JUnit+parsing+in+Bamboo http://www.fossology.org/projects/fossology/wiki/Junit_xml_format http://pytest.org/latest/usage.html Setup pip install python-subunit junitxml assuming virtuelnv and myenv/bin/activate , junitxml is a hidden dependency :( Do not use apt-get install subunit as even with 14.04 Ubuntu it has an older version does not contain timings and subunit2junitxml creates \"skip\" instead of \"skipped\" Example UnitTest Class import unittest class john ( unittest . TestCase ): def test_success ( self ): self . assertTrue ( True ) def test_fail ( self ): self . assertTrue ( False ) @unittest.skipIf ( True , 'always skip' ) def test_skip ( self ): self . assertTrue ( False ) Example Usage One Liner python -m subunit.run foo | subunit2junitxml --no-passthrough --output-to test-results forward = non-subunit output will be encapsulated in subunit Intermediate Subunit Results File python -m subunit.run test_some_filename_with_py_truncated > test-results.subunit Do not use python -m subunit.run test_some_filename_with_py_truncated to stdout as it expects to have binary delimiters which screw up the console command line subunit-ls < test-results.subunit subunit-stats < test-results.subunit python -m subunit.run foo >> test-results.subunit append some more test results subunit-stats < test-results.subunit subunit2junitxml --no-passthrough --output-to test-results.xml < test-results.subunit no passthrough does not pass/convert any extraneous non subunit data/lines to the junit xml <testsuite errors= \"0\" failures= \"1\" name= \"\" tests= \"3\" time= \"0.001\" > <testcase classname= \"john.john\" name= \"test_fail\" time= \"0.000\" > <failure type= \"testtools.testresult.real._StringException\" > _StringException: Traceback (most recent call last): File \"john.py\", line 9, in test_fail self.assertTrue(False) File \"/usr/lib/python2.7/unittest/case.py\", line 424, in assertTrue raise self.failureException(msg) AssertionError: False is not true </failure> </testcase> <testcase classname= \"john.john\" name= \"test_skip\" time= \"0.000\" > <skipped> always skip </skipped> </testcase> <testcase classname= \"john.john\" name= \"test_success\" time= \"0.000\" /> </testsuite> Twisted UnitTesting `trial --reporter=subunit foo | subunit2junitxml --forward --output-to=junitxml-result.xml Troubleshooting ImportError: No module named 'junitxml' You may not have installed the junitxml module which subunit apparently sometimes depends on: pip install junitxml use sudo only if not using virtualenv AttributeError: 'AutoTimingTestResultDecorator' object has no attribute 'errors' This occured becaused TestSomeClass(unittest.TestCase) definition had an errors property/attribute which resulted in a namespace collision =( Empty results like this: <testsuite errors=\"0\" failures=\"0\" name=\"\" tests=\"0\" time=\"0.003\"> if you view/cat your results.subunit you will notice: test: directory.path.foobar.FooBar.test_constructor successful: directory.path.foobar.FooBar.test_constructor That is old subunit output (i.e. an old version of Twisted: trial --reporter=subunit), the new version 2 uses non printable characters instead of newlines (which sometimes ruins output to console) Resolution for old subunit version converted to new subunit version : trial --reporter=subunit foo | subunit-1to2 >> /tmp/results.subunit ; subunit2junitxml --no-passthrough --output-to test-results.xml < /tmp/test-results.subunit More Info http://www.tech-foo.net/making-the-most-of-subunit.html https://pypi.python.org/pypi/python-subunit https://pypi.python.org/pypi/junitxml https://launchpad.net/subunit","tags":"programming","title":"Subunit and Subunit2JunitXML to get JUnitXML test result output from UnitTest"},{"url":"https://blog.john-pfeiffer.com/virtualenv-python-interpreter-from-source/","text":"When building an application (including an external facing webapp or an internal test suite) it is critical to manage your dependencies. Virtualenv is a tool that keeps all the dependencies in a file system based container (and overcomes permissions based issues as well). To really isolate your application from the environment not only do you need a specific version of libraries (i.e. you know your application works fine with requests 2.4.3 and selenium 2.44) but additionally a specific version of the Python Interpreter. Build python from source wget https://www.python.org/ftp/python/2.7.8/Python-2.7.8.tgz tar -xf Python-2.7.8.tar.gz cd Python-2.7.8 ./configure --prefix=/home/ubuntu/python --enable-unicode=ucs4 make && make altinstall /home/ubuntu/python/bin/python2.7 --version altinstall ensures we do not try to override the existing /usr/bin/python binary which can be important if you want python 2.7.3 and python 2.7.8 to exist side by side Optionally: echo 'alias py=\"/home/ubuntu/python/bin/python2.7\"' >> ~/.bashrc python3 from source wget https://www.python.org/ftp/python/3.4.2/Python-3.4.2.tgz tar -xf Python-3.4.2.tar.gz cd Python-3.4.2 ./configure --prefix=/opt/python3.4.2 && make -j$(nproc) && make altinstall ls -ahl /usr/local/bin | grep 3.4 /usr/local/bin/pip3.4 install --upgrade virtualenv If you mess up your OS level python apt-get install python3 python3 depends on dh-python; however: Package dh-python is not configured yet. Look at the stack trace, reinstalling may not have put all of the helper directory and .py files in place File \"/usr/lib/python3.4/site.py\", line 586, in <module> ImportError: No module named '_sysconfigdata_m' mv /usr/lib/python3.4 /usr/lib/python3.4-OLD wget http://mirrors.kernel.org/ubuntu/pool/main/d/dh-python/dh-python_1.20140128-1ubuntu8_all.deb dpkg -i --force-depends dh-python_1.20140128-1ubuntu8_all.deb apt-get install python3 And in my case I needed to reinstall loads of python3 stuff: apt-get install xubuntu-destkop Installing virtualenv sudo pip install --upgrade virtualenv getting the latest version of virtualenv as any OS packages are likely to be outdated alternatively you can go all out and just use virtualenv locally from source virtualenv from source wget https://pypi.python.org/packages/source/v/virtualenv/virtualenv-X.X.tar.gz tar -xf virtualenv-X.X.tar.gz cd virtualenv-X.X /home/ubuntu/python/bin/python2.7 virtualenv.py myvenv does not require sudo and works around path or permissions requirements Example Usage virtualenv myvenv creates a local copy of required files like the python interpreter and its own version of pip myvenv | -- bin | -- activate | -- easy_install | -- pip | -- python | -- include | -- lib | -- python2.7 | -- site-packages | -- pip | -- setuptools | -- local myenv/bin/pip install --upgrade requests no sudo was required to add locally myvenv/lib/python2.7/site-packages/requests myenv / bin / python >>> import requests >>> print requests . __version__ Using virtualenv with a specific python version or binary virtualenv -p python3.5 venv source venv/bin/activate python --version which pip Python 3.5.0+ the shortest example to use the OS python3.5 binary when creating the venv directory with the virtual environment virtualenv --python=/home/ubuntu/python/bin/python2.7 myvenv myenv/bin/python Python 2.7.8 the advanced example uses a python binary that was created from source to ensure the application does not suffer when the OS has a python upgrade (or your libraries need a newer version of python than provided) virtualenv --version just in case your version of virtualenv has a bug and needs to be upgraded first activate and deactivate to update your environment temporarily Rather than using the explicit paths (which is the most clear but cumbersome) you can override your shell Environment: /usr/bin/python --version 2.7.3 source myenv/bin/activate python --version 2.7.8 pip install requests no sudo was required to add locally myvenv/lib/python2.7/site-packages/requests deactivate More Info When using git make sure .gitignore contains the \"myenv\" directory as you do not want to store these binaries in version control. Typically Heroku or other PaaS allow you to specify a python interpreter version and library requirements in a configuration file. http://virtualenv.readthedocs.org/en/latest/virtualenv.html https://www.python.org/downloads https://www.digitalocean.com/community/tutorials/common-python-tools-using-virtualenv-installing-with-pip-and-managing-packages","tags":"build-CI-CD-devops","title":"Virtualenv Python Interpreter from source"},{"url":"https://blog.john-pfeiffer.com/xubuntu-hotkey-shortcuts-and-zoom-also-for-xfce/","text":"Interacting with the computer is so much faster with keyboard hotkey shortcuts and other tricks, these are applicable as of Xubuntu 14.04 Xubuntu Zoom (magnifier) Alt + scrollwheel up to Zoom in or on a laptop with a touchpad two finger swipe up Alt + scrollwheel down to Zoom out or on a laptop with a touchpad two finger swipe down Hotkey Application Shortcuts Mouse (start button) -> Settings -> Settings Manager -> Keyboard -> Application Shortcuts Add -> sh -c \"sleep 1 && xset dpms force off\" -> OK (will open another popup) press control + alt + q on the keyboard (the popup will go away and the hotkeys will be saved) Double click on the command in the Command column to edit the command Double click on the hotkeys in the Shortcut column to modify the hotkey combination sleep 1 && xset dpms force off control + alt + q exo-open --launch TerminalEmulator control + alt + t gnome-calculator control + alt + g /usr/bin/galculator control + alt + g /usr/bin/leafpad --tab-width=4 control + alt + f vi /usr/share/applications/leafpad.desktop Exec=leafpad --tab-width=4 %f /usr/bin/chromium-browser control + alt + a /usr/bin/filezilla control + alt + i /opt/pycharm/bin/pycharm.sh control + alt + p xfce4-screenshooter -r printscreen select region (-f fullscreen , -w active window) amixer set Master 5%- -q Alt + Down amixer set Master 5%+ -q Alt + Up xflock4 control + alt + delete retext control + alt + r /home/ubuntu/.config/xfce4/xfconf/xfce-perchannel-xml/xfce4-keyboard-shortcuts.xml /etc/xdg/menus/xfce-applications.menu XFCE Keyboard Shortcuts Settings Editor -> xfce4-keyboard-shortcuts commands -> custom... New /commands/custom/ f leafpad /xfwm4/custom/ d show_desktop_key /commands/custom/ e mousepad exo-open --launch FileManager Ubuntu Keyboard Shortcuts ubuntu 12.04 keyboard shortcuts System Settings -> Keyboard (may not be visible so type it in the search box) -> Shortcuts Either modify an existing shortcut (i.e. disable one that is annoying) OR Custom Shortcuts -> + (add a new one) e.g. name : chromium command : /usr/bin/ chromium - browser Then highlight the row (clicking on the right area where it's \"disabled\") and type the key combination desired to trigger the shortcut (e.g. control + alt + a) HINTS: sudo find / -iname \"*chromium*\" sudo which chromium-browser","tags":"linux","title":"Xubuntu hotkey shortcuts and Zoom (also for xfce)"},{"url":"https://blog.john-pfeiffer.com/mobile-edit-cloud-execution-of-python-code/","text":"Haven't you just wanted to work through a coding kata http://codekata.com or puzzle on your phone? Python is a great language for getting stuff done, and while there are some mobile apps often they are limited by the platform (eg ios sans file system). Using the link from a Dropbox text file and a linode server (could be openshift red hat cloud?)... start running the script on the remote server that waits for new code to execute python myflaskapp.py & I can edit using Nocs Nocs syncs from iOS to Dropbox hitting a URL in my browser the python script downloads the latest version of the code (using shell to curl as Dropbox use javascript to authorize and return a link to the latest version) executes using the remote python environment returns the output Note! This may be dangerous as hackers could exploit to run arbitrary code, use at your own risk. *Also, clearly, downloading from Dropbox using curl is a hack with no guarantee of future support =] from flask import Flask import os from subprocess import Popen , PIPE import urllib2 app = Flask ( __name__ ) @app.route ( '/mysecreturl' ) def update_and_run (): output = '' try : name = 'exercises.py' path = '/var/www/mystuff' urlpath = 'https://www.dropbox.com/s/ancdefgrandom/exercises.py?dl=0' os . system ( 'curl --silent --location --insecure --output exercises.py https://www.dropbox.com/s/ancdefgrandom/exercises.py?dl=0' ) output = Popen ([ \"python\" , name ], stdout = PIPE ) . communicate ()[ 0 ] except Exception as error : return str ( error ) return output if __name__ == '__main__' : app . run ( '0.0.0.0' , 8080 , use_reloader = True ) Flask Application on OpenShift git clone https://github.com/openshift/flask-example.git Use the OpenShift WebUI to create an application On the right of your application the WebUI has a note on how to clone the default repo: git clone ssh://12345random@appname-domain.rhcloud.com/~/git/appname.git/ cd appname git remote add upstream -m master git://github.com/openshift/flask-example.git git pull -s recursive -X theirs upstream master git push cd wsgi/ virtualenv venv vi myflaskapp.py from flask import Flask import os from subprocess import Popen , PIPE import urllib2 app = Flask ( __name__ ) @app.route ( \"/\" ) def hello (): return \"Hello World!\" @app.route ( '/mysecreturl' ) def update_and_run (): output = '' try : path = '/var/lib/openshift/12345appid/app-root/data/exercises.py' urlpath = 'https://www.dropbox.com/s/12345random/exercises.py?dl=0' os . system ( 'curl --silent --location --insecure --output ' + path + ' ' + urlpath ) output = Popen ([ \"python\" , path ], stdout = PIPE ) . communicate ()[ 0 ] # output = Popen([\"touch\",path], stdout=PIPE).communicate()[0] except Exception as error : return str ( error ) return output if __name__ == \"__main__\" : app . run () More Info https://www.openshift.com/blogs/beginners-guide-to-writing-flask-apps-on-openshift","tags":"programming","title":"Mobile edit cloud execution of python code"},{"url":"https://blog.john-pfeiffer.com/fix-fn-screen-brightness-ubuntu-1404-intel-graphics/","text":"I discovered post upgrade that Ubuntu 14.04 has a glaring bug with the Intel graphics card (which was working fine in 12.04), the Fn key no longer controlled the brightness. sudo su ls /sys/class/backlight if it lists intel_backlight then this solution of adding the following should work for you too... vi /usr/share/X11/xorg.conf.d/20-intel.conf Section \"Device\" Identifier \"card0\" Driver \"intel\" Option \"Backlight\" \"intel_backlight\" BusID \"PCI:0:2:0\" EndSection Log out and log back in, function keys should now control the brightness again (no more glaring bug!)","tags":"linux","title":"Fix Fn screen brightness Ubuntu 14.04 intel graphics"},{"url":"https://blog.john-pfeiffer.com/ubuntu-bootable-usb-apt-get-and-dpkg-and-the-best-packages-to-install/","text":"If you have a modern computer (BIOS) that can boot from USB it is well worth it since having the latest Ubuntu ISO on DVD tends to pile up extra plastic. After setting up the Operating System you will need to install some software (packages). And if you have an SSD drive you will want to optimize your OS to not wear it out unnecessarily. Write an ISO to usb Be very careful with sudo or using the root user as this can permanently remove files or render your operating system inoperable. dd is a low level command that writes bytes directly without any prompts sudo su fdisk -l umount /dev/sdc1 dd if=/home/ubuntu/Desktop/ubuntu-14.04.1-server-amd64.iso of=/dev/sdc 1171456+0 records in 1171456+0 records out 599785472 bytes (600 MB) copied, 260.364 s, 2.3 MB/s fdisk allows you to see the device (e.g. an 8GB usb stick) dd will overwrite from the \"infile\" to the \"outfile\" so make sure you get that target location correct! Ubuntu Recovery mode (which is access to a single root user command line) boot in recovery mode by using the arrow keys during boot (down to select Recovery) mount networking root shell mount -o rw,remount / mount --all # might be needed too Now you can fix grub or /etc/passwd or free up some hard drive space package management with apt commands Debian has precompiled packages of binaries and libraries that can very easily be installed via the command line (or GUI) using Advanced Packaging Tool (APT) https://wiki.debian.org/Apt Since apt is a wonderful wrapper/manager of dpkg when you're in doubt most likely there is a dpkg command that will do what you need but it may take a lot of research and 8 parameters to do it ;] https://en.wikipedia.org/wiki/Advanced_Packaging_Tool Hint: Ubuntu is based upon Debian apt-cache apt-cache search ssh to find packages with the name ssh apt-cache search ssh | grep server if there are too many results pipe to grep to filter down the results apt-cache show ssh to show the details about a specific package apt-cache showpkg ssh to show more general info about a package apt-cache depends ssh to show the package dependencies Force Apt to use IPv4 to avoid lengthy IPv6 timeouts sudo apt-get -o Acquire : :ForceIPv4 = true update sudo apt-get -o Acquire : :ForceIPv4 = true install vim sudo echo 'Acquire::ForceIPv4 \"true\";' > / etc / apt / apt .conf.d / 99force-ipv4 update, then install vim, then save the persistent config to always use ipv4 apt-get updating package indices with apt-get update Apt contains indices that need to be updated from the upstream repositories /etc/apt/sources.list is the main ubuntu repository listing /etc/apt/sources.list.d is the directory where additional apt repositories can be added (usually from ppa or 3rd party vendors) http://www.debian.org/mirror/mirrors_full for the Debian package mirror sites http://packages.ubuntu.com/ for a web ui based search of package details apt-get clean /var/cache/apt/archive folder keeps a copy of the downloaded .deb files you will need an internet connection to download again any removed .deb files rm -rf /var/lib/apt/lists/* remove the indices in case they have gotten orphaned or corrupted, needs to be followed by apt-get update to repopulate apt-get update use /etc/apt/sources.list and /etc/apt/sources.list.d to update the package indices to determine if there are newer packages available deb file:///file_store/archive trusty main universe a snippet for how to configure apt to use a local repository (e.g. use reprepro to make a local mirror) sudo apt-get update -o Dir::Etc::sourcelist=\"sources.list.d/example.list\" -o Dir::Etc::sourceparts=\"-\" -o APT::Get::List-Cleanup=\"0\" update only a single repository apt-cache dump shows all installed packages To install netselect, a debian application that allows you to choose the \"best\" package mirror: sudo apt-get install netselect netselect-apt netselect-apt installing and force installing with apt sudo apt-get install --dry-run byobu simulate what will happen but do not change the system sudo apt-get install --download-only byobu packages are retrieved but not installed sudo apt-get install --yes byobu install and pre-emptively answer yes to the yes/no prompt sudo apt-get install --reinstall byobu reinstall even if the package is installed sudo apt-get install --fix-broken byobu attempt to fix broken dependencies DEBIAN_FRONTEND=noninteractive apt-get -y -o Dpkg::Options::=\"--force-confdef\" -o Dpkg::Options::=\"--force-confold\" install --reinstall byobu the most non interactive way to force install a package where all prompts are auto answered such that old configuration files are maintained http://manpages.ubuntu.com/manpages/precise/man8/apt-get.8.html https://help.ubuntu.com/community/AptGet/Howto apt-get upgrade upgrades to the latest version of existing packages, no new packages (so if the new version has new dependencies nothing happens) apt-get dist-upgrade upgrades to the latest version of existing packages and will try to grab any new dependencies as required apt-get install update-manager-core newer versions of ubuntu require a helper utility, http://packages.ubuntu.com/trusty/admin/update-manager-core Before you do a major upgrade of Ubuntu you should bring all packages to the latest version... (apt-get update && apt-get dist-upgrade) do-release-upgrade -f DistUpgradeViewNonInteractive non interactive upgrade to a new version of Ubuntu (hold onto your seat!), often requires a reboot after for kernel upgrades lsb_release -a cat /etc/lsb_release uname -a verify that your system has been upgraded (kernel upgrades often require a reboot to become loaded in memory)) removing packages with apt apt-get remove wget uninstall a package apt-get purge wget remove the package and all files from disk apt-get autoremove attempt to clean up packages that are no longer needed (i.e. old versions of dependencies or unused kernel images) apt-key sudo apt-key update if apt errors: WARNING: The following packages cannot be authenticated dpkg really manages everything Underneath apt is dpkg (and similar tools) which actually does all of the hard work but are sometimes hard to use =) listing and finding packages with dpkg dpkg -l lists all of the packages installed (name, version, architecture, description) dpkg -l | grep foobar lists all of the packages but filters for something specific (i.e. a prefix or partial match) dpkg -l packagename > myoutput.txt lists whether a specific package is installed or not and redirects the output to a file dpkg --get-selections lists the package names and the state (installed, uninstalled, etc.) dpkg-query -f '${binary:Package}\\n' -W lists just the package names, slightly more convenient is apt-cache pkgnames | sort dpkg -S stdio.h find a package that contains a specific file dpkg -c packagename.deb list the contents of the .deb file https://wiki.debian.org/ListInstalledPackages you can also manually inspect /var/lib/apt and /var/lib/dpkg dpkg logs vi /var/log/dpkg.log tail -f /var/log/dpkg.log in conjunction with apt-get upgrade -y Installing and removing packages with dpkg dpkg -i packagename.deb install the .deb file, dpkg -i *.deb will install all of the .deb files in the current directory dpkg -i --force depends packagename.deb installs and turns a dependency error into a warning (i.e. libc6 circular dependency) dpkg -L packagename list the locations of the installed files dpkg -s packagename shows if the package is installed and information about it, dpkg -s | grep Version or dpkg -l | awk '$2==\"packagename\" { print $3 }' to only print the version (if it exists) dpkg -r packagename.deb remove a package but leave the configuration files, also known as dpkg --remove dpkg --purge remove a package and delete all configuration files (even if they have been customized by the user) dpkg --force-help to manually install a package (forcefully if synaptic and apt-get are stuck) mv /var/lib/dpkg/info/postgresql.* /tmp/ dpkg --remove --force-remove-reinstreq postgresql-9.1 do the same for postgresql-common and other packages apt-get install postgresql-9.1 apt-get purge postgresql-9.1 postgresql-client-9.1 postgresql-common postgresql-client-common in order to have apt-get remove all of the binaries Best Ubuntu Packages as of Utopic 14.10 sudo apt-get update sudo apt-get install -y byobu build-essential elinks unzip unrar nano vim wget curl ntp rcconf dialog git-core sudo apt-get install -y python-pip && sudo pip install --upgrade pip pip is the package manager for python packages (different from the debian OS packages) so useful if you do any python development or run python applications An alternative to the usually stale OS pip version is to use the not entirely secure grab the .py file from the internet and run it... wget -qO- https://bootstrap.pypa.io/get-pip.py | sudo python openssh-server libssl = the secure remote shell service and encryption dependency http://packages.ubuntu.com/search?keywords=openssh-server build-essential = tools for compiling and building debian packages http://packages.ubuntu.com/lucid/build-essential byobu = console terminal multi screen (survives network disconnects) http://byobu.co wget and curl = utilities to download files elinks = cli browser (just in case your GUI dies and you need to research) http://kmandla.wordpress.com/2011/01/13/a-comparison-of-text-based-browsers unzip and unrar = utilities to decompress compressed things nano = a simple text editor (much easier than vi/vim for just writing new text) ntp = network time protocol client daemon to keep your clock in sync rcconf = easier way to manage what services start at boot https://packages.debian.org/jessie/rcconf dialog = user friendly dialog boxes for shell scripts (dependency for rcconf) git-core = the distributed version control software that is eating the developer world python-setuptools = Sometimes required to install pip http://pythonhosted.org/setuptools icedtea = open java (plugin = browser java) sudo apt-get install openconnect network-manager-openconnect network-manager-openconnect-gnome openvpn = opensource vpn client https://openvpn.net openconnect = opensource compatible with cisco anyconnect vpn https://en.wikipedia.org/wiki/OpenConnect jdk = java development kit iced-tea-7-plugin = open source java 7 support for browsers sudo apt-get install openjdk-8-jre for just the java runtime (thank goodness not Oracle Sun) openjdk-8-jdk for the full java development kit - needed for some packages to run correctly GUI Xubuntu Desktop is my preferred \"lightweight\" GUI for Ubuntu: http://xubuntu.org apt-get install -y chromium-browser pepperflashplugin-nonfree geany keepassx xdiskusage apt-get install -y arandr rdesktop chromium-browser = opensource branch/clone of google chrome browser, maybe srware.net with privacy badger and adblock plus (fanboy block lists) too? geany = tabbed text notepad (with syntax highlighting) keepassx = secure password inventory (has a mini version for iphone as well) xdiskusage = graphical view of disk space usage by folder and file arandr = multi display gui config rdesktop = RDP client grdesktop = gnome UI for rdesktop sudo echo \"autologin-user=ubuntu\" >> /etc/lightdm/lightdm.conf.d/10-xubuntu.conf Better yet use the UI and just choose auto login ;) TODO: Disable guest user , *Disable crash reports: apport * dropbox = cloud file storage deb http://linux.dropbox.com/ubuntu utopic main sudo apt-key adv --keyserver pgp.mit.edu --recv-keys 5044912E deb http://downloads.hipchat.com/linux/apt stable main wget -O - https://www.hipchat.com/keys/hipchat-linux.key | apt-key add - apt-get update; apt-get install dropbox hipchat filezilla = file transfer protocol client (that supports sftp = secure ssh ftp) music and video sudo apt-get install ubuntu-restricted-extras vlc vlc = movies/music ubuntu-restricted-extras = all of the encumbered with licenses packages to generally just watch or listen to stuff :( spotify in ubuntu 15.04 https://www.spotify.com/us/download/linux/ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys D2C19886 trust the spotify repository libnss3-1d : Depends: libnss3 (= 2:3.17.4-0ubuntu1) but 2:3.19.2-0ubuntu15.04.1 is to be installed spotify is behind the times or only wants to support 14.04 and LTS releases =( https://launchpad.net/ubuntu/vivid/amd64/libnss3/2:3.17.4-0ubuntu1 https://launchpad.net/ubuntu/wily/amd64/libnss3/2:3.19.2.1-0ubuntu0.15.10.1 dpkg -i libnss3_3.17.4-0ubuntu1_amd64.deb apt-get install spotify-client apt-get -f install spotify: error while loading shared libraries: libgcrypt.so.11: cannot open shared object file: No such file or directory what fun, the internet explains 15.04 (vivid) and 15.10 (wily) use the new libgcrypt20 so... wget https://launchpad.net/ubuntu/+archive/primary/+files/libgcrypt11_1.5.3-2ubuntu4.2_amd64.deb dpkg -i libgcrypt*.deb apt-get install --reinstall spotify-client pithos is an open source pandora client sudo add-apt-repository ppa:pithos/ppa sudo apt-get install pithos more codecs and DVD playback sudo apt-get install ffmpeg gstreamer0.10-plugins-bad lame libavcodec-extra sudo /usr/share/doc/libdvdread4/install-css.sh Packages you will probably want to remove apt-get remove brltty unless you are using braille on your computer Other Useful Packages Heroku CLI deb http://toolbelt.heroku.com/ubuntu ./ wget -O- https://toolbelt.heroku.com/apt/release.key | apt-key add - apt-get install -y heroku-toolbelt Ruby and OpenShift CLI https://gorails.com/setup/ubuntu/14.04 sudo apt-get install git-core curl zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 \\ libxml2-dev libxslt1-dev libcurl4-openssl-dev python-software-properties libgdbm-dev libncurses5-dev automake libtool bison libffi-dev source ~/.rvm/scripts/rvm echo \"source ~/.rvm/scripts/rvm\" >> ~/.bashrc rvm install 2.1.2 rvm use 2.1.2 --default ruby -v echo \"gem: --no-ri --no-rdoc\" > ~/.gemrc gem install rhc red hat client for openshift SSD Optimization Write Logs to tmpfs instead of disk tmpfs ram (memory) virtual disk will just use memory (which I guess is overly abundant now) instead of wearing out the Solid State Drive sudo vi /etc/fstab # /etc/fstab: static file system information. # # Use 'blkid' to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # <file system> <mount point> <type> <options> <dump> <pass> # / was on /dev/sda2 during installation UUID=b7577587-22db-42f6-95d1-264a24f9dd90 / ext4 noatime,errors=remount-ro 0 1 tmpfs /tmp tmpfs defaults,noatime 0 0 tmpfs /var/tmp tmpfs defaults,noatime 0 0 tmpfs /var/log/apparmor tmpfs defaults,noatime 0 0 tmpfs /var/log/apt tmpfs defaults,noatime 0 0 tmpfs /var/log/cups tmpfs defaults,noatime 0 0 tmpfs /var/log/dist-upgrade tmpfs defaults,noatime 0 0 tmpfs /var/log/installer tmpfs defaults,noatime 0 0 tmpfs /var/log/lightdm tmpfs defaults,noatime 0 0 tmpfs /var/log/unattended-upgrades tmpfs defaults,noatime 0 0 the tmpfs disks created in my fstab were discovered through trial and error and will differ based on what applications are actually running (Xubuntu!) an older simpler example causes errors as applications create /var/log/SOMETHING directories during installation and then expect them on boot every time later /dev/sda1 / ext4 noatime,errors=remount-ro 0 1 tmpfs /tmp tmpfs defaults,noatime 0 0 tmpfs /var/tmp tmpfs defaults,noatime 0 0 tmpfs /var/log tmpfs defaults,noatime 0 0 here is a list of directories that probably need to be generated for dir in apparmor apt cups dist-upgrade fsck gdm installer news samba unattended-upgrades ; do mkdir -p /var/log/$dir done Customize Grub Boot Options I prefer seeing my bootup screens so I remove some but add the SSD enhancement vi nano /etc/default/grub GRUB_TIMEOUT=1 # GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\" GRUB_CMDLINE_LINUX_DEFAULT=\"elevator=noop\" sudo update-grub2 cat /boot/grub/grub.cfg root=UUID=f0ae2c59-83d2-42e7-81c4-2e870b6b255d ro quiet splash elevator=noop only prompt for 1 second remove the quiet so the console displays all of the boot information the noop scheduler is a simple FIFO scheduler which is usually optimal for SSD or virtual machines https://en.wikipedia.org/wiki/Noop_scheduler since any OS attempt at optimization may cnoflict with more accurate information from the Disk or Hypervisor update-grub2 is to apply the update https://help.ubuntu.com/community/Grub2 manually verify the changes by examining all of the boot menu options (i.e. find the noop line) cat /sys/block/sda/queue/scheduler [noop] deadline cfq list what schedulers are available , http://www.linuxhowtos.org/System/iosched.htm , note that noop is selected Note: the above command needs to be run as root, but sudo does not work with it on my system. Run sudo -i if you have a problem to get a root prompt.) Good Digital Ocean Droplet Tips useradd -s /bin/bash -m NEWUSERNAME usermod -a -G admin NEWUSERNAME passwd NEWUSERNAME mkdir -p /home/NEWUSERNAME/.ssh vi /home/NEWUSERNAME/.ssh/authorized_keys visudo #includedir /etc/sudoers.d NEWUSERNAME ALL = (ALL) NOPASSWD: ALL of course set the timezone to UTC and use network time protocol dpkg-reconfigure tzdata apt-get update apt-get install byobu vim ntp Secure SSH by removing root login with vi /etc/ssh/sshd_config: PermitRootLogin no , PasswordAuthentication no Optionally change the SSH port to something different from the default: Port 22 /etc/init.d/ssh restart Use the Digital Ocean Web UI to poweroff and take a snapshot of the fresh system","tags":"linux","title":"Ubuntu Bootable USB, apt-get and dpkg, and the best packages to install"},{"url":"https://blog.john-pfeiffer.com/listjs-sort-filters-search-and-more-for-html-lists-and-tables-in-javascript/","text":"Self contained javascript library to make lists of information awesome! http://listjs.com/docs listjs.html < ! DOCTYPE html > < html > < head > < script src= \"http://listjs.com/no-cdn/list.js\" ></script > < style type= \"text/css\" > . list { font - family : sans - serif ; margin : 0 ; padding : 20 px 0 0 ; } . list > li { display : block ; background - color : #eee ; padding : 10 px ; box - shadow : inset 0 1 px 0 #fff ; } . avatar { max - width : 150 px ; } img { max - width : 100 %; } h3 { font - size : 16 px ; margin : 0 0 0.3 rem ; font - weight : normal ; font - weight : bold ; } p { margin : 0 ; } input { border : solid 1 px #ccc ; border - radius : 5 px ; padding : 7 px 14 px ; margin - bottom : 10 px } input : focus { outline : none ; border - color : #aaa ; } . sort { padding : 8 px 30 px ; border - radius : 6 px ; border : none ; display : inline - block ; color : #fff ; text - decoration : none ; background - color : # 28 a8e0 ; height : 30 px ; } . sort : hover { text - decoration : none ; background - color : # 1 b8aba ; } . sort : focus { outline : none ; } . sort : after { width : 0 ; height : 0 ; border - left : 5 px solid transparent ; border - right : 5 px solid transparent ; border - bottom : 5 px solid transparent ; content : \"\" ; position : relative ; top : - 10 px ; right : - 5 px ; } . sort . asc : after { width : 0 ; height : 0 ; border - left : 5 px solid transparent ; border - right : 5 px solid transparent ; border - top : 5 px solid #fff ; content : \"\" ; position : relative ; top : 13 px ; right : - 5 px ; } . sort . desc : after { width : 0 ; height : 0 ; border - left : 5 px solid transparent ; border - right : 5 px solid transparent ; border - bottom : 5 px solid #fff ; </style > < meta charset = utf - 8 /> < title > Existing list</title > </head > < body > <div id= \"users\" > < input class= \"search\" placeholder= \"Search\" /> < button class= \"sort\" data - sort= \"name\" > Sort by name </button > < ul class= \"list\" > < li > < h3 class= \"name\" > Jonny Stromberg </h3 > < p class= \"born\" > 1986 </p > </li > < li > < h3 class= \"name\" > Jonas Arnklint </h3 > < p class= \"born\" > 1985 </p > </li > < li > < h3 class= \"name\" > Martina Elm </h3 > < p class= \"born\" > 1986 </p > </li > < li > < h3 class= \"name\" > Gustaf Lindqvist </h3 > < p class= \"born\" > 1983 </p > </li > </ul > </ div> < script type= \"text/javascript\" > var options = { valueNames: [ 'name' , 'born' ] }; var userList = new List ( 'users' , options ); </script > </body > </html > ListJS with Pelican and Jinja2 ListJS with the Pelican elegant theme to list all the articles sortable/searchable, default pagination for ListJS set to 1000 items {% extends \"base.html\" %} {% block title %} All Categories · {{ super() }} {% endblock title %} {% block head_description %} All categories of the {{ SITENAME|striptags }} blog. {% endblock head_description %} {% block content %} < head > < script src = \"http://listjs.com/no-cdn/list.js\" ></ script > < style type = \"text/css\" > h3 { font-size : 16px ; margin : 0 0 0 . 3 rem ; font-weight : normal ; font-weight : bold ; } p { margin : 0 ; } input { border : solid 1px #ccc ; border - radius : 5px ; padding : 7px 14px ; margin-bottom : 10px } input :focus { outline : none ; border-color : #aaa ; } .sort { padding : 8px 30px ; border - radius : 6px ; border : none ; display : inline - block ; color : #fff ; text-decoration : none ; background-color : #28a8e0 ; height : 30px ; } .sort :hover { text-decoration : none ; background-color : #1b8aba ; } .sort :focus { outline : none ; } .sort :after { width : 0 ; height : 0 ; border-left : 5px solid transparent ; border-right : 5px solid transparent ; border-bottom : 5px solid transparent ; content : \"\" ; position : relative ; top :- 10px ; right :- 5px ; } .sort.asc :after { width : 0 ; height : 0 ; border-left : 5px solid transparent ; border-right : 5px solid transparent ; border-top : 5px solid #fff ; content : \"\" ; position : relative ; top : 13px ; right :- 5px ; } .sort.desc :after { width : 0 ; height : 0 ; border-left : 5px solid transparent ; border-right : 5px solid transparent ; border-bottom : 5px solid #fff ; } </ style > </ head > < body > < div id = \"article-list\" > < button class = \"sort\" data-sort = \"date\" > Sort by date </ button > < button class = \"sort\" data-sort = \"title\" > Sort by title </ button > < input class = \"search\" placeholder = \"Search\" style = \"margin-top: 10px; height: 16px;\" /> < ul class = \"list\" > {% for category, articles in categories %} {% for article in articles %} < li > < span class = \"date\" style = \"padding-right: 10px;\" > < time pubdate = \"pubdate\" datetime = \"{{ article.date.isoformat() }}\" > {{ article.locale_date }} </ time > </ span > < a href = \"{{ SITEURL }}/{{ article.url }}\" >< span class = \"title\" > {{ article.title }} {%if article.subtitle %} < small > {{ article.subtitle }} </ small > {% endif %} </ span > </ a > </ li > {% endfor %} {% endfor %} </ ul > </ div > {% endblock content %} {% block script %} {{ super() }} < script language = \"javascript\" type = \"text/javascript\" > function uncollapse () { $ ( window . location . hash ). collapse ({ toggle : true }) } </ script > < script type = \"text/javascript\" language = \"JavaScript\" > uncollapse (); </ script > < script type = \"text/javascript\" > var options = { valueNames : [ 'date' , 'title' ], page : 1000 }; var hackerList = new List ( 'article-list' , options ); hackerList . sort ( 'date' ) </ script > {% endblock script %}","tags":"programming","title":"ListJS: Sort, Filters, Search and more for HTML lists and tables in Javascript"},{"url":"https://blog.john-pfeiffer.com/publish-a-pelican-blog-using-a-bitbucket-post-webhook/","text":"Webhooks are an incredibly useful way to tie together disparate network parts, WHEN something happens in one place, it sends a POST HTTP request to another place. Create the Bitbucket Webhook and Setup a Server to Receive the Webhook Log in to the Bitbucket WebUI Choose the repository Choose to administer the repository (gear symbol) -> Hooks (left menu) , or simply https://bitbucket.org/username/reponame/admin/hooks Select Hook Type (dropdown) , POST , Add Hook (Button) Enter your target URL, SAVE Setup a webserver (easiest might be Bamboo or Jenkins) somewhere Ensure there is a URL that accepts POST requests Ensure that when the POST is received it runs the pelican content generation commands to make the new output Ensure new output is visible You may notice any existing POST webhooks, i.e. a HipChat notification add-on, listed: https://hipchat-bitbucket.herokuapp.com/commit?client_id=f955ddb5 Flask and Bash source code to publish a pelican static web site This custom solution requires running that flask app manually, i.e. python mypublish.py It also requires having two repositories, one for the pelican source content, the other repo (i.e. a bitbucket static web site) will only contain the output (.html files) vi mypublish.py from flask import Flask import os from subprocess import Popen , PIPE app = Flask ( __name__ ) @app.route ( '/someuniquekeyhere' , methods = [ 'GET' , 'POST' ]) def mypublish (): try : output = Popen ([ \"./mypublish.sh\" ], stdout = PIPE ) . communicate ()[ 0 ] except Exception as error : return str ( error ) return output if __name__ == '__main__' : app . run ( '0.0.0.0' , 8443 , use_reloader = True ) vi mypublish.sh #!/bin/bash git pull GITMESSAGE = $( git log -n 1 ) OUTPUT = \"../outputreponame.bitbucket.org\" ./clean-output.sh \"../sourcereponame.bitbucket.org\" # removes all of the old content echo \" $GITMESSAGE \" pelican content cp -a ./output/* $OUTPUT rm -rf ./output rm -rf ./cache rm -f *.pyc cd \" $OUTPUT \" git add --all ./content git commit -m \"source $GITMESSAGE \" git push vi clean-output.sh #!/bin/bash rm -rf ./output rm -rf ./cache rm -f *.pyc for ITEM in $SOURCE /* do if [ -d \" $ITEM \" ] ; then rm -rf \" $ITEM \" else rm -f \" $ITEM \" fi done More info https://read-the-docs.readthedocs.org/en/latest/webhooks.html https://confluence.atlassian.com/bitbucket/manage-webhooks-735643732.html","tags":"programming","title":"Publish a pelican blog using a Bitbucket POST Webhook"},{"url":"https://blog.john-pfeiffer.com/using-vagrant-to-deploy-instances-on-aws/","text":"Vagrant is an infrastructure tool that simplifies deployment, such as virtual machines or in this case Amazon EC2 instances. Install Vagrant and the Vagrant AWS plugin Download and install vagrant: https://www.vagrantup.com/downloads wget https://releases.hashicorp.com/vagrant/1.7.4/vagrant_1.7.4_x86_64.deb dpkg -i vagrant_1.7.4_x86_64.deb vagrant --version vagrant plugin install vagrant-aws Quickstart Vagrant and VirtualBox with Ubuntu Trusty 14.04 The simple local VirtualBox method was: virtualbox --help vagrant init ubuntu/trusty64; vagrant up --provider virtualbox ==> default: Box 'ubuntu/trusty64' could not be found. Attempting to find and install... default: Box Provider: virtualbox default: Box Version: >= 0 ==> default: Loading metadata for box 'ubuntu/trusty64' default: URL: https://atlas.hashicorp.com/ubuntu/trusty64 this will do all the extra work for you of finding and downloading the \"box\" and starting it in VirtualBox Vagrant and AWS EC2 with Ubuntu Precise 12.04 vagrant box add dummy https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box https://atlas.hashicorp.com/boxes/search vagrant box list vi Vagrantfile Vagrant.configure(\"2\") do |config| config.vm.box = \"dummy\" config.vm.provider :aws do |aws, override| aws.access_key_id = \"YOURACCESSKEY\" aws.secret_access_key = \"YOURSECRETKEY\" aws.keypair_name = \"YOURKEYPAIRNAME\" aws.ami = \"ami-7747d01e\" aws.instance_ready_timeout = 300 aws.instance_type = \"m4.large\" aws.tags = { \"Name\" => \"MyCloudInstance\", } override.vm.box = \"dummy\" override.vm.box_url = \"https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box\" override.ssh.username = \"ubuntu\" override.ssh.private_key_path = \"./YOURKEYPAIRNAME.pem\" end end ubuntu/images/ubuntu-precise-12.04-amd64-server-20130204 - ami-7747d01e , no ebs storage - just instance storage , https://cloud-images.ubuntu.com/releases/ and https://atlas.hashicorp.com/boxes/search vagrant will by default upload all folders and files in your \"project\" folder where the Vagrantfile is located vagrant will start with the current working directory and look for a Vagrantfile, then go up one directory until it finds one: https://docs.vagrantup.com/v2/vagrantfile/ vagrant up --provider=aws --debug --debug is interactive mode and requires pressing enter between every step Bringing machine 'default' up with 'aws' provider... ==> default: HandleBoxUrl middleware is deprecated. Use HandleBox instead. ==> default: This is a bug with the provider. Please contact the creator ==> default: of the provider you use to fix this. ==> default: Warning! The AWS provider doesn't support any of the Vagrant ==> default: high-level network configurations (`config.vm.network`). They ==> default: will be silently ignored. ==> default: Launching an instance with the following settings... ==> default: -- Type: m4.large ==> default: -- AMI: ami-7747d01e ==> default: -- Region: us-east-1 ==> default: -- Keypair: YOURKEYPAIRHERE ==> default: -- Block Device Mapping: [] ==> default: -- Terminate On Shutdown: false ==> default: -- Monitoring: false ==> default: -- EBS optimized: false ==> default: -- Assigning a public IP address in a VPC: false ==> default: Waiting for instance to become \"ready\"... ==> default: Waiting for SSH to become available... ==> default: Machine is booted and ready for use! ==> default: Rsyncing folder: /home/ubuntu/myproject/ => /vagrant vagrant ssh the default-easiest-interface way of getting SSH access into the machine vagrant ssh-config vagrant ssh-config > vagrant-ssh ssh -F vagrant-ssh default alternative interactive ssh session: Use the HostName or AWS EC2 WebUI or http://aws.amazon.com/cli to discover the remote machine IP address using ssh with the vagrant-ssh file output seems the simplest ssh -i YOURKEYPAIRHERE.pem ubuntu@1.2.3.4 ls -ahl /vagrant exit vagrant ssh -c \"ls -ahl\" for a non interactive listing of the home directory vagrant ssh -c \"pidof ntpd | xargs sudo kill -9\" vagrant up , vagrant reload , and vagrant provision will have the AWS provider use rsync to push data to /vagrant https://docs.vagrantup.com/v2/synced-folders/rsync.html vagrant stop UnsupportedOperation => The instance 'i-1295bf39' does not have an 'ebs' root device type and cannot be stopped. (Fog::Compute::AWS::Error) vagrant destroy > default: Are you sure you want to destroy the 'default' VM? [y/N] Y > > ==> default: Terminating the instance... vagrant destroy -f non interactively destroy the instance and avoid the misleading error message: Vagrant is attempting to interface with the UI in a way that requires a TTY Vagrant provisioning Allows for automated installation of software bundled into the vagrant up command vagrant up --provider=aws --no-provision to prevent any provisioning config.vm.provision \"shell\", inline: \"echo Hello, World\" config.vm.provision \"shell\", path: \"script.sh\" config.vm.provision \"shell\", path: \"https://example.com/script.sh\" http://docs.vagrantup.com/v2/provisioning http://docs.vagrantup.com/v2/provisioning/shell.html Advanced Vagrantfile example # -*- mode: ruby -*- # vi: set ft=ruby : if ENV['UPDATEFQDN'] updatedfqdn=ENV['UPDATEFQDN'] end $ fqdnscript = <<FQDNSCRIPT echo \"I am updating fqdn to #{ updatedfqdn } ...\" cat /etc/hosts | grep \" #{ updatedfqdn } \" || sudo sed 's/127.0.0.1/127.0.0.1 #{ updatedfqdn } /' -i /etc/hosts hostname | grep \" #{ updatedfqdn } \" || sudo hostname #{ updatedfqdn } FQDNSCRIPT Vagrant.configure(\"2\") do |config| config.vm.box = \"dummy\" config.vm.provider :aws do |aws, override| aws.access_key_id = \"YOURACCESSKEY\" aws.secret_access_key = \"YOURSECRETKEY\" aws.keypair_name = \"YOURKEYPAIRNAME\" aws.ami = \"ami-7747d01e\" aws.instance_ready_timeout = 300 aws.instance_type = \"m4.large\" aws.tags = { \"Name\" => \"MyCloudInstance\", } if ENV['bamboo_aws_use_iops'] aws.block_device_mapping = [ { 'DeviceName' => '/dev/sda1', 'Ebs.VolumeSize' => 100, 'Ebs.VolumeType' => 'io1', 'Ebs.Iops' => 3000 }] else aws.block_device_mapping = [ { 'DeviceName' => '/dev/sda1', 'Ebs.VolumeSize' => 16, 'Ebs.VolumeType' => 'gp2' }] end override.vm.box = \"dummy\" override.vm.box_url = \"https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box\" override.ssh.username = \"ubuntu\" override.ssh.private_key_path = \"./YOURKEYPAIRNAME.pem\" override.vm.synced_folder \"/etc/sslcerts\", \"/tmp/sourcecode\", type: \"rsync\", create: true, rsync__exclude: \".git/\" end config.vm.provision :shell, :inline => \"echo `hostname -f` >> /home/ubuntu/currenthostname.txt\" config.vm.provision :shell, :inline => $ fqdnscript end Bringing machine 'default' up with 'aws' provider... ==> default: Running provisioner: shell... ==> default: Running: inline script stdin: is not a ttty I am provisioning and updating hostname... synced_folder is to sync other folders in your filesystem besides the folder with the Vagrantfile config.vm.hostname does not appear to work on AWS EC2 so the workaround above (|| statements to prevent extra reconfiguration) https://github.com/mitchellh/vagrant-aws http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html Vagrant and EC2 VPC (AMI that does not have/allow sudo) # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\"2\") do |config| config.vm.box = \"dummy\" config.ssh.pty = true config.vm.synced_folder \".\", \"/vagrant\", disabled: true config.vm.provider :aws do |aws, override| aws.access_key_id = \"YOURACCESSKEY\" aws.secret_access_key = \"YOURSECRETKEY\" aws.keypair_name = \"YOURKEYPAIRNAME\" aws.ami = \"ami-7747d01e\" aws.instance_ready_timeout = 300 aws.instance_type = \"m4.large\" aws.tags = { \"Name\" => \"MyCloudInstance\", } aws.security_groups = [ \"my_aws_security_group_id\" ] aws.subnet_id = \"my_aws_subnet_id\" aws.associate_public_ip = true aws.user_data = \"#cloud-boothook\\n#!/bin/bash\\ntouch /opt/.license/.eula\\n\" override.vm.box = \"dummy\" override.vm.box_url = \"https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box\" override.ssh.username = \"ubuntu\" override.ssh.private_key_path = \"/home/ubuntu/my_aws.pem\" end end vagrant up --provider=aws --no-provision --debug INFO vagrant: `vagrant` invoked: [\"up\", \"--provider=aws\", \"--no-provision\", \"--debug\"] Bringing machine 'default' up with 'aws' provider... ==> default: Launching an instance with the following settings... INFO interface: info: -- Type: m4.large INFO interface: info: ==> default: -- User Data: #cloud-boothook INFO interface: info: ==> default: -- Assigning a public IP address in a VPC: true ==> default: Waiting for instance to become \"ready\"... ==> default: Waiting for SSH to become available... DEBUG ssh: == Net-SSH connection debug-level log END == INFO ssh: SSH is ready! DEBUG ssh: Re-using SSH connection. INFO ssh: Execute: (sudo=false) DEBUG ssh: pty obtained for connection DEBUG ssh: stdout: export TERM=vt100 **JOHN: cloud-boothook script should have run by now in here** DEBUG ssh: stdout: logout DEBUG ssh: Exit status: 0 INFO run_instance: Time for SSH ready: 48.444087982177734 INFO interface: info: Machine is booted and ready for use! which rsync DEBUG ssh: stdout: /usr/bin/rsync INFO interface: info: Machine not provisioning because `--no-provision` is specified. After all of that it is safe to either have vagrant provision or vagrant ssh -c \"ls -ahl\" Using a pseudo tty is a required workaround if using an AMI that does not support tty / sudo (i.e. Amazon's default Linux AMI) https://docs.vagrantup.com/v2/vagrantfile/ssh_settings.html https://github.com/mitchellh/vagrant/issues/1482 Disabling the /vagrant synced project folder is nice if you don't automatically want the Vagrantfile and everything in there rsynced to your EC2 instance (and avoids the ugly mkdir -p /vagrant which requires sudo) AWS User Data can be pushed in via Vagrant which allows for custom scripts / commands / package installation during the EC2 instance boot https://help.ubuntu.com/community/CloudInit https://cloudinit.readthedocs.org/en/latest/topics/format.html#cloud-boothook http://stackoverflow.com/questions/17413598/vagrant-rsync-error-before-provisioning One of the use cases for an aws.user_data #cloud-boothook script has been to add to /etc/sudoers.d/ (thus avoiding later sudo issues with rsync) VirtualBox A really easy A way to start an Ubuntu 14.04 box locally with VirtualBox, the shell provisioner is less elegant than chef/puppet/ansible but gets the job done (installs Docker and Docker Compose) https://www.virtualbox.org/wiki/Linux_Downloads Vagrantfile VERSIONS = { 'trusty' => { 'box' => \"canonical-ubuntu-14.04\", 'box_url' => \"https://cloud-images.ubuntu.com/vagrant/trusty/current/trusty-server-cloudimg-amd64-vagrant-disk1.box\", 'ami' => \"ami-018c9568\", }, } Vagrant.configure(\"2\") do |config| config.ssh.forward_agent = true version = VERSIONS[(\"trusty\")] config.vm.provider \"virtualbox\" do |v, override| v.customize [\"modifyvm\", :id, \"--memory\", ENV['VM_MEMORY'] || 4096] v.customize [\"modifyvm\", :id, \"--cpus\", ENV['VM_CPUS'] || 2] override.vm.network :private_network, ip: \"192.168.33.10\" override.vm.box = version['box'] override.vm.box_url = version['box_url'] end config.vm.provision :shell, :inline => \"sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9\" config.vm.provision :shell, :inline => \"sudo sh -c 'echo deb https://get.docker.io/ubuntu docker main > /etc/apt/sources.list.d/docker.list'\" config.vm.provision :shell, :inline => \"sudo apt-get update\" config.vm.provision :shell, :inline => \"sudo apt-get install -y lxc-docker python-pip\" config.vm.provision :shell, :inline => \"sudo pip install --upgrade pip\" config.vm.provision :shell, :inline => \"sudo pip install --upgrade docker-compose\" end more info ps aux | grep vagrant nothing to see here but there is still state for machines started... vagrant global-status vagrant global-status --prune rm -rf .vagrant rm -rf /home/ubuntu/.vagrant.d If you use vagrant 1.7 don not be surprised if you see errors related to SSL, 1.6.3 FTW https://github.com/mitchellh/vagrant-aws Troubleshooting vagrant up --provider=aws --debug ERROR: The provider 'aws' could not be found, but was requested to back the machine 'default'. Please use a provider that exists. RESOLUTION: try re-installing the vagrant-aws plugin again and immediately running the vagrant up command afterwards ERROR: Timeout when waiting for SSH , SSH not up: ... The private key to connect to the machine via SSH must be owned RESOLUTION: chown root:root and chmod 400 ERROR: INFO ssh: SSH not up: #<Vagrant::Errors::SSHAuthenticationFailed: SSH authentication failed! This is typically caused by the public/private keypair for the SSH user not being properly set on the guest VM RESOLUTION: ensure the correct user, i.e. ec2-user or ubuntu etc. is used in the override.ssh.username to match what the AMI expects ERROR: sudo: no tty present and no askpass program specified RESOLUTION: your VM (or more likely, AMI) does not have tty or allow sudo so try using the config.ssh.pty = true (and make sure no provisioning commands require sudo)","tags":"virtualization","title":"Using Vagrant to deploy instances on AWS"},{"url":"https://blog.john-pfeiffer.com/docker-intro-install-run-and-port-forward/","text":"Docker is a union file system based layer system (previously leveraging linux lxc containers) for ultra lightweight virtualization/compartmentalization. Much like AWS cloud servers (api based dynamic deployment that should be tolerant of node failure) and automated deployment/configuration infrastructure (chef or puppet such that cloud servers are created idempotent, remotely and automatically managed at scale), Docker requires a change of mindset. Docker encourages design of modular, deterministic and defined, single purpose components that are easy to compose into larger services. As any tool, using it for managing complexity and packaging can be very helpful but it does expose other potential issues (composability, orchestration, security). Images are the initial templates, each image has a unique ID https://docs.docker.com/engine/userguide/storagedriver/imagesandcontainers/#content-addressable-storage Containers are the running virtual machines, each container has a unique ID http://docs.docker.com/terms/container From now on it is assumed you use sudo before any docker command! Install Docker https://docs.docker.com/installation/ubuntulinux/ apt-key adv --keyserver hkp://pgp.mit.edu:80 --recv-keys 58118E89F3A912897C070ADBF76221572C52609D sudo sh -c \"echo 'deb https://apt.dockerproject.org/repo ubuntu-trusty main' > /etc/apt/sources.list.d/docker.list\" sudo apt-get update sudo apt-get install linux-image-extra-$(uname -r) apt-get install docker-engine service docker status docker info Make sure it lists /var/lib/docker/aufs OPTIONAL STEP IF YOU HAD AN OLD DOCKER INSTALLATION apt-get purge lxc-docker* Installing on Mac or Windows https://www.docker.com/docker-toolbox Quick Start Summary docker run --rm busybox /bin/sh -c \"echo 'hi'\" hi docker run -it --rm -e MYVAR=123 busybox env \"run\" will pull the image from Docker Hub by default, e injects an environment variable, overrides the Docker Image CMD with \"env\" docker run -it --rm --entrypoint=/bin/sh python:latest run the docker container (from the latest public python image) and start the shell prompt instead of the python interpreter the entrypoint parameter overrides the Docker Image (in case they do not provide a helpfully overridable CMD) http://docs.docker.com/engine/reference/builder/#entrypoint Download a docker image Official Images are he easiest to experiment with: https://hub.docker.com/explore/ sudo docker pull ubuntu:trusty ( grabs the latest, i.e. 14.04.1 ) or use a different tag to download a more specific version sudo docker pull ubuntu:12.04.3 CRITICAL WARNING! use the colon and a specific version! downloading all of the ubuntu images by accident sucks =( sudo docker pull redis:latest choose the latest for just messing around but... ALWAYS use a specific version to avoid having your dependencies change unexpectedly Finding what versions of images (tags) you can pull requires using either the UI or the API: https://hub.docker.com/r/library/redis/tags/ docker pull redis:3 docker pull redis:2.8 docker pull redis:2.6 Many of the tags are synonyms/symlinks, so latest is the same as 3 is the same as 3.0 Remove a docker image docker rmi redis docker rmi -f $(docker images --all --quiet | grep -v 5506de2b643b) remove ALL images except one by taking the output (quiet means only image ids), excluding a specific one, and then force removing the images (by id) docker images --quiet --filter \"dangling=true\" | xargs docker rmi remove all images that do not have a tag and are not a parent of a tagged image docker rmi -f $(docker images --all --quiet) remove ALL images du -sh /var/lib/docker/aufs Summarize the amount of disk space taken by images and layers, e.g.: 72K /var/lib/docker/aufs/ Sometimes a docker image is still connected to a container (already exited or forgotten) docker ps -a docker rm name_or_id docker rm a1b2 docker rmi image_id it will \"smart match\" the first characters of the container ID the same as git short sha Docker info docker get a helpful list of all the commands docker --version docker info :::text Containers: 1 Images: 23 Storage Driver: aufs Root Dir: /var/lib/docker/aufs Dirs: 25 Execution Driver: native-0.2 Kernel Version: 3.13.0-35-generic Operating System: Ubuntu 14.04.1 LTS WARNING: No swap limit support du -sh /var/lib/docker/aufs 469M /var/lib/docker/aufs/ docker ps --all no containers are running yet docker ps --help docker images --tree is a deprecated command to view the hashes and sizes of all of the parent images Controlling Containers Starting a container from an image Create a container First you must create a container from an image: docker run get a helpful list of how to run a container docker run --rm -i -t ubuntu:14.04 /bin/bash :::text creates a container --rm: automatically remove the container when it exits -i: keep stdin open even if not attached -t: allocate a tty, attach stdin and stdout use the ubuntu 14.04 minimal image Docker automatically gives the container a random name Runs an interactive bash shell This will continue to exist in a stopped state once exited (see \"docker ps -a\") root@f5878ed6016e:/# cat /etc/issue root@f5878ed6016e:/# uname -a root@f5878ed6016e:/# df -h Control-p then Control-q to detach the tty without exiting the shell docker ps docker run --detach --name myapp -p 127.0.0.1:5000:5000 training/webapp python app.py detached with port 5000 available only to the host and executing the command python with parameter app.py docker exec myapp ls -ahl runs the ls command inside the container named \"myapp\" docker run --rm -i -t --link myhipchat_mariadb_1 mariadb:5 /bin/bash -c \"exec mysql --version\" docker run --rm -i -t --link myhipchat_mariadb_1:mysql mariadb:5 /bin/bash -c 'exec mysql -h\"$MYSQL_PORT_3306_TCP_ADDR\" -P\"$MYSQL_PORT_3306_TCP_PORT\" -uroot -p' an Image can contain both the server and client code so run a \"client container\" to connect to a running server Container Start (resume) a container After a container has already been created (which starts it so ironically this is actually a \"restart\") docker start --interactive --attach container_id_or_name Attaching to a running container docker ps -a CONTAINER ID ... STATUS NAMES 9e0ebf4421dd ... Up 6 seconds myexample docker attach 9e0ebf4421dd docker attach myexample since the above will expect the container to have /bin/bash it will reuse the instance of shell sudo docker exec -i -t 9e0ebf4421dd /bin/bash # ps aux # exit instead a new /bin/bash is executed inside creating a second shell - use the exit command to not leave it around Stopping a container Part of the efficiency in docker is that containers can run in the background automatically docker run --detach --name myredis redis docker ps docker stop myredis Another efficiency is that a docker container will only run as long as it takes to execute a command (and any changes are not forgotten ) docker run ubuntu:trusty uname -a this runs the container only as long as it takes to execute the command docker attach f5878ed6016e Control + C (now we have exited the container and it will clean itself up) docker ps -a spun up another container but only long enough to run the command CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e4b436320442 ubuntu:14.04 -uname -a 3 minutes ago elegant_engelbart Copying a file out of a container docker cp <containerId>:/file/path/within/container /host/path/target https://docs.docker.com/reference/commandline/cli/#cp Deleting aka removing a container docker rm e4b436320442 Alternatively: docker rm --force elegant_engelbart docker rm -f $(docker ps -a -q) Deletes forcibly all containers (be careful!) Dockerfile to automate building an image Dockerfiles allow automating the creation docker images. One advantage to Dockerfile is that each command creates a separate layer so if a specific layer fails all of the previous intermediate images can be re-used. Also the version style of imagename:tag allows for chaining of upgrades of child images Containers as fast, reliable, and deterministic prod/qa/dev environments can also be extended to be just an improved experimentation sandbox (for those used to SSH and using Linux as a common base OS). mkdir -p dockerfiles/trustyssh vi dockerfiles/trustyssh/Dockerfile one Dockerfile per directory FROM ubuntu:trusty MAINTAINER John Pfeiffer \"https://bitbucket.org/johnpfeiffer\" RUN apt-get update -y RUN apt-get install -y openssh-server byobu RUN mkdir /var/run/sshd RUN echo 'root:root' | chpasswd RUN echo 'ubuntu:ubuntu' | chpasswd RUN sed -ri 's/&#94;PermitRootLogin\\s+.*/PermitRootLogin yes/' /etc/ssh/sshd_config RUN sed -ri 's/UsePAM yes/#UsePAM yes/g' /etc/ssh/sshd_config EXPOSE 22 CMD [\"/usr/sbin/sshd\", \"-D\"] docker build --tag=newimagename --rm ./dockerfiles/ubuntu-trusty-ssh Each RUN command creates an intermediate container, so make sure you use the -rm option Sending build context to Docker daemon 2.56 kB Sending build context to Docker daemon Step 0 : FROM ubuntu:trusty ---> 5506de2b643b ... Removing intermediate container 135b686d82a6 Step 2 : RUN apt-get update -y ---> Running in 7fe06a9ef41b Ign http://archive.ubuntu.com trusty InRelease ... Since each run command creates a new layer it is best practice to consolidate commands for a single logical action when possible: RUN echo \"#!/bin/bash\" > /root/build-and-run.sh && \\ echo \"cd /opt/mydata/pelican-project\" >> /root/build-and-run.sh && \\ echo \"pelican content -r &\" >> /root/build-and-run.sh && \\ echo \"cd /opt/mydata/pelican-project/output\" >> /root/build-and-run.sh && \\ echo \"python -m SimpleHTTPServer\" >> /root/build-and-run.sh && \\ chmod +x /root/build-and-run.sh docker history ubuntu-utopic-pelican:latest view the history of hashes (which can be run by themselves) and timestamps and sizes IMAGE CREATED CREATED BY SIZE 39ac388d3c0d 37 seconds ago /bin/sh -c #(nop) CMD [/bin/bash /root/build- 0 B 218edf407f18 37 seconds ago /bin/sh -c echo \"#!/bin/bash\" > /root/build-a 129 B d9d774d344bd 2 weeks ago /bin/sh -c #(nop) EXPOSE 8000/tcp 0 B ae1733e0e1b9 2 weeks ago /bin/sh -c pip install pelican Markdown beaut 20.64 MB 24561ed8052f 2 weeks ago /bin/sh -c curl https://bootstrap.pypa.io/get 9.826 MB 1878a9a052eb 2 weeks ago /bin/sh -c apt-get update && apt-get install 60.85 MB 5e5e0e9171da 2 weeks ago /bin/sh -c #(nop) MAINTAINER John Pfeiffer \"h 0 B 78949b1e1cfd 7 weeks ago /bin/sh -c #(nop) CMD [/bin/bash] 0 B 21abcc4ef877 7 weeks ago /bin/sh -c sed -i 's/&#94;#\\s*\\(deb.*universe\\)$/ 1.895 kB f552c527d701 7 weeks ago /bin/sh -c echo '#!/bin/sh' > /usr/sbin/polic 215 kB c4c77a6165f9 7 weeks ago /bin/sh -c #(nop) ADD file:24ed1895f2e500dcec 194.2 MB 511136ea3c5a 22 months ago 0 B experimental: docker save 49b5a7a88d5 | sudo docker-squash -t ubuntu-utopic-pelican:squash | docker load Add a port to a container docker run --detach --publish 127.0.0.1:2222:22 --name johnssh trustyssh run a detached container based on the trustyssh image that binds the private container port 22 to the host port 2222 (in this case the trustyssh image from above is running sshd on port 22) docker port johnssh 22 127.0.0.1:2222 netstat -antp | grep 2222 tcp 0 0 127.0.0.1:2222 0.0.0.0:* LISTEN 24393/docker-proxy ssh -o StrictHostKeychecking=no -p 2222 root@127.0.0.1 root@19ad0614b237:~# command output docker run --detach --publish 0.0.0.0:6379:6379 --name redis redis feeb79581810a8c182202c73d4e1c6b905960bcfc860e04285f1ae03c6a47f18 docker port redis 6379/tcp -> 0.0.0.0:6379 docker port redis 6379 0.0.0.0:6379 redis-cli -h docker.example.com ping PONG docker run --detach --publish-all --name redis redis ff2f6d2e04d565f11d71664bf6cf23638656d9b633e4d3c94444c81b18b807bb docker port redis 6379/tcp -> 0.0.0.0:49153 docker port redis 6379 0.0.0.0:49153 docker run --detach -P --name johnssh trustyssh publish all EXPOSE'd ports to random ports on the host Logs from the containers docker logs container_name view the latest logs of a specific container in stdout docker logs -f container_name tail the logs for Host data with a Docker Container Volumes are where Docker Containers can access storage (either from the Host or other Containers) https://docs.docker.com/userguide/dockervolumes docker run --interactive --tty --name mydata --volume /tmp/mydata:/opt/mydata trustyssh /bin/bash create an interactive container named \"mydata\" that maps /tmp/mydata from the host onto /opt/mydata (warning: overriding any existing!) Managing or limiting the resources available to a Container https://docs.docker.com/engine/reference/run/#runtime-constraints-on-resources docker run -i -t --rm --cpuset-cpu 0 --memory 512m ubuntu:14.04 /bin/bash the usual ubuntu trusty bash prompt but anything we run will be pinned to cpu 0 (i.e. 25% of a 4 core system) and have at most 512 MB of RAM and 512 MB of swap available Using Docker for a GUI application Mostly outside of the vision of containerization (neither isolation nor performance exactly) is using Docker to run GUI applications without installing them on the Host. Jessie Frazelle has done some excellent work pointing out how sharing the X11 socket from the host means lots of apps can run \"without being installed\" https://github.com/jfrazelle/dockerfiles One tip she did not include was the part about XManager security, run the following if you run into an error xhost local:root - https://www.netsarang.com/knowledgebase/xmanager/3898/xhost_and_how_to_use_it - http://www.x.org/archive/X11R6.8.0/doc/xhost.1.html xhost local:root; docker run --rm -v /tmp/.X11-unix:/tmp/.X11-unix -e DISPLAY=unix$DISPLAY --device /dev/snd jess/chromium - modify the xhost security to allow access to x windows - ephemeral so do not save anything - share the X11 unix socket - bind to the current display (i.e. :0.0) - allow sound from the Docker container Saving a docker container as a new image docker commit --help docker commit for a container: ubuntu:trusty with git docker run -i -t ubuntu:trusty /bin/bash Control + p then Control + q (to detach the tty without exiting the shell) docker ps -a (make a note of the ID or NAME) docker attach ID_OR_NAME apt-get update; apt-get install git -y cd /root git --version git clone https://johnpfeiffer@bitbucket.org/johnpfeiffer/myrepo.git exit exit now the Container will have git installed, a repo cloned, and will be stopped docker commit container_name_here johnpfeiffer_git_repo 4a74440186d976caeccc52f5ed2bd44269beb84d472391a7ce26ee3db8ffc1e9 docker images output REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE johnpfeiffer_git_repo latest 4a74440186d9 54 seconds ago 402 MB ubuntu 14.04 e54ca5efa2e9 3 weeks ago 276.5 MB Misc Python and Pelican static site generator on docker ubuntu:14.04 apt-get update; apt-get install python python-setuptools openssl wget -y wget -qO- https://raw.github.com/pypa/pip/master/contrib/get-pip.py | sudo python pip install pelican Markdown beautifulsoup4 docker run -i -t -p 127.0.0.1:8000:8000 --name john johnpfeiffer_git_repo:latest /bin/bash control+p then control+q docker attach john cd /root/myrepo pelican content (assuming pelicanconf.py is here) cd output python -m SimpleHTTPServer control+p then control+q docker ps -a http://127.0.0.1:8000 Advanced Docker pre built images docker search stackbrew/ubuntu FYI stackbrew/ubuntu is the same as ubuntu , stackbrew is the curated Docker registry https://registry.hub.docker.com Redis https://registry.hub.docker.com/_/redis/ docker run --detach --publish 127.0.0.1:6379:6379 --name redis redis detached container based on the redis latest bound to the host on port 6379 the image already includes by default the expose port command: EXPOSE 6379... apt-get install redis-tools; redis-cli connect from the host to the redis container to the redis interactive cli docker run -i -t --link redis:db johnssh /bin/bash root@d95a758eaa6b:/ # apt-get install redis-tools env redis-cli -h $ DB_PORT_6379_TCP_ADDR ping PONG get mykey \"somevalue\" docker stop johnssh docker start --interactive --attach johnssh root@d95a758eaa6b:/# Environment variables are tricky sudo docker run --detach --publish 127.0.0.1:2222:22 --name johnssh trustyssh if you SSH into your container or use byobu \"env\" will not show you the info From the host: sudo docker inspect --format '{{ .NetworkSettings.IPAddress }}' redis 172.17.0.72 ssh -p 2222 root@localhost apt-get install redis-tools redis-cli -h 172.17.0.72 keys * 1) mykey Add a route to the Host From inside a Container/Guest http://docs.docker.com/reference/commandline/cli/#adding-entries-to-a-container-hosts-file HOST: ip addr show ip addr show | grep docker0 ip addr show | grep docker0 | grep global | awk '{print $2}' | cut -d / -f1 HOSTIP=$(ip addr show | grep docker0 | grep global | awk '{print $2}' | cut -d / -f1) sudo docker run -i -t --rm --add-host=docker:${HOSTIP} python:2 /bin/bash root @1 bbe25092f19 : / # cat / etc / hosts 172.17.0.7 1 bbe25092f19 127.0.0.1 localhost :: 1 localhost ip6 - localhost ip6 - loopback fe00 :: 0 ip6 - localnet ff00 :: 0 ip6 - mcastprefix ff02 :: 1 ip6 - allnodes ff02 :: 2 ip6 - allrouters 172.17.42.1 docker root @1 bbe25092f19 : / # ping docker PING docker ( 172.17.42.1 ) : 56 data bytes 64 bytes from 172.17.42.1 : icmp_seq = 0 ttl = 64 time = 0.158 ms Docker Compose Complex real systems have multiple dependencies and following the recommended Docker pattern of \"do one thing per container\" means needing a way to start/orchestrate a bunch of things at once. While there are some amazing open source projects ( http://kubernetes.io/ , http://mesos.apache.org/documentation/latest/docker-containerizer/ ) it is instructive to start with the simplest model provided directly from Docker, https://docs.docker.com/compose/ sudo pip install --upgrade docker-compose docker-compose --version app.py from flask import Flask from redis import Redis import os app = Flask ( __name__ ) redis = Redis ( host = 'redis' , port = 6379 ) @app.route ( '/' ) def hello (): redis . incr ( 'hits' ) return 'Hello World! I have been seen %s times.' % redis . get ( 'hits' ) if __name__ == \"__main__\" : app . run ( host = \"0.0.0.0\" , debug = True ) requirements.txt flask redis Dockerfile FROM python:2.7 ADD . /code WORKDIR /code RUN pip install -r requirements.txt CMD python app.py docker-compose.yml web : build : . ports : - \"5000:5000\" volumes : - .:/ code links : - redis redis : image : redis dependencies like redis are \"linked\" for network access (ordering in the file is important) \"web\" app (which comes from the current directory Dockerfile) ports: exposes ports to the host (all of these are just like the docker run CLI parameters) volumes: allows live editing of app.py redis comes from the public docker registry image (not a Dockerfile nor a private registry) and is using the default tag of \"latest\" docker-compose up docker-compose up this will docker pull redis and build from the Dockerfile and then start them all in the correct order docker-compose ps run this command in the directory with the docker-compose.yml to see the state of the system Name Command State Ports ------------------------------------------------------------------------------------- composeexample_redis_1 /entrypoint.sh redis-server Up 6379/tcp composeexample_web_1 /bin/sh -c python app.py Up 0.0.0.0:5000->5000/tcp docker ps -a docker-compose stop http://localhost:5000/ Hello World! I have been seen 2 times. https://bitbucket.org/johnpfeiffer/docker/src Troubleshooting Building images with Dockerfile Building images often depends on the network and DNS If you are using Wifi be careful as intermittent network connectivity may cause frustrating issues. For DNS with docker installed onto ubuntu via apt-get, try changing to Google DNS by uncommenting in: vi /etc/default/docker sudo service docker restart Building images often depends on dependencies look closely at error messages, i.e. make: not found and ensure that an early RUN statement has apt-get update && apt-get install -y build-essential More Info https://docs.docker.com/reference/commandline/cli/#run Real World example of using Docker for behind the firewall delivery: https://bitbucket.org/atlassianlabs/ac-koa-hipchat-sassy/pull-request/6/readmemd-contains-instructions-on-how-to/diff https://docs.docker.com/articles/basics https://github.com/wsargent/docker-cheat-sheet http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker http://jpetazzo.github.io/2014/06/23/docker-ssh-considered-evil Docker API and the Docker Hub Public Registry Docker used to have an API endpoint at registry.hub.docker.com/v1 but in a fairly typical move for them they changed it so a lot of internet \"documentation\" examples are wrong. Also the Docker Hub was deprecated as of 1.7 so this is the last docs on how to use it (because the service is still running or compatible) curl https://index.docker.io/v1/_ping true It uses basic authentication and while you can use the API to sign up it may just be easier to use the web site: https://hub.docker.com/ curl https://username:password@index.docker.io/v1/users/ \"OK\" just curl with basic auth in order to check the credentials, the trailing slash is IMPORTANT Using a browser with the Docker REST API is often more convenient as it caches the Basic Authentication https://docs.docker.com/v1.7/reference/api/docker-io_api/#users https://docs.docker.com/v1.6/reference/api/registry_api/ (because the registry api was deprecated earlier?) To see all of the images for a given repository (it is json formatted and there will be a lot of results!) curl https://username:password@index.docker.io/v1/repositories/python/images https://docs.docker.com/v1.6/reference/api/registry_api/ If you docker search python and want to see the tags (i.e. you do not want to pull every python image ever made), then try curl with REST: curl https://username:password@index.docker.io/v1/repositories/python/tags [{\"layer\": \"a2db1214\", \"name\": \"latest\"}, {\"layer\": \"edb21ec7\", \"name\": \"2\"}, {\"layer\": \"82b600dd\", \"name\": \"2-alpine\"}, {\"layer\": \"6a4e9662\", \"name\": \"2-onbuild\"}, {\"layer\": \"99b38a11\", \"name\": \"2-slim\"}, {\"layer\": \"fe724fa0\", \"name\": \"2-wheezy\"}, {\"layer\": \"edb21ec7\", \"name\": \"2.7\"}, {\"layer\": \"82b600dd\", \"name\": \"2.7-alpine\"}, {\"layer\": \"6a4e9662\", \"name\": \"2.7-onbuild\"}, {\"layer\": \"99b38a11\", \"name\": \"2.7-slim\"}, {\"layer\": \"fe724fa0\", \"name\": \"2.7-wheezy\"}, {\"layer\": \"c71c2739\", \"name\": \"2.7.10\"}, {\"layer\": \"f1f35fa4\", \"name\": \"2.7.10-onbuild\"}, {\"layer\": \"843123ac\", \"name\": \"2.7.10-slim\"}, {\"layer\": \"fde41dc3\", \"name\": \"2.7.10-wheezy\"}, {\"layer\": \"edb21ec7\", \"name\": \"2.7.11\"}, {\"layer\": \"82b600dd\", \"name\": \"2.7.11-alpine\"}, {\"layer\": \"6a4e9662\", \"name\": \"2.7.11-onbuild\"}, {\"layer\": \"99b38a11\", \"name\": \"2.7.11-slim\"}, {\"layer\": \"fe724fa0\", \"name\": \"2.7.11-wheezy\"}, {\"layer\": \"a87a2288\", \"name\": \"2.7.7\"}, {\"layer\": \"481b175a\", \"name\": \"2.7.8\"}, {\"layer\": \"fbb30ed2\", \"name\": \"2.7.8-onbuild\"}, {\"layer\": \"3cf7f142\", \"name\": \"2.7.8-slim\"}, {\"layer\": \"6a873836\", \"name\": \"2.7.8-wheezy\"}, {\"layer\": \"2d0d0130\", \"name\": \"2.7.9\"}, {\"layer\": \"10948f7c\", \"name\": \"2.7.9-onbuild\"}, {\"layer\": \"e86252d0\", \"name\": \"2.7.9-slim\"}, {\"layer\": \"a11d441b\", \"name\": \"2.7.9-wheezy\"}, {\"layer\": \"a2db1214\", \"name\": \"3\"}, {\"layer\": \"bb6cd371\", \"name\": \"3-alpine\"}, {\"layer\": \"80662aa6\", \"name\": \"3-onbuild\"}, {\"layer\": \"07bfefb9\", \"name\": \"3-slim\"}, {\"layer\": \"2edf9614\", \"name\": \"3-wheezy\"}, {\"layer\": \"7575f4a5\", \"name\": \"3.2\"}, {\"layer\": \"31b273f6\", \"name\": \"3.2-onbuild\"}, {\"layer\": \"ca0a0ed6\", \"name\": \"3.2-slim\"}, {\"layer\": \"f5644650\", \"name\": \"3.2-wheezy\"}, {\"layer\": \"7575f4a5\", \"name\": \"3.2.6\"}, {\"layer\": \"31b273f6\", \"name\": \"3.2.6-onbuild\"}, {\"layer\": \"ca0a0ed6\", \"name\": \"3.2.6-slim\"}, {\"layer\": \"f5644650\", \"name\": \"3.2.6-wheezy\"}, {\"layer\": \"84717b99\", \"name\": \"3.3\"}, {\"layer\": \"4de0c1a0\", \"name\": \"3.3-alpine\"}, {\"layer\": \"e0985e72\", \"name\": \"3.3-onbuild\"}, {\"layer\": \"3c0f39af\", \"name\": \"3.3-slim\"}, {\"layer\": \"a13ad718\", \"name\": \"3.3-wheezy\"}, {\"layer\": \"e663e96e\", \"name\": \"3.3.5\"}, {\"layer\": \"79d3367e\", \"name\": \"3.3.5-onbuild\"}, {\"layer\": \"84717b99\", \"name\": \"3.3.6\"}, {\"layer\": \"4de0c1a0\", \"name\": \"3.3.6-alpine\"}, {\"layer\": \"e0985e72\", \"name\": \"3.3.6-onbuild\"}, {\"layer\": \"3c0f39af\", \"name\": \"3.3.6-slim\"}, {\"layer\": \"a13ad718\", \"name\": \"3.3.6-wheezy\"}, {\"layer\": \"c7184f4f\", \"name\": \"3.4\"}, {\"layer\": \"e6310f15\", \"name\": \"3.4-alpine\"}, {\"layer\": \"c38d9f7b\", \"name\": \"3.4-onbuild\"}, {\"layer\": \"ab9f7f65\", \"name\": \"3.4-slim\"}, {\"layer\": \"c98c4a9d\", \"name\": \"3.4-wheezy\"}, {\"layer\": \"b504e00c\", \"name\": \"3.4.1\"}, {\"layer\": \"07e5901a\", \"name\": \"3.4.1-onbuild\"}, {\"layer\": \"ec50e6a0\", \"name\": \"3.4.2\"}, {\"layer\": \"ade8543e\", \"name\": \"3.4.2-onbuild\"}, {\"layer\": \"dd1dee45\", \"name\": \"3.4.2-slim\"}, {\"layer\": \"de6911d6\", \"name\": \"3.4.2-wheezy\"}, {\"layer\": \"48bc52cc\", \"name\": \"3.4.3\"}, {\"layer\": \"bf599bc6\", \"name\": \"3.4.3-onbuild\"}, {\"layer\": \"0b92f173\", \"name\": \"3.4.3-slim\"}, {\"layer\": \"b8845e5b\", \"name\": \"3.4.3-wheezy\"}, {\"layer\": \"c7184f4f\", \"name\": \"3.4.4\"}, {\"layer\": \"e6310f15\", \"name\": \"3.4.4-alpine\"}, {\"layer\": \"c38d9f7b\", \"name\": \"3.4.4-onbuild\"}, {\"layer\": \"ab9f7f65\", \"name\": \"3.4.4-slim\"}, {\"layer\": \"c98c4a9d\", \"name\": \"3.4.4-wheezy\"}, {\"layer\": \"a2db1214\", \"name\": \"3.5\"}, {\"layer\": \"bb6cd371\", \"name\": \"3.5-alpine\"}, {\"layer\": \"80662aa6\", \"name\": \"3.5-onbuild\"}, {\"layer\": \"07bfefb9\", \"name\": \"3.5-slim\"}, {\"layer\": \"c64596cb\", \"name\": \"3.5.0\"}, {\"layer\": \"c9744d7e\", \"name\": \"3.5.0-onbuild\"}, {\"layer\": \"ac60c7d8\", \"name\": \"3.5.0-slim\"}, {\"layer\": \"31ef8f64\", \"name\": \"3.5.0b3\"}, {\"layer\": \"7c5e081c\", \"name\": \"3.5.0b3-onbuild\"}, {\"layer\": \"0c47d2de\", \"name\": \"3.5.0b3-slim\"}, {\"layer\": \"a2db1214\", \"name\": \"3.5.1\"}, {\"layer\": \"bb6cd371\", \"name\": \"3.5.1-alpine\"}, {\"layer\": \"80662aa6\", \"name\": \"3.5.1-onbuild\"}, {\"layer\": \"07bfefb9\", \"name\": \"3.5.1-slim\"}, {\"layer\": \"bb6cd371\", \"name\": \"alpine\"}, {\"layer\": \"80662aa6\", \"name\": \"onbuild\"}, {\"layer\": \"07bfefb9\", \"name\": \"slim\"}, {\"layer\": \"2edf9614\", \"name\": \"wheezy\"}] Docker Engine internal API If instead of the docker client you wish to interact more programatically... https://docs.docker.com/engine/reference/api/docker_remote_api/ https://github.com/docker/distribution Private Docker Registry Login to a private docker registry curl -i https://username:password@docker.example.com/v2/ attempt to login to a private registry Using the docker client to login to a private registry docker login docker.example.com:443 > Username: user@example.com > WARNING: login credentials saved in /home/USER/.docker/config.json > Login Succeeded private registry basics via the browser How to deploy your own docker registry: https://docs.docker.com/registry/deploying/ https://docker.example.com/_ping {} https://docker.example.com/info https://docker.example.com/version note these commands may not be enabled or available in your private registry version https://docker.example.com/v1/repositories/library/ubuntu/tags {\"13.04\": \"5e47ac691989afcd10285ea4e67b46bc0fdc98d90844e57a6d4221c1e3ab4388\"} https://docker.example.com/v1/repositories/micros/baseimage-ubuntu/tags {\"latest\": \"5a14c1498ff4983793f6e5eddd16868dbad257195f0e85c66ece94d881ecb28f\"} https://docker.example.com/v1/repositories/micros/baseimage-ubuntu/images list of the images available: [{\"id\":\"8254ff58b098b72425854555204171352a69f5427ba83dee4642ba45d301d0b1\"}] https://docker.example.com/v1/repositories/micros/baseimage-ubuntu/json inspect an image (what OS, kernel, etc.) {\"arch\": \"amd64\", \"docker_go_version\": \"go1.3.3\", \"docker_version\": \"1.3.3\", \"kernel\": \"3.16.7-tinycore64\", \"last_update\": 1426041024, \"os\": \"linux\"} https://docker.example.com/v1/repositories/myuser/nginx/0348bf1e7cc54327b8c9ce8407c5b3eadade1ef1771d642d08ae16a6aad5bed5/json inspect a very specific image (by id) Searching a private docker registry https://registry.hub.docker.com/v1/search?q=pfeiffer the public docker registry search query, deprecated in APIv2 so no longer functional docker search docker.example.com/myuser the cli command returns a listing of all of the images for a user, deprecated in APIv2 so no longer functional If there is a proxy in front: docker search user:password@docker.example.com/myuser curl -s -X GET https://user:password@docker.example.com/v1/search LIST ALL IMAGES: or use a browser https://docker.example.com/v1/search curl -X GET https://user:password@docker.example.com/v1/search?q=ubuntu https://docker.example.com/v1/search?q=ubuntu {\"num_results\": 4, \"query\": \"ubuntu\", \"results\": [{\"description\": null, \"name\": \"example/ubuntu\"}, {\"description\": null, \"name\": \"library/ubuntu\"}, {\"description\": null, \"name\": \"micros/baseimage-ubuntu-ansible\"}, {\"description\": null, \"name\": \"micros/baseimage-ubuntu\"}]} https://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-14-04","tags":"virtualization","title":"Docker Intro install run and port forward"},{"url":"https://blog.john-pfeiffer.com/markdown-syntax-cheatsheet/","text":"Markdown Syntax html <em> is markdown *emphasis* or _italics_ = emphasis or italics html <strong> is markdown **strong* or __bold__ = strong or bold html strikethrough is not supported but can just be <del>strikethrough</del> = strikethrough html <blockquote> is markdown > at the start of each line html unordered list <ul> = - item or alternatives: + item , * item html <hr /> is markdown - - - or alternatives: * * * , *** , ***** html <a href= for hyper links is: <http://blog.john-pfeiffer.com> converts into a link that is automatically turned clickable: <a href=\"http://johnpfeiffer.bitbucket.org\">http://johnpfeiffer.bitbucket.org</a> [an example](http://example.com/ \"ExampleTitle\") an example both absolute and relative links are supported, as well as reference links that are defined elsewhere: This is [an example][someid] [someid]: http://example.com/ \"Optional Title Here\" numbered list 1 at the beginning of each line any digit will do, the numbering is rendered in order ensure the numbered list is surrounded by empty lines Inline code is markdown `backtick around the text` = backtick around the text A code block is markdown indent 4 spaces or 1 tab a blank line in the code block still needs to be indented ensure the code block is surrounded by empty lines :::text or :::bash at the top of a code block will control the syntax highlighting, see http://pygments.org/docs/lexers H1 # H1 H6 ###### H6 Tables are (sometimes) not supported but... Table with left justified (GitHub Flavored Markdown) |in|out|other| |---|---|---| |yes|no|maybe| in out other yes no maybe <em> *emphasis* emphasis Table with text center aligned |short|long centered| |:-:|:-:| |y|n| short long centered y n HTML Table < table >< th > header </ th > < tr > < td > first column in row 1 </ td >< td > 2nd column </ td > </ tr > </ table > more info http://daringfireball.net/projects/markdown/syntax","tags":"programming","title":"Markdown syntax cheatsheet"},{"url":"https://blog.john-pfeiffer.com/creating-a-static-web-site-with-bitbucket/","text":"create a bitbucket account with username create a repo named username.bitbucket.org mkdir username.bitbucket.org cd username.bitbucket.org echo \"hi\" > index.html git init . git add git commit -m \"first site index\" git remote add origin git@bitbucket.org:username/username.bitbucket.org.git git push origin master git branch --set-upstream master origin/master git pull https://username.bitbucket.org https://confluence.atlassian.com/display/BITBUCKET/Publishing+a+Website+on+Bitbucket Scroll down to the comments: You can setup a CNAME for your account . You cannot set up a CNAME for a static website hosted on Bitbucket. The account you setup a CNAME for may have a repository that represents a static website.","tags":"programming","title":"Creating a static web site with Bitbucket"},{"url":"https://blog.john-pfeiffer.com/how-to-set-up-a-pelican-static-blog-site/","text":"Pelican is an open source project that converts static text files into an html site. Why use a static site generator (pelican) instead of a hosted blog platform or a CMS (Content Management System)? Because less is more and you should use the right tool for the right job A static site made of html pages is very easy to maintain It is also more secure and performance is good too =) Allows for straightforward use of version control (git) Developers prefer to be able to customize and add functionality (python and javascript) Using widely adopted open source software reduces risk (more contributors, more testers, more bugfixes) Using a widely adopted platform increases leverage (themes, plugins, tutorials, etc.) Install Pelican sudo pip install pelican Markdown beautifulsoup4 installing both the pelican and the Markdown packages beautifulsoup4 is a dependency for the later step of the elegant theme TOC and search plugins optionally use virtualenv venv; source venv/bin/activate pelican-quickstart Welcome to pelican-quickstart v3.3.0. This script will help you create a new Pelican-based website. Please answer the following questions so this script can generate the files needed by Pelican. > Where do you want to create your new web site? [.] > What will be the title of this web site? johnpfeiffer.bitbucket.org > Who will be the author of this web site? john pfeiffer > What will be the default language of this web site? [en] > Do you want to specify a URL prefix? e.g., http://example.com (Y/n) > What is your URL prefix? (see above example; no trailing slash) https://johnpfeiffer.bitbucket.org > Do you want to enable article pagination? (Y/n) > Do you want to generate a Fabfile/Makefile to automate generation and publishing? (Y/n) > Do you want an auto-reload & simpleHTTP script to assist with theme and site development? (Y/n) > Do you want to upload your website using FTP? (y/N) > Do you want to upload your website using SSH? (y/N) > Do you want to upload your website using Dropbox? (y/N) > Do you want to upload your website using S3? (y/N) > Do you want to upload your website using Rackspace Cloud Files? (y/N) Done. Your new project is available at /home/ubuntu/BLOG tree . ├── content ├── develop_server.sh ├── fabfile.py ├── Makefile ├── output ├── pelicanconf.py └── publishconf.py 2 directories, 5 files Create Content vi content/hello_world.md Title: My first blog post Date: 2014-06-21 20:20 Tags: python Slug: my-first-blog-post Author: John Pfeiffer Summary: Short version for index and feeds This is the content of my first blog post. optional UI markdown editor: sudo apt-get install retext Run a dev server to see the results locally make devserver ...Starting up Pelican and pelican.server... ./develop_server.sh stop stop the dev server (required if reloading the .conf file) This only works with the basic first setup, after that it is better to manually use multiple screens: make clean make regenerate auto detects any content changes and reloads itself cd output; python -m SimpleHTTPServer Serving HTTP on 0.0.0.0 port 8000 ... (Control + C to quit) http://localhost:8000 http://docs.getpelican.com/en/latest/publish.html Setup a static Pages directory Besides lots of articles in categories it can be useful to have a few pages like About or Contact mkdir -p content/pages echo \"Title: About Us\" > content/pages/about.md http://docs.getpelican.com/en/latest/content.html#pages Publish I just use the pelicanconf output rather than publishconf, and I use git with a bitbucket static html site. Example pelican configuration file Contains the elegant theme and tipue search plugin vi pelicanconf.py #!/usr/bin/env python # -*- coding: utf-8 -*- # from __future__ import unicode_literals AUTHOR = u'john pfeiffer' SITENAME = u'johnpfeiffer' SITEURL = u'https://blog-john-pfeiffer.com' OUTPUT_PATH = 'output/' DEFAULT_DATE_FORMAT = '%Y-%m-%d' TIMEZONE=\"America/Los_Angeles\" # Feed generation is usually not desired when developing FEED_ALL_ATOM = None CATEGORY_FEED_ATOM = None TRANSLATION_FEED_ATOM = None # clean urls for pages , trailing slash to support HTTPS PAGE_URL = '{slug}/' PAGE_SAVE_AS = '{slug}/index.html' # clean urls for articles ARTICLE_SAVE_AS = '{slug}/index.html' ARTICLE_URL = '{slug}/' DEFAULT_PAGINATION = 10 THEME = 'themes/pelican-elegant' PLUGIN_PATHS = ['plugins'] PLUGINS = ['sitemap', 'extract_toc', 'tipue_search', 'post_stats'] MD_EXTENSIONS = ['codehilite(css_class=highlight)', 'extra', 'headerid', 'toc'] DIRECT_TEMPLATES = (('index', 'tags', 'categories','archives', 'search', '404')) STATIC_PATHS = ['theme/images', 'images'] SITEMAP = { 'format': 'xml', 'priorities': { 'articles': 0.5, 'indexes': 0.5, 'pages': 0.5 }, 'changefreqs': { 'articles': 'monthly', 'indexes': 'daily', 'pages': 'monthly' } } Hint: if you use SSL (e.g. cloudflare) then make sure your SITEURL is https and there are trailing slashes on your clean url helpers PAGE_URL and ARTICLE_URL otherwise you may get mixed content warnings or mixed http and https links in your output html cp -a pelicanconf.py publishconf.py Update publishconf.py with any final tweaks that should occur only in Production Pelican Themes https://github.com/getpelican/pelican-themes git clone --recursive https://github.com/getpelican/pelican-themes ~/pelican-themes http://pelican.readthedocs.org/en/latest/pelican-themes.html mkdir themes cp -a pelican-themes/elegant themes/ This is the lazy way, the correct way is to only copy in the theme you are using ;) FYI For the elegant with tipuesearch you need to also install the tipuesearch plugin http://moparx.com/2014/04/adding-search-capabilities-within-your-pelican-powered-site-using-tipue-search/ Depedency Management by Vendoring in Version Control Dependencies are always hard and leveraging the work of others (themes and plugins) means you have to consider how to manage changes to those moving parts too. Using your Build/CI/CD server to grab the latest themes/plugins is a mistake because while you would benefit from any new features and bug fixes you will also get burned (yes it has happened to me!) when you are publishing a blog post and suddenly your site doesn't work. (Oh are you using Continuous Post Deploy Smoke Tests and Realtime Everything Monitoring to detect this on your blog?) I highly recommend \"vendoring\" by copying pelican-elegant into the themes subdirectory in your version control. This sort of pinning makes an explicit choice that will ensure your site will continue to work until you choose to next upgrade. Pinning in the Build/CI/CD server by git sha will also work but makes it harder to keep everything in one place for development. The drawback of \"vendoring\" is the technical debt by the inevitable customization that occurs in this \"directory separated\" fork of the original Pelican Plugins https://github.com/getpelican/pelican-plugins git clone https://github.com/getpelican/pelican-plugins mkdir plugins cp -a pelican-plugins/sitemap plugins/ note this is the lazy way, a more precise way would be to only copy in the plugins used Hacking ListJS and per Article word count into a theme Basically once you get the hang of the config file format (just python really) and the templating engine (HTML with some jinja2) you can add all sorts of interesting pieces. In this example I leverage some javascript and another pelican plugin (python) to allow sorting and filtering on more data than the usual \"archives.html\" provides. <div id= \"article-list\" > <button class= \"sort\" data-sort= \"date\" > date </button> <button class= \"sort\" data-sort= \"title\" > title </button> <button class= \"sort\" data-sort= \"wordcount\" > word count </button> <button class= \"sort\" data-sort= \"category\" > category </button> <input class= \"search\" placeholder= \"Find by filter\" style= \"margin-top: 10px; height: 16px;\" /> &nbsp; from {{ dates | length }} articles <ul class= \"list\" > {% for category , articles in categories %} {% for article in articles %} <li> <span class= \"date\" style= \"padding-right: 10px;\" > <time pubdate= \"pubdate\" datetime= \" {{ article.date.isoformat () }} \" > {{ article.locale_date }} </time> </span> <a href= \" {{ SITEURL }} / {{ article.url }} \" ><span class= \"title\" > {{ article.title }} {% if article.subtitle %} <small> {{ article.subtitle }} </small> {% endif %} </span> </a> <em><span class= \"wordcount\" > ( {{ article.stats [ 'wc' ] }} words) </span></em> <em><span class= \"category\" > {{ article.category }} </span></em> </li> {% endfor %} {% endfor %} </ul> </div> ... ... <script type= \"text/javascript\" > var options = { valueNames: [ 'date', 'title', 'wordcount', 'category'], page: 1000 }; var hackerList = new List('article-list', options); hackerList.sort('date', { order: \"desc\" }) </script> https://blog.john-pfeiffer.com/listjs-sort-filters-search-and-more-for-html-lists-and-tables-in-javascript/ https://github.com/getpelican/pelican-plugins/tree/master/post_stats Advanced: skipping the Makefile pelican --help pelican ./content -o ./output -s ./publishconf.py cd output ; python -m SimpleHTTPServer Importing from Drupal with pelican-import Hack the Drupal files to allow a lot more than 10 items per feed grep -r 'items per feed' . learned from drupal-7.28/modules/system/system.module vi modules/system/system.admin.inc $form['feed_default_items'] Add to the dropdown choices of 10, 15, 30 etc. the option of 999 sudo apt - get install pandoc sudo pip install feedparser pelican - import - h pelican - import -- feed http : // blog . example . com / rss . xml - o output / - m markdown more info https://pelican.readthedocs.org/en/latest/settings.html Tweaking default syntax highlighting: http://pygments.org/docs/lexers http://pygments.org/demo https://bitbucket.org/johnpfeiffer/docker/src","tags":"programming","title":"How to set up a Pelican static blog site"},{"url":"https://blog.john-pfeiffer.com/cumulus-compatible-s3-nginx-and-hmac-signed-requests/","text":"With the exceptionally fast, reliable and popular web server nginx as a front end customers can use a browser to access their uploaded files via a simple URL, the same as the SaaS Amazon S3 implementation, without knowing about the Cumulus backend . Unfortunately there were edge cases around the encodings of spaces, pluses, slashes, etc. where nginx + Cumulus was returning \"Access Denied\" when trying to GET a file. Examining the relevant RFC's ( http://tools.ietf.org/html/rfc3986#section-2.1 ), PHP ( http://php.net/manual/en/function.rawurlencode.php ) and Python ﻿( http://docs.python.org/2/library/urllib.html ) references, and examining the logs, I could see the files were PUT correctly, s3cmd could retrieve the binary objects (files) from Cumulus fine... but the logs were showing a change in the URL's. Increasing the debugging in nginx , digging into the Cumulus source code and nginx AWS Authentication Module (and adding more logging statements in Python and C respectively), I realized there was a mismatch in the REST URL signature process. Since Cumulus was using the open source Python Boto library which is actually supported by Amazon (the de facto rulers of the S3 \"standard\"), I decided that their signing process was authoritative. A lot of digging into nginx configs and source, along with learning a bit about nginx module development and hacking the source of the ngx_aws_auth module, I finally came up with a matching signature process, (success!) ngx_aws_auth/ngx_http_aws_auth.c /* uses the source and length to copy the uri, does not escape characters (the argument signature is compatible with ngx_escape_uri) */ uintptr_t ngx_uri_extractor ( u_char * dst , u_char * src , size_t size , ngx_uint_t type ) { while ( size ) { * dst ++ = * src ++ ; size -- ; } return ( uintptr_t ) dst ; } /* customized to calculate the signature using the non escaped URI, compatible with cumulus boto */ static ngx_int_t ngx_http_aws_auth_variable_s3 ( ngx_http_request_t * r , ngx_http_variable_value_t * v , uintptr_t data ) { ngx_http_aws_auth_conf_t * aws_conf ; unsigned int md_len ; unsigned char md [ EVP_MAX_MD_SIZE ]; aws_conf = ngx_http_get_module_loc_conf ( r , ngx_http_aws_auth_module ); /* * This Block of code added to deal with paths that are not on the root - * that is, via proxy_pass that are being redirected and the base part of * the proxy url needs to be taken off the beginning of the URI in order * to sign it correctly. */ u_char * uri = ngx_palloc ( r -> pool , r -> uri . len + 200 ); // allow room for escaping /* u_char *uri_end = (u_char*) ngx_escape_uri(uri,r->uri.data, r->uri.len, NGX_ESCAPE_URI); */ u_char * uri_end = ( u_char * ) ngx_uri_extractor ( uri , r -> unparsed_uri . data , r -> unparsed_uri . len , NGX_ESCAPE_URI ); * uri_end = '\\0' ; // null terminate ... }","tags":"programming","title":"Cumulus compatible S3, nginx, and HMAC signed requests"},{"url":"https://blog.john-pfeiffer.com/selenium-headless-browser-automated-testing-with-phantomjs-and-python/","text":"Overview selenium: an automation framework for interactions with websites (like a programmatic browser) http://docs.seleniumhq.org/docs/02_selenium_ide.jsp#introduction webdriver is the interface: http://selenium-python.readthedocs.org/api.html useful for looking up things like driver.current_url phantomjs: headless browser http://phantomjs.org Install sudo pip install selenium wget <https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-1.9.8-linux-x86_64.tar.bz2> tar -xjvf phantomjs-1.9.8-linux-x86_64.tar.bz2 Run phantomjs-1.9.8-linux-x86_64/bin/phantomjs --webdriver=9134 ghostdriver included and running on port 9134 Example python script mini script to just show usage from selenium import webdriver driver = webdriver . PhantomJS ( executable_path = '/opt/phantomjs-1.9.8-linux-x86_64/bin/phantomjs' , port = 9134 ) driver . get ( \"http://127.0.0.1\" ) print driver . current_url driver . quit print \"done\" phantomjs - 1.9 . 8 - linux - x86_64 / bin / phantomjs -- webdriver = 9134 -- ignore - ssl - errors = true Advanced Python example more complete example with python unittest framework (used the Firefox Selenium IDE plugin -> Export) logs in, asserts there is an Admin tab which when clicked shows Group Info from selenium import webdriver from selenium.webdriver.common.by import By from selenium.webdriver.common.keys import Keys from selenium.webdriver.support.ui import Select from selenium.common.exceptions import NoSuchElementException import unittest , time , re class SeleniumAdminLogin ( unittest . TestCase ): def setUp ( self ): self . driver = webdriver . PhantomJS ( '/opt/phantomjs-1.9.2-linux-x86_64/bin/phantomjs' , port = 9134 ) self . driver . implicitly_wait ( 30 ) self . base_url = \"https://myexample.org\" self . verificationErrors = [] self . accept_next_alert = True def test_selenium_admin_login ( self ): driver = self . driver driver . get ( self . base_url + \"/\" ) driver . find_element_by_id ( \"email\" ) . clear () driver . find_element_by_id ( \"email\" ) . send_keys ( \"admin@example.org\" ) driver . find_element_by_id ( \"password\" ) . clear () driver . find_element_by_id ( \"password\" ) . send_keys ( \"mypassword\" ) driver . find_element_by_id ( \"signin\" ) . click () self . assertEqual ( \"https://myexample.org/home\" , driver . current_url ) self . assertTrue ( self . is_element_present ( By . LINK_TEXT , \"Launch the web app\" )) self . assertTrue ( self . is_element_present ( By . CSS_SELECTOR , \"a.admin > span\" )) driver . find_element_by_css_selector ( \"a.admin > span\" ) . click () self . assertEqual ( \"Group Info\" , driver . find_element_by_css_selector ( \"h1\" ) . text ) def is_element_present ( self , how , what ): try : self . driver . find_element ( by = how , value = what ) except NoSuchElementException , e : return False return True def is_alert_present ( self ): try : self . driver . switch_to_alert () except NoAlertPresentException , e : return False return True def close_alert_and_get_its_text ( self ): try : alert = self . driver . switch_to_alert () alert_text = alert . text if self . accept_next_alert : alert . accept () else : alert . dismiss () return alert_text finally : self . accept_next_alert = True def tearDown ( self ): self . driver . quit () self . assertEqual ([], self . verificationErrors ) if __name__ == \"__main__\" : unittest . main () Basic Polling with Firefox and Mac import datetime import os import sys import time import urllib2 from selenium import webdriver from selenium.webdriver.firefox.firefox_binary import FirefoxBinary FIREFOX_MAC_PATH = '/Applications/Firefox.app/Contents/MacOS/firefox-bin' G_URL = 'https://g.example.com' if __name__ == '__main__' : if len ( sys . argv ) < 2 : print ( 'Usage error: requires a username' ) sys . exit ( 1 ) print ( sys . argv ) username = sys . argv [ 1 ] # https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior new_server_name = datetime . datetime . now () . strftime ( ' %d %b%H%M%S' ) . lower () if os . path . exists ( FIREFOX_MAC_PATH ): driver = FirefoxBinary ( firefox_path = FIREFOX_MAC_PATH ) else : driver = webdriver . Firefox () driver . get ( G_URL ) print ( driver . current_url ) if driver . current_url . endswith ( 'login' ): driver . find_element_by_name ( 'username' ) . clear () driver . find_element_by_name ( 'username' ) . send_keys ( username ) driver . find_element_by_css_selector ( 'input[type=\"submit\"]' ) . click () driver . get ( '{}/deploy/simple_server' . format ( G_URL )) driver . find_element_by_name ( 'name' ) . clear () driver . find_element_by_name ( 'name' ) . send_keys ( '{}-{}' . format ( username , new_server_name )) driver . find_element_by_name ( 'hostname' ) . clear () driver . find_element_by_name ( 'hostname' ) . send_keys ( '{}-{}' . format ( username , new_server_name )) driver . find_element_by_css_selector ( 'input[type=\"submit\"]' ) . click () url = 'https://{}-{}.example.com' . format ( username , new_server_name ) print ( 'requested build of {}' . format ( url )) for _ in xrange ( 20 ): status_code = 404 try : connection = urllib2 . urlopen ( urllib2 . Request ( url )) status_code = connection . getcode () content = connection . read () except Exception as error : print ( 'waiting for {}' . format ( url )) if status_code == 200 : break time . sleep ( 30 ) driver . get ( url ) print ( driver . current_url ) print ( driver . title ) driver . quit () print ( 'done' ) References http://phantomjs.org/release-1.8.html https://github.com/ariya/phantomjs/wiki/API-Reference http://www.realpython.com/blog/python/headless-selenium-testing-with-python-and-phantomjs","tags":"programming","title":"Selenium headless browser automated testing with PhantomJS and Python"},{"url":"https://blog.john-pfeiffer.com/attack-of-the-spiders-bots-and-crawlers/","text":"It is well known that search engines need to index what the contents of webpages are in order to return accurate results to Users. It may not be well known how much traffic that generates. In this sampling from my logs (not representative of anything), 57% of the traffic in the log is from bots (I'm not turning them away, skip to the end on how to do that). Drupal (CMS) is remarkably good at making content for bots to read so it's not surprising they're all slurping, and of course if you want to be popular you need all of those indexes to know about you... How many hits from each kind of bot cat john-pfeiffer.access | wc -l 1108 hits cat john-pfeiffer.access | grep -v \"Baiduspider\" | grep -v \"bingbot\" | grep -v \"YandexBot\" | grep -v \"Sogou\" | grep -v \"Mail.RU\\_Bot\" | grep -v \"Googlebot\" | grep -v \"SISTRIX Crawler\" | grep -v \"MJ12bot\" | wc -l 637 cat john-pfeiffer.access | grep \"Googlebot\" | wc -l 105 hits from https://en.wikipedia.org/wiki/Googlebot cat john-pfeiffer.access | grep \"bingbot\" | wc -l 96 hits from http://en.wikipedia.org/wiki/Bingbot cat john-pfeiffer.access | grep \"Baiduspider\" | wc -l 93 hits from http://baidu.com/search/spider_english.html cat john-pfeiffer.access | grep \"SISTRIX Crawler\" | wc -l 76 hits from http://crawler.sistrix.net cat john-pfeiffer.access | grep \"YandexBot\" | wc -l 64 hits from http://www.botopedia.org/user-agent-list/search-bots/yandex-bot cat john-pfeiffer.access | grep \"Mail.RU\\_Bot\" | wc -l 23 hits from http://www.webmasterworld.com/search_engine_spiders/4520951.htm , http://www.botopedia.org/user-agent-list/search-bots/mailru-bot cat john-pfeiffer.access | grep \"MJ12bot\" | wc -l 12 hits from http://www.majestic12.co.uk/projects/dsearch/mj12bot.php cat john-pfeiffer.access | grep \"Sogou\" | wc -l 7 hits from http://www.botopedia.org/user-agent-list/search-bots/sogou-spider More Info on Bots and Crawlers http://www.incapsula.com/the-incapsula-blog/item/393-know-your-top-10-bots http://searchenginewatch.com/article/2067357/Bye-bye-Crawler-Blocking-the-Parasites (Besides robots.txt you are pretty much left with ban by User-Agent or IP Address Range.)","tags":"linux","title":"Attack of the Spiders, Bots, and Crawlers"},{"url":"https://blog.john-pfeiffer.com/logic-puzzles/","text":"How to have two 1 hour uneven burning fuses burn for 45 mins? light only ONE end burn both ends = 30 mins when it finishes, light the other end Three light bulbs are all off in one room, three light switches in another... ...If you can't see the bulbs from the switch room, how can you tell which connects to which if you can only enter the bulb room once? Flip the first switch for one bulb for many minutes, then turn it off. Flip the second switch on. Enter the bulb room, the bulb on connects to the second switch, the warmer of the two bulbs off is the first switch. At night 4 people arrive at one side of a bridge... ...that can only support 2 people at a time and they have only one flashlight. If each were alone here's how fast they could cross: the first in 15 minutes, the second in 10 minutes, the third in 2 minutes and the fourth in 1 minute. If the flashlight only has 17 minutes how do they cross? The 1 minute and 2 minute cross together (takes 2 minutes). The 1 minute returns (total is now 3 minutes). The 10 and 5 minutes cross together (total is now 13 minutes). The 2 minute returns (the total is now 15 minutes). Finally the 1 minute and 2 minute cross again (17 minutes and complete). 3 bags contain all oranges, all apples, or a mix of oranges and apples... ...BUT the label on each definitely is incorrect. How do you find out which bag has what and how many bags to you need to open? Choose the one labeled \"mixed\" and if it contains apples, therefore the remaining bag which is labelled oranges must not contain all oranges and so must be the mixed one. There are 3 red hats and 2 blue hats in a bag.... Andrew, Bob, and Carol each reach into the box and place one of the hats on their own head. They cannot see what color hat they have chosen. Then they sit down all facing in the same direction: Andrew <-- Bob <--Carol Such that Carol can see what color Bob and Andrew are wearing. Bob can see what color Andrew is wearing. Andrew can't see anyone's hats. The first person to say what color hat they are wearing wins. The winner of the game gets a billion dollars... But if they guess wrong, they die... so nobody wants to try \"just guessing\"... SO... after they grab their hats and sit down, a long time passes... Eventually somebody says \"I have the answer\" and successfully states what color hat they have. Since Carol can see Andrew and Bob, if they both were wearing blue hats she would immediately know that she was wearing red, so unlikely that a long time would pass before she answered (and take the money)... Therefore Andrew and Bob have one red hat and one blue hat. (And Carol knows there's a 33% chance she's got a blue hat and a 66% chance she's got a red hat). Since Bob knows that Carol delayed in answering \"a long time passes\" AND he can see the color of hat that Andrew is wearing, then Either Andrew is wearing a blue hat, and therefore Bob must be wearing a red hat. Or Andrew is wearing a red hat, and therefore Bob must be wearing a blue hat. Bob successfully answers... (unless Carol is more interested in sadistic plots and even though she knows both Bob and Andrew have blue hats she just waits to watch Bob get it wrong...) NOTE: this variation from the NY Times is slightly incorrect as it is essentially asking why Andrew wins... The clue in order for \"Andrew\" to know (the delay) is the same as what Bob uses, but Bob will be able to deduce it much faster. (Again, Carol could be deliberately waiting to watch Andrew fail too...) http://tierneylab.blogs.nytimes.com/2009/03/16/the-puzzle-of-the-3-hats One ball of 8 weighs more than the other 7, how can you identify it with two measurings of a balance scale? Weigh 3 vs 3, if they match then weigh one of the \"good\" ones against one of the remaining ones, if they're unequal you have the heavier one, if they're equal then the last is the heavy one. If the 3 vs 3 do not match then take two of the heavy side and weigh them against each other: if they're unequal you have the heavier one, if they're equal then the last of the heavy side is the heavy one. One ball of 12 weighs more OR less than the other 11, how can you identify it and whether it's heavier or lighter with three measurings of a balance scale? Weigh 4 vs 4, label them ABCD and EFGH. If they balance then weigh \"good\" ABC against WXY. If the second weighing balances then weigh Z against A to see if Z is light or heavy. If the second weighing has WXY heavier or lighter (make a note of which as we have solved whether the imbalance is heavy or light), then weigh W with X. If they balance then Y is the imbalance from step 2 (heavier or lighter). If they do not balance then whichever matches the imbalance from step 2 is the off ball. IF at the first weighing ABCD was lighter than EFGH, in the second weighing rotate such that ABCZ is weighed against D with \"good\" XYZ. If ... Note that this problem can also be solved by carefully measuring the results as the left and right groups of 4 are rotated. 12 becomes 4 v 4 v (4) - if the balance the last 4 are easy if not balanced: 1,2,5 vs 3,6,12 5 pots of 10g coins BUT one contains 9g coins, which pot is off by measuring the weight once? 1 from pot 1, 2 from pot 2, 3 from pot 3, 4 from pot 4, and 5 from pot 5 Expected weight should be 150g: if it's 149 then pot 1 , if it's 148 then pot 2, etc. How can you use a weighted coin (i.e. head more than tails) and still create a fair system of flips? Aggregate the outcomes: treat two flips as a single result such that heads then tails = Heads, and tails followed by heads = Tails. 10 are heads of N coins, how to create 2 groups that have the same number of heads up? Take any 10 from N and flip them (inverted will mirror the number of heads in the N - 10 group) 3 colors of socks: how many to take out until you have a pair? 4 (1 of each and the last 1 must match 1 of the first 3) 100 closed lockers, open all of the lockers on the first pass... ...close every 2nd locker on the second pass, on the third pass and for every 3rd locker open it if it's closed, close it if it's open. After the hundreth pass, how many lockers are open? i.e. after the hundreth the locker 12 was opened on pass 1, closed pass 2, opened pass 3, closed pass 4, opened pass 6, closed pass 12. locker 13 (prime) was opened on pass 1 and closed on pass 13. locker 16: opened on 1, closed on 2, opened on 4, closed on 8, opened on 16! There are 9 perfect squares under one hundred: 1, 4, 9, 16, 25, 36, 49, 64, 81 Incomplete Puzzles Minutes Degrees from 12:00 = mins * 6 Hours Degrees From 12:00 = hours * 30 + mins * .5 Find 1 duplicate in 100 numbers of 1 to 100 = add them up and subtract sum of 1 to 100 ALTERNATIVES: hashmap O(1), sort and then scan looking for a double entry (nLog(n) if mergesort? + n ), n\\&#94;2 if using brute force Sorted List rotated: find min = naive if curr < prev is O(n) Binary Search compare first and mid, if ordered then reset is in partition > mid (RIGHT) find any elem: if > first and Ordered then LEFT BAD: binary search against the \"fixed\" list using the min location as an offset with mod list size ? nxn matrix of numbers in ascending order in both dimensions how would you go about finding if the number y is in the matrix. 5623 players in a tournament, how many matches must be played? 5622 (everybody loses at least once except the winner) Unsolved Puzzles N different flavored Cakes (each with a different Volume) for K people, each person should get an equal volume of Cake (but only a single flavor) V's 1, 2, 3, 4 for 5 people add all volumes together / people = \"theoretical best\" (i.e. 10/5 = 2) start with 2 cakes, 4 + 1, so V=1 for K, next 4 + 2 = doesn't work, 4+3 doesn't work 3 cakes: 4+3+2 = 9 BUT not divisible by 5 1 * 1, 2 * 1, 2 * 1 = waste 5 ( 10-5) discard Random Facts World Population (estimated 2012) = 7 Billion US Population (estimated 2012) = 316 Million Earth circumference (equator) = 24,901.55 miles (40,075.16 kilometers)","tags":"puzzles","title":"Logic Puzzles"},{"url":"https://blog.john-pfeiffer.com/a-more-complete-education/","text":"Introduction to Programming Stanford \"Programming Methodology\" by Mehran Sahami (very fun and Java is a good starting point) http://see.stanford.edu/see/lecturelist.aspx?coll=824a47e1-135f-4508-a5aa-866adcae1111 (also at https://itunes.apple.com/us/itunes-u/programming-methodology/id384232896 ) Harvard http://www.extension.harvard.edu/open-learning-initiative/intensive-introduction-computer-science An Introduction to Interactive Programming in Python https://www.coursera.org/course/interactivepython MIT https://www.edx.org/course/introduction-computer-science-python-mitx-6-00-1x Design of Computer Programs https://www.udacity.com/course/cs101 https://www.udacity.com/course/ud036 https://www.udacity.com/course/cs212 https://www.udacity.com/course/cs253 Networking http://online.stanford.edu/course/intro-computer-networking-winter-2014 Algorithms Algorithms of course! MIT http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-006-introduction-to-algorithms-fall-2011 (also at https://itunes.apple.com/us/itunes-u/introduction-to-algorithms/id341597754 ) http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-046j-introduction-to-algorithms-sma-5503-fall-2005/video-lectures Stanford http://see.stanford.edu/see/lecturelist.aspx?coll=11f4f422-5670-4b4c-889c-008262e09e4e (also at https://itunes.apple.com/us/itunes-u/programming-abstractions/id384232917 ) https://www.coursera.org/course/algo https://www.coursera.org/course/algo2 Princeton https://www.coursera.org/course/algs4partI https://www.coursera.org/course/algs4partII Theoretical Computer Science https://www.udacity.com/course/cs313 Information and Models MIT http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-050j-information-and-entropy-spring-2008/ (also at https://itunes.apple.com/us/itunes-u/information-and-entropy/id424082281 ) University of Michigan https://class.coursera.org/modelthinking-2012-002/class/index Harvard http://www.extension.harvard.edu/open-learning-initiative/bits Software Engineering https://www.coursera.org/course/security https://www.udacity.com/course/ud805 https://www.udacity.com/course/cs258 Vanderbilt https://www.coursera.org/course/posa MIT http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/ PRACTICE, PRACTICE, PRACTICE http://codingbat.com/python (beginner) http://projecteuler.net http://acm.timus.ru http://www.spoj.com/problems/classical http://leetcode.com/onlinejudge http://codeforces.com/problemset https://www.hackerrank.com","tags":"it","title":"A More Complete Education"},{"url":"https://blog.john-pfeiffer.com/jinja2-a-web-html-template-layout-for-everyone/","text":"Web development used to be so hard (and static). (read &#60;bold&#62;) Now everyone realizes (along with version control and automated testing) that decoupling views and displays from dynamic code makes everyone's life easier! (Refresh your colors and layout without having to touch your backend logic! Re-engineer your persistence layer and business logic but leave your uber wow design intact! Allow front end and back end developers to work in parallel!) Jinja2 is an excellent framework for python developers (to go with your uwsgi + django or webapp2) to get html templates (do not repeat yourself!) that can show off all of your backend python data magic with some pizazz. It's fairly easy to do all sorts of powerful things (like accessing variables, loops, etc.) http://jinja.pocoo.org/docs When combined with css and jquery (i.e. tablesorter) you can quickly throw together a decent looking interactive experience. http://jquery.com/download http://tablesorter.com/docs Here's my budding ode to sorting algorithms (yes, Google App Engine is free) http://john-pfeiffer.appspot.com/algorithms #TODO: finish using jquery tablesorter A webapp2 project layout and source code example (please excuse the code formatting, you'll need to imagine the correct indents)... File Layout assets/css/style.css assets/css/tablesorter.css assets/images/css/asc.gif (bg.gif, desc.gif, etc.) assets/js/jquery-1.9.1.min.js assets/js/jquery.tablesorter.min.js mainhandler.py templates/main.html app.yaml (only required if using AppEngine) main.py routes.py app.yaml application : john - jinja2 version : 1 runtime : python27 api_version : 1 threadsafe : true handlers : - url : / favicon . ico static_files : favicon . ico upload : favicon . ico - url : / css static_dir : assets / css - url : / js static_dir : assets / js - url : / images static_dir : assets / images - url : /.* script : main . app libraries : - name : webapp2 version : \"2.5.2\" - name : jinja2 version : latest main.py # -*- coding: utf-8 -*- import webapp2 from routes import entry_points # must be named \"application\" for uwsgi webapp2, in AppEngine it should be \"app\" application = webapp2 . WSGIApplication ( entry_points , debug = False ) routes.py # -*- coding: utf-8 -*- import webapp2 from mainhandler import MainHandler # Map url's to handlers in the handlers module , optionally choosing specific target method and request type entry_points = [ webapp2 . Route ( '/main' , handler = MainHandler , handler_method = 'get' , methods = [ 'GET' ] ), ] mainhandler.py # -*- coding: utf-8 -*- import webapp2 import jinja2 import os # weird hack to ensure we go up a directory level to correctly find the templates directory jinja_environment = jinja2 . Environment ( loader = jinja2 . FileSystemLoader ( os . path . dirname ( __file__ ) ) ) class MainHandler ( webapp2 . RequestHandler ): def get ( self ): # a list of tuples result_list = [ ( 'apples' , 'green' ), ( 'bananas' , 'yellow' ), ( 'cherries' , 'red' ) ] template = jinja_environment . get_template ( 'templates/main.html' ) template_values = { 'title' : 'fruits and colors' , 'body_content' : 'fruits and colors' , 'result_list' : result_list } self . response . content_type = 'text/html' self . response . out . write ( template . render ( template_values ) ) templates/main.html <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"> < html xmlns = \"http://www.w3.org/1999/xhtml\" xmlns = \"http://www.w3.org/1999/html\" > < head > < meta http-equiv = \"Content-Type\" content = \"text/html; charset=utf-8\" /> < title > {{ title }} </ title > < link type = \"text/css\" rel = \"stylesheet\" href = \"/css/tablesorter.css\" /> < script type = \"text/javascript\" src = \"/js/jquery-1.9.1.min.js\" ></ script > < script type = \"text/javascript\" src = \"/js/jquery.tablesorter.min.js\" ></ script > </ head > < body > {{ body_content }} < div > < table id = \"results\" class = \"tablesorter\" > < thead > < tr > < th > Fruit </ th > < th > Color </ th > </ tr > </ thead > < tbody > {% for item in result_list %} < tr > < td >< a href = \"/ {{ item [ 0 ] }} \" > {{ item [ 0 ] }} </ a ></ td >< td > {{ item [ 1 ] }} </ td > </ tr > {% endfor %} </ tbody > </ table > </ div >< br /> < script type = \"text/javascript\" > $ ( document ). ready ( function () { $ ( \"#results\" ). tablesorter ( { sortList : [[ 0 , 1 ]]} ); # sort descending by the first element }); </ script > </ body > </ html > Non Google App Engine /etc/init.d/uwsgi.sh might look like: #!/bin/bash # 2013-02-22 johnpfeiffer start (){ /usr/local/bin/uwsgi --pidfile /var/www/pidfile-uwsgi.pid --touch-reload = /var/www/pidfile-uwsgi.pid --logto2 /var/www/python-john/uwsgi.log --http :8080 --wsgi-file /var/www/main.py --pythonpath /var/www/ & } stop (){ kill -INT ` cat /var/www/pidfile-uwsgi.pid ` sleep 1 } status (){ ps aux | grep uwsgi } case \" $1 \" in status ) status exit 0 ;; start ) start exit 0 ;; stop ) stop exit 0 ;; reload | restart | force-reload ) stop start exit 0 ;; ** ) echo \"Usage: $0 {start|stop|reload}\" 1> & 2 exit 1 ;; esac Alternate Flask Jinja2 Example https://bitbucket.org/johnpfeiffer/tddflask/src","tags":"programming","title":"Jinja2, a web html template layout for everyone"},{"url":"https://blog.john-pfeiffer.com/fix-byobu-infinite-scroll-bug-on-ubuntu-1204-precise-pangolin/","text":"After installing Ubuntu Server 12.04 (Precise Pangolin) I was disappointed to see that one of my favorite utilities, byobu (an improvement on the classic screen = multi ssh screen with status and hotkeys), had an infinite scroll problem. (Quick, type exit before your screen disappears entirely!) Amazingly this bug shipped in the official Ubuntu Release even though byobu 5.17 lists it as a fixed. Easy workaround is: byobu-config Toggle status notifications Use the arrow keys to scroll down and space bar to disable the logo Tab and Enter to Apply -> then Exit Now you can safely use \"byobu\" on the command line in Ubuntu 12.04! (apparently not persisting in 12.04.2?) p.s. Control + a (screen mode) and then Control + a , then c ... now you've got multiple screens! (Control + a + 0 to go to screen 0, control + a + a to jump to the last used screen) https://help.ubuntu.com/community/Byobu Of course you can always use normal BASH navigation: http://www.gnu.org/software/bash/manual/html_node/Commands-For-Moving.html A fix for Windows Putty users ... Window -> Translation -> UTF8 (the ISO-8859 + byobu UTF8 logo = ugh) (An untested alternate workaround: .byobu/status::tmux_left : \"logo\" -> \"#logo\" ) UPDATE for 12.04.2! byobu in Ubuntu 12.04 uses tmux as the backend. You can change this by running byobu-select-backend and selecting screen Thanks for the tip Eric! ALTERNATE: .byobu/status::tmux_left : \"logo\" -> \"#logo\" ) byobu installation and basics installing a better tmux (multiple remote virtual console screens) BYOBU = BETTER GUI FOR SCREEN + CPU/RAM USAGE + DATETIME https://help.ubuntu.com/community/Byobu (an improvement on the classic \"screen\" = multi ssh screen with status and hotkeys) sudo apt-get install byobu byobu-config Toggle status notifications -> spacebar to disable logo -> Apply -> Exit byobu F2 ... or control + a , then c to create a new screen F3 to move to a previous screen F4 to move to the next screen control + a , then 0 = to go to screen zero, etc. control + a , then a = to go to the last used screen byobu-enable will have it start on every ssh connection byobu-disable stops auto-starting of byobu If byobu seems stuck when using vi try Control + Q (or Control + S) Moving the cursor in byobu (f7 and beyond) F7 = scrollback mode , vi like commands to search and copy paste h - Move the cursor left by one character j - Move the cursor down by one line k - Move the cursor up by one line l - Move the cursor right by one character 0 - Move to the beginning of the current line $ - Move to the end of the current line G - Moves to the specified line (defaults to the end of the buffer) / - Search forward ? - Search backward n - Moves to the next match, either forward or backword screen - the previous generation of remote virtual console utility sudo apt-get install screen SSH into your machine and type: screen //to start your screen session. ctrl + a + c //to create a new session ctrl + a + d //to disconnect from the screen session, then log out of your SSH session when you get disconnected/dropped by the network then log back in using SSH screen -ls //shows current screen sessions There is a screen on: 19894.pts-1.servername (Attached) 1 Socket in /var/run/screen/S-username. screen -r //When you're ready to reconnect to the last screen session screen -d 19894 //detach the screen from the other ssh session screen -r 19894 //connect to the screen to see everything's ok ctrl-a-d //detach from the screen who -u //find the id of the \"lost\" ssh session sudo kill sessionid //kill off the disconnected ssh session screen -r 19894 //resume work screen -r -d //force detach an existing and attach a session setting the config for no visual bell! in your /home/USERNAME (or /root) directory touch .screenrc vbell off","tags":"linux","title":"Fix Byobu infinite scroll bug on Ubuntu 12.04 Precise Pangolin"},{"url":"https://blog.john-pfeiffer.com/google-app-engine-python/","text":"Setting Up Sign up for Google App Engine (gmail + SMS verification) \"Google App Engine (often referred to as GAE or simply App Engine, and also used by the acronym GAE/J) is a platform as a service (PaaS) cloud computing platform for developing and hosting web applications in Google-managed data centers.\" The Admin Dashboard is linked to your Google profile https://console.cloud.google.com Login and create an Application (e.g. named john-pfeiffer reachable at http://john-pfeiffer.appspot.com ) Download and extract the SDK (e.g. gae-python.zip) cd google_appengine cp -a new_project_template helloworld Inside is the minimum file structucture required to have an application app.yaml is the configuration file index.yaml is how to override configuration for database indices favicon.ico is the icon that appears in the browser tab or bookmark https://en.wikipedia.org/wiki/Favicon main.py is the entrypoint for your application A first helloworld/app.yaml application : john - pfeiffer version : 1 runtime : python27 api_version : 1 threadsafe : yes handlers : - url : / favicon \\. ico static_files : favicon . ico upload : favicon \\. ico - url : .* script : main . app libraries : - name : webapp2 version : \"2.5.2\" helloworld/main.py #!/usr/bin/env python import webapp2 class MainHandler ( webapp2 . RequestHandler ): def get ( self ): self . response . write ( 'Hi World!' ) app = webapp2 . WSGIApplication ([( '/' , MainHandler )], debug = True ) View the app on your local machine cd google_appengine ./dev_appserver.py ./helloworld INFO 2012-12-27 04:20:20,399 dev_appserver_multiprocess.py:655] Running application dev\\~john-pfeiffer on port 8080: http://localhost:8080 INFO 2012-12-27 04:20:20,399 dev_appserver_multiprocess.py:657] Admin console is available at: http://localhost:8080/\\_ah/admin Upload the app to Google App Engine ./ appcfg .py update helloworld / 07 :44 PM Host : appengine .google.com 07 :44 PM Application : john-pfeiffer ; version : 1 07 :44 PM Starting update of app : john-pfeiffer , version : 1 07 :44 PM Getting current resource limits . 07 :44 PM Scanning files on local disk . 07 :44 PM Cloning 1 static file . 07 :44 PM Cloning 3 application files . 07 :44 PM Uploading 1 files and blobs . 07 :44 PM Uploaded 1 files and blobs 07 :44 PM Compilation starting . 07 :44 PM Compilation completed . 07 :44 PM Starting deployment . 07 :45 PM Checking if deployment succeeded . 07 :45 PM Will check again in 1 seconds . 07 :45 PM Checking if deployment succeeded . 07 :45 PM Will check again in 2 seconds . 07 :45 PM Checking if deployment succeeded . 07 :45 PM Deployment successful . Verify with curl http://john-pfeiffer.appspot.com Download an existing application from Google App Engine mkdir -p /tmp/myapp appcfg.py download_app -A john-pfeiffer /tmp/myapp This will use OAuth 2 to open a browser/give you a link where you can confirm the action Once confirmed it will download the latest version off your application code and files This is independent of and not a replacement for version control! Updating the Application for routes and POST requests It is easy to use the MVC pattern while inheriting from the framework https://webapp-improved.appspot.com/guide/handlers.html app.yaml Note that the version has to be explicitly updated in order to deploy something new application : john - pfeiffer version : 6 runtime : python27 api_version : 1 threadsafe : yes handlers : - url : / favicon . ico static_files : favicon . ico upload : favicon . ico - url : /.* script : main . app libraries : - name : webapp2 version : \"2.5.2\" main.py #!/usr/bin/env python import webapp2 class MainHandler ( webapp2 . RequestHandler ): def get ( self ): self . response . write ( 'hello John Pfeiffer!' ) class PageOne ( webapp2 . RequestHandler ): def get ( self ): self . response . write ( \"\"\" PageOne \"\"\" ); class PageTwo ( webapp2 . RequestHandler ): def get ( self ): self . response . write ( 'PageTwo' ) def post ( self ): get_values = self . request . GET post_values = self . request . POST self . response . write ( str ( get_values ) + \" \\n \" ) self . response . write ( str ( post_values )) app = webapp2 . WSGIApplication ([ ( '/' , MainHandler ), ( '/page-one' , PageOne ), ( '/page-two' , PageTwo ) ], debug = True ) def main (): run_wsgi_app ( application ) if __name__ == \"__main__\" : main () Update the google app engine application code that is running in the cloud ./appcfg.py update john-pfeiffer update does not publish the new version yet ./appcfg.py set_default_version john-pfeiffer or you can use the WebUI dashboard to change the default or delete a Version for an Application Instance https://cloud.google.com/appengine/docs/python/config/appref A simple single file CRUD app using the Google AppEngine Database Google AppEngine applications can leverage the platforms NoSQL database https://cloud.google.com/appengine/docs/python/datastore/ The example application below also shows how to override the default 404 and 500 errors with custom jinja2 templates which would require installing the jinja2 dependency and an extra subdirectory named templates with the HTML https://blog.john-pfeiffer.com/jinja2-a-web-html-template-layout-for-everyone/ https://cloud.google.com/appengine/docs/python/ndb/ has improved and deprecated the DB Datastore library used below :::python !/usr/bin/env python 2013-01-20 johnpfeiffer import os import logging import traceback import cgi import datetime import webapp2 import jinja2 from google.appengine.ext import db jinja_environment = jinja2.Environment( loader=jinja2.FileSystemLoader( os.path.dirname( os.path.dirname( file ) ) ) ) class Node( db.Model ): id = db.StringProperty() name = db.StringProperty() parent_id = db.StringProperty() date = db.DateTimeProperty( auto_now_add = True ) TODO: navigation bar class MainPage( webapp2.RequestHandler ): def get( self ): self.response.out.write( ' ' ) self.response.out.write( \"\"\"Welcome <form action= \"/listnodes\" method= \"get\" > <input type= \"submit\" value= \"List Nodes\" > </form> <br/> <form action= \"/createnodeform\" method= \"get\" > <input type= \"submit\" value= \"Create Node\" > </form> <form action= \"/deletenode\" method= \"post\" > <label> id </label></td><td><input type= \"text\" id= \"id\" name= \"id\" /> <input type= \"submit\" value= \"Delete Node\" > </form> \"\"\" ) self.response.out.write( \" </body></html> \" ) class ListNodes( webapp2.RequestHandler ): def get( self ): self.response.out.write( ' ' ) self.response.out.write( 'Welcome ' ) q = db.Query( Node ) q.order( \"name\" ) q.fetch( 100 ) self.response.out.write( \"%s , %s , %s ( %s ) \" % ( \"name\" , \"id\" , \"parent_id\" , \"created\" ) ) for node in q : self.response.out.write( \"%s , %s , %s ( %s ) \" % ( node.name , node.id , node.parent , node.date ) ) self.response.out.write( \" </body></html> \" ) TODO: use template system class CreateNode( webapp2.RequestHandler ): def get( self ): self.response.out.write( ' ' ) self.response.write( 'Create uses POST' ) self.response.out.write( \" \" ) def post( self ): self.response.out.write( ' ' ) post_values = self.request.POST # todo extract to helper for input validation and sanitization name = post_values[ \"nodename\" ] id = post_values[ \"id\" ] parent_id = post_values[ \"parentid\" ] if( name == None or id == None or parent_id == None ): self.response.out.write( 'ERROR: DEBUG:' , post_values ) else: node = Node() node.name = name node.id = id node.parent_id = parent_id node.put() self.response.out.write( 'successfully created: ' + name ) self.response.out.write( \"\"\" <form action= \"/listnodes\" method= \"get\" > <input type= \"submit\" value= \"List Nodes\" > </form> \"\"\" ) self.response.out.write( \" </body></html> \" ) class CreateNodeForm( webapp2.RequestHandler ): def get( self ): self.response.out.write( ' ' ) self.response.out.write( \"\"\" node name id parent id \"\"\" ) self.response.out.write( \" \" ) TODO: use template system class DeleteNode( webapp2.RequestHandler ): def get( self ): self.response.out.write( ' ' ) self.response.write( 'Delete uses POST' ) self.response.out.write( \" \" ) def post( self ): self.response.out.write( ' ' ) post_values = self.request.POST # todo extract to helper for input validation and sanitization id = post_values[ \"id\" ] q = db.Query( Node ) # keys_only is faster and cheaper than retrieving the entities q.filter( \"id=\" , id ) q = Node.all( keys_only = True ).filter( \"id=\", id ) node_to_delete = q.run() db.delete( ) node_key = # key self.response.out.write( \"%s , %s , %s ( %s ) <br/> \" % ( \"name\" , id , \"parent_id\" , node_to_delete ) ) self.response.out.write( \" </body></html> \" ) url handler below ----------------------------- app = webapp2.WSGIApplication( [ ( '/', MainPage ), ( '/listnodes' , ListNodes ), ( '/createnode' , CreateNode ), ( '/createnodeform' , CreateNodeForm ), ( '/deletenode' , DeleteNode ) ], debug = True ) def handle_404 (request , response , exception) : template_dictionary = { 'title' : 'ERROR 404' , 'body_content' : exception.status } template = jinja_environment.get_template ( 'templates/error.html' ) response.write ( template.render ( template_dictionary ) ) response.set_status ( exception.status_int ) def handle_500 (request , response , exception) : logging.error ( traceback.print_exc ( ) ) logging.error ( exception ) template_dictionary = { 'title' : 'Meow' , 'body_content' : 'Meow. Meow meow meow, meow meow.' } template = jinja_environment.get_template ( 'templates/error.html' ) response.write ( template.render ( template_dictionary ) ) response.set_status ( 500 ) app.error_handlers [ 404 ] = handle_404 app.error_handlers [ 500 ] = handle_500","tags":"programming","title":"Google App Engine Python"},{"url":"https://blog.john-pfeiffer.com/server-operations-cloud-versus-build-your-own/","text":"Here's my response to Jeff Atwood's calculations and incorrect conclusion about Building Your Own Server: I'm not sure I know when an organization's Production deployment doesn't need reduced complexity/costs (people!), flexibility for load, and redundancy... \"Anyway, I want to make it clear that building and colocating your own servers isn't (always) crazy, it isn't scary, heck, it isn't even particularly hard. In some situations it can make sense to build and rack your own servers, provided ... you want absolute top of the line server performance without paying thousands of dollars per month for the privilege you are willing to invest the time in building, racking, and configuring your servers you have the capital to invest up front you desire total control over the hardware you aren't worried about the flexibility of quickly provisioning new servers to handle unanticipated load you don't need the redundancy, geographical backup, and flexibility that comes with cloud virtualization\" http://www.codinghorror.com/blog/2012/10/building-servers-for-fun-and-prof-ok-maybe-just-for-fun.html Hi Jeff, long time fan, first time commenter... I love building servers too and I've managed a small group of servers, I personally use Linode, and my currently company uses AWS and some internal servers... You would agree that in coding you pick the right tool for the job (scientific computing would use a different technology stack than a standard ecommerce startup website)... AWS is elastic (you pay a premium for being to scale up or down - and there's value to the agility with which you can change or add new services) AWS RDS is a huge improvement over managing MySQL replication, and they have Elastic Load Balancing and lots of other addons that take serious Ops chops to create and maintain Server operations cost is not the raw hardware: The biggest cost in Ops is people (same as coding), so leveraging Amazon saves on how many people you need to pay to manage your server farm (yes, SysAdmins take holidays and change jobs so cost = N+1 )... you can outsource half way by colocating but setting up the redundancy, monitoring, auto scaling, etc. becomes a physical pain (you want West Coast and East Coast servers, right?). The infrastructure of cooling, UPS, network (bandwidth!), backups, etc. is also a big factor in Operations (does your server room have building security? a backup generator?) My point is that for a stealth mode startup or any internal lab testing buying servers is a no brainer - do it with ESXi or OpenStack and hack away! BUT for Production you'll need some Cloud strategy (AWS competitors: RedHat OpenShift, RackSpace Cloud, IBM, ATT Compute, Google AppEngine, etc. means lower prices and improved services) Unless, as you've already mentioned, \"if you happen to have hanging around a pile of cash and tech expertise that's underutilized...\"","tags":"it","title":"Server Operations: Cloud versus Build Your Own"},{"url":"https://blog.john-pfeiffer.com/tomcat-deployment-on-openshift-for-free/","text":"openshift is the cloud PaaS offering from RedHat Prerequisites and dependencies sudo apt-get update sudo apt-get install ruby1.9.3 git-core //yay for ubuntu 12.04 sudo gem install rhc //red hat openshift client OpenShift Client tools setup rhc setup Created local config file: /home/ubuntu/.openshift/express.conf The express.conf file contains user configuration, and can be transferred to different computers. No SSH keys were found. We will generate a pair of keys for you. 2: No such file or directory Created: /home/ubuntu/.ssh/id_rsa.pub Your public ssh key must be uploaded to the OpenShift server. Would you like us to upload it for you? (yes/no) yes rhc commands rhc -h rhc domain show prompts for password rhc app create -h Valid application types are (nodejs-0.6, ruby-1.9, jbossas-7, python-2.6, jenkins-1.4, ruby-1.8, jbosseap-6.0, diy-0.1, php-5.3, perl-5.10) rhc app create -a john -t diy-0.1 rhc app show -a john rhc app cartridge list Your local git repo ON YOUR LOCAL MACHINE BROWSE TO WHERE YOU WANT TO STORE YOUR GIT REPO git clone ssh://a261d0fc2932413694456e3473fdc972@APPNAME-DOMAIN.rhcloud.com/~/git/... git status git pull REPO LAYOUT of ~/john/repo .openshift/action_hooks/start - Script that gets run to start your application .openshift/action_hooks/stop - Script that gets run to stop your application .openshift/action_hooks/pre_build - Script that gets run every git push before the build .openshift/action_hooks/build - Script that gets run every git push as part of the build process ( on the CI system if available ) .openshift/action_hooks/deploy - Script that gets run every git push after build but before the app is restarted .openshift/action_hooks/post_deploy - Script that gets run every git push after the app is restarted diy misc README static/ If it exists externally exposed static content goes here CHANGES ARE ONE DIRECTIONAL FROM THE GIT CLONE TO THE OPENSHIFT BOX mv diy/testrubyserver.rb ../misc mv diy/index.html ../misc git add -A git commit -m \"moved initial test stuff to /misc\" git push if the app is running then a git push automatically ... Counting objects: 6, done. Delta compression using up to 4 threads. Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 607 bytes, done. Total 4 (delta 1), reused 1 (delta 0) remote: Stopping application... remote: Done remote: ~/git/john.git ~/git/john.git remote: ~/git/john.git remote: Running .openshift/action_hooks/pre_build remote: Running .openshift/action_hooks/build remote: Running .openshift/action_hooks/deploy remote: Starting application... remote: Done remote: Running .openshift/action_hooks/post_deploy ssh 33d90ea45fd91e42096651d937e@john-pfeiffer.rhcloud.com //note that HOME is /var/lib/stickshift/12315longidentifier cd app-root/data wget http://mirror.cc.columbia.edu/pub/software/apache/tomcat/tomcat-7/v7.0.29/bin/apache-tomcat-7.0.29.tar.gz tar -xzvf apache-tomcat-7.0.29.tar.gz expands to 13MB, note that JAVA is already installed by default rm apache-tomcat-7.0.29.tar.gz Openshift ports and proxy Since OpenShift has a proxy setup that passes port 80 to your local server on port 8080, BUT OpenShift does not allow users to bind to any port below 15000 other than 8080, so... ssh 33d90ea45fd91e42096651d937e@john-pfeiffer.rhcloud.com env shows you all of the environment variables in the openShift multitenant config env | grep INTERNAL OPENSHIFT_INTERNAL_PORT=8080 OPENSHIFT_INTERNAL_IP=127.13.29.1 vi app-root/data/apache-tomcat-7.0.29/conf/server.xml escape key then :x (save and quit) cd app-root/data/apache-tomcat-7.0.29/bin sh startup.sh && tail -f ../logs/* (this is how you can start it manually from within ssh, you'll have to stop it manually too!) ADDING Tomcat to your default start and stop scripts (which are used during every git push) In your local git repo there is a hidden directory \".openshift\" cd APPNAME/.openshift/action_hooks vi .openshift/action_hooks/start #nohup $OPENSHIFT_REPO_DIR/diy/testrubyserver.rb $OPENSHIFT_INTERNAL_IP $OPENSHIFT_REPO_DIR /diy > $OPENSHIFT_LOG_DIR /server.log 2> & 1 & cd $OPENSHIFT \\_ DATA \\_ DIR/apache-tomcat-7.0.29/bin nohup sh startup.sh echo \"completed tomcat7 startup\" vi .openshift/action_hooks/stop cd $OPENSHIFT_DATA_DIR /apache-tomcat-7.0.29/bin nohup sh shutdown.sh echo \"completed tomcat7 shutdown\" exit 0 git add -A `git commit -m \"removed testrubyserver.rb and added tomcat to start/stop scripts\" git push http://APPNAME-DOMAINNAME.rhcloud.com Autodeploy the latest MOVE YOUR WEBAPPS DIRECTORY TO THE GIT REPO SO THAT A GIT PUSH WILL AUTO DEPLOY THE NEWEST mv $OPENSHIFT_DATA_DIR/apache-tomcat-7.0.29/webapps ~/john/repo/diy/webapps ln -s ~/john/repo/diy/webapps webapps Strongly advised to remove the manager and example apps (just deploy your .war's) rhc app stop -a APPNAME -p YOURPASSWORD yes, it uses your RHCloud account username and password for app management ssh 33d90ea45fd91e42096651d937e@john-pfeiffer.rhcloud.com mv ~/app-root/data/apache-tomcat-7.0.29/webapps/* app-root/repo/misc mv app-root/repo/misc/ROOT ~/app-root/data/apache-tomcat-7.0.29/webapps rhc app start -a APPNAME -p YOURPASSWORD ONCE YOU'VE SSH'D IN... help ps|ls| ctl_app start [stop|restart|status] mysql | mongo | quota NOTE: sometimes it's easier to use a UI https://openshift.redhat.com/app/console/applications My Account -> Public Keys My Applications -> APPLICATION_NAME -> rhc app add-alias -a myapp --alias myapp.net Future Thoughts Eclipse + m2e (maven plugin) + jetty plugin for fast and easy dependency management -> mvn install + added custom script can put your .war into your local openshift repo for continuous deployment. https://www.openshift.com/","tags":"programming","title":"Tomcat deployment on Openshift for free"},{"url":"https://blog.john-pfeiffer.com/creating-economies-of-scale-in-software-development/","text":"Planning to be a successful software company Economies of Scale in Software Development is about planning to be successful. Software development is rather cheap when compared to physical manufacturing but that doesn't mean it doesn't cost money to create a product or service. The classic myth of the lone wolf hacker who creates a viral product runs counter to the reality of billions of dollars generated by the software industry. Elements of a Software Team Consider the many elements required to create a software product: Developers (hiring, location, communication, etc.) Developer tools (IDE, compiler, debugger, etc.) QA (you do run tests of some sort, don't you?!?!) Distribution/Release (shrink wrapped or delivered by Internet continuously, it still costs something) Support (assuming you've actually got users) Making Software Better, Faster, Cheaper What can you do to make your software cheaper, faster, and most importantly, better? 1. Use Open Source (like facebook, twitter, google, netflix). Leverage the crowd and reduce your costs (e.g. not buying software licenses, not paying someone to audit your software licenses, etc.) Spending money on licenses becomes cost prohibitive for large successful organizations. More importantly, get the quality of years of development by lots of different perspectives. The homogeneous gets wiped out by a single cause, get diversified! Concerns about competitive advantage based on your supply chain (e.g. the myth \"using Open Source tools doesn't give you an edge\") are unfounded as: a. If your competitive advantage is a \"secret supplier\" then your business will go bust as soon as anyone else finds out your secret. b. If you're paranoid enough to worry your competitors are messing with your tools then you shouldn't trust a closed source vendor - keep it in the open where everyone's watching. Giving back to open source projects you depend on has a multiplier effect: a healthy community project is far cheaper than paying full time to support your critical requirements and has built in support and marketing. http://readwrite.com/2013/10/17/is-facebook-the-worlds-largest-open-source-company http://www.cnet.com/news/worlds-biggest-open-source-company-google/ http://techblog.netflix.com/2010/12/why-we-use-and-contribute-to-open.html 2. Standardization Consider the trend of large clusters of commodity hardware. Using the same internal development tools means it's easier to focus on the real problems and not get lost in translation. This doesn't mean sticking to old versions or being afraid to innovate, but make success a formula and not an accident. Larger efficiencies can be generated by getting a decent level of determinism in the workflow. This means use the same IDE, the same dependency libraries, communication channels, etc. If you have rock stars that can't learn the common tool or won't teach others why their method is better then you run the risk of a dysfunctional \"all-star\" time bomb. This also applies to fairness: office perks, salaries, etc. Make it an organization about transparent achievable results, not a labyrinth of back room exceptions. http://techblog.netflix.com/2011/08/building-with-legos.html http://www.quora.com/Why-hasn-t-Facebook-migrated-away-from-PHP http://spectrum.ieee.org/telecom/internet/under-the-hood-at-google-and-facebook http://www.infoq.com/presentations/Development-at-Google 3. Reduced specialization (and silos) System Administrator's whose sole purpose is to watch machines is a dying breed. An army of QA who manually walk through the same test plan over and over is also legacy. Start with \"DRY - Do Not Repeat Yourself\". Develop automation early and make the process simple and obvious. (i.e. Amazon's internal conversion to APIs http://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis ). This also means avoiding purchasing hardware (and the associated inventory/maintenance/overhead). The more virtual servers you buy the larger volume discount you can negotiate. It's not entirely about cost: you're renting somebody else's implementation of best practice you need to make your product number one, distractions from that reduce velocity. Your feature set will continually grow. (Even with careful pruning). By looking at software as the solution to quality/delivery/maintenance/etc. you can reduce your running cost to a fraction of what it would be AND be able scale up quickly when you become wildly successful. Note: people are still irreplaceable but by having a lot more Developers (and less unique specialists) you can distribute the load more evenly. Getting more cross functional individuals means less silos and less communication gaps. The network mesh effect destroys productivity if everyone's a separate bottleneck. http://www.learningapi.com/blog/archives/000079.html http://techblog.netflix.com/2012/06/netflix-operations-part-i-going.html http://www.infoq.com/articles/sw-eating-silos https://hbr.org/2011/12/quiet-but-unsubtle-innovation.html 4. Make it real Code that lives solves problems and is valuable. Uncommitted, unused, and otherwise un-useful code costs money to debug and deconstruct; much worse it costs time. Continuous integration gets your stable unit tested code some real world bruises. Continuous deployment gets your code crunching data and making users happy. http://royal.pingdom.com/2010/06/18/the-software-behind-facebook/ http://www.infoq.com/presentations/cd-linkedin Critical Leadership There will be bugs. Far more important are to ensure the product fits the market and have the ability to quickly fix what's broken - sometimes amputation due to business needs is critical (i.e. how Flickr the photo sensation was born http://en.m.wikipedia.org/wiki/Flickr ) Your team must be able to execute. You lead them by example (both the late nights and the high fives) and these real people will deliver exponential success. http://www.fastcompany.com/3044952/hit-the-ground-running/how-bill-gates-andy-grove-and-steve-jobs-found-success-without-busine https://hbr.org/2012/04/the-real-leadership-lessons-of-steve-jobs https://blog.kissmetrics.com/brian-chesky-alfred-lin-culture/ http://www.fastcompany.com/3007268/where-are-they-now/not-happy-accident-how-google-deliberately-designs-workplace-satisfaction","tags":"it","title":"Creating Economies of Scale in Software Development"},{"url":"https://blog.john-pfeiffer.com/an-evolution-of-questions-and-answers/","text":"Socrates oral questions (and answers) were immortalized by Plato's writings. I love books, the distillation of the wisdom of the ages, and the libraries where they are enshrined and shared. Visiting a library is an exciting insight into the culture and values of a location. Encyclopedias, Dictionaries, and all manner of reference books were the compendium of answers; questions had to be researched (sometimes involving extensive legwork, detective work, and persistence). Digitization of books (e.g. encyclopedia on cd-rom) enabled huge improvement in the access and distribution of information. Physical bulletin boards and community centers are also places of information exchange; digital communication (modems!) transformed the reach and capacity. Television and Video, despite many attempts, has long remained a one way medium. (Though too there's something inherently relaxing about passive entertainment). The Internet (and especially Google search) means specialist websites and esoteric questions are a couple links away; it has also enabled crowd sourcing answers. I'm amazed at the speed and accuracy of answers aggregated by Wikipedia, StackExchange, Quora, etc. (which admittedly still require filtering and research for correctness and comprehension). It's gotten so that now I receive answers to questions which I didn't realize I had!","tags":"it","title":"An Evolution of Questions and Answers"},{"url":"https://blog.john-pfeiffer.com/mid-2012-technology-and-business-prediction-for-2013-and-beyond/","text":"The well known perceived future business model is constant (mobile) targeted (user profiling) advertising (everywhere) with instant purchase (online + mobile) and delivery (streaming) So Apple (Android) + Facebook + Google + Amazon + (Netflix?) , right? I think there's more to think about: the infrastructure is the gold; Hardware Manufacturers (CPU + RAM + SSD), Servers, Routers, ISP's, Content Producers, Transaction Processing, Data and User Platforms. The new ecosystem is inherently more distributed (more users globally, more devices) and fragmented (more OS, Apps, Content Producers, etc.) than the \"Microsoft Era\". Complexity increases with more data to aggregate/index/filter and many smaller transactions: consider previously purchasing a single desktop, monitor, (Windows bundled with it) and MS Word + one or two apps (Quickbooks or Photoshop). Now it's a laptop + phone + tablet (Windows + Android + iOS), each with many apps (including online services like Facebook) and the challenge of maintaining a consistent view of security, data, and even application state. Business purchases used to be larger, even grouped for volume (i.e. a company orders 1,000 desktops + monitors at $1,500 a piece); now it's bring your own device ($500) and many small app and content purchases for 99 cents to $10. Companies that solve the multi platform and scalability problems (along with of course actually creating a product that fulfills a need or desire) will get bigger faster . The opportunities are growing exponentially (network effect) for the talented, hard working teams that can execute.","tags":"it","title":"Mid 2012 technology and business prediction for 2013 and beyond"},{"url":"https://blog.john-pfeiffer.com/amazon-s3-bucket-html-redirect/","text":"Goal: More efficient and less error prone method to update a regularly changed downloadable file Web page redirects enable you to change the URL of a web page on your S3 hosted website (e.g., from www.example.com/oldpage to www.example.com/newpage) without breaking links pointing to the old URL. Users accessing the old URL will automatically be redirected to the new one. Redirect a single object Amazon updated their functionality to allow per object meta data based redirects: WebUI Object properties Metadata Add Website Redirect Location either /newpage.html (internal redirect) or http://example2.com/page.html (external redirect) Or PUT the object (or a zero byte file) with the x-amz-website-redirect-location header set (i.e. http://example2.com/page.html) Or use the universal static html redirect method: So upload the following download.html that includes < head > < meta http-equiv = \"refresh\" content = \"0; url=http://example.com/file-v2.tar.gz\" > </ head > Whenever you have a new version of your file you only have to upload a new \"download.html\" with an updated meta refresh header and any Users and links will always download the newest version of your file. Note, javascript may help you open the download and then return to the original page but have strange interactions for a .txt file... < script type = \"text/javascript\" > <!-- window . location = \"http://example.com/file-v2.tar.gz\" //--> < /script> Redirect a domain With two domains, example1.com and example2.com: Create an s3 bucket for example1.com (static web hosting) Set bucket property in the \"Static Web Hosting\" section, select \"Redirect all requests to another host name\" to redirect to example2.com Configure Route53 (AWS DNS) for example1.com to have an A record that has an Alias Target as an S3 Website Endpoint (the bucket from step 1) Register example1.com to use the Amazon name servers (from Route53) Add any further subdomain redirects (e.g. www.example1.com) by repeating steps 1 and 2 more info http://docs.aws.amazon.com/AmazonS3/latest/dev/how-to-page-redirect.html","tags":"programming","title":"Amazon S3 Bucket HTML Redirect"},{"url":"https://blog.john-pfeiffer.com/self-configuration-tests-for-scalability/","text":"Goal: Customers (Users = system admins) able to self verify configuration. The more a user can self verify with software, the less support required per customer deployment. I used the Java API to quickly prototype a solution and exported a runnable .jar file... https://code.google.com/p/atmos-java/ Java source code for a configuration validation import com.emc.esu.api.EsuApi ; import com.emc.esu.api.rest.EsuRestApi ; import com.emc.esu.api.EsuException ; import com.emc.esu.api.ObjectId ; import com.emc.esu.api.ObjectInfo ; import org.apache.log4j.Level ; import org.apache.log4j.Logger ; import org.apache.log4j.ConsoleAppender ; import org.apache.log4j.PatternLayout ; public class AtmosConnect { static Logger rootLogger = Logger . getRootLogger (); public static void main ( String [] args ) { if ( ! rootLogger . getAllAppenders (). hasMoreElements () ) { rootLogger . setLevel ( Level . INFO ); rootLogger . addAppender ( new ConsoleAppender ( new PatternLayout ( PatternLayout . TTCC_CONVERSION_PATTERN ) ) ); rootLogger . info ( \"Entering application\" ); } if ( args . length != 4 ) { System . out . println ( args . length + \" does not equal the 4 required arguments.\" ); System . out . println ( \"version 0.1: java -jar AtmosConnect.jar HOST PORT SUBTENANTID/UID SECRETKEY\" ); System . exit ( 1 ); } String HOST = args [ 0 ]; int PORT = Integer . parseInt ( args [ 1 ] ); String FULLTOKENID = args [ 2 ]; String SECRETKEY = args [ 3 ]; displayConnectionCredentials ( HOST , PORT , FULLTOKENID , SECRETKEY ); EsuApi myEsuAPI = null ; try { myEsuAPI = new EsuRestApi ( HOST , PORT , FULLTOKENID , SECRETKEY ); } catch ( EsuException e ) { System . out . println ( \"EsuRestApi Constructor failed.\" ); System . out . println ( e . getMessage () ); e . printStackTrace (); } ObjectId myObjectId = null ; myObjectId = createAtmosObject ( myObjectId , myEsuAPI ); displayAtmosObject ( myObjectId , myEsuAPI ); deleteAtmosObject ( myObjectId , myEsuAPI ); rootLogger . info ( \"Application Successful\" ); } // end main() private static void displayConnectionCredentials ( String HOST , int PORT , String FULLTOKENID , String SECRETKEY ) { System . out . println ( \"Connecting to Host: \" + HOST ); System . out . println ( \"Connecting on Port: \" + PORT ); System . out . println ( \"Full Token ID: \" + FULLTOKENID ); System . out . println ( \"Secret Key: **************\" ); //System.out.println( \"Secret Key: \" + SECRETKEY ); } private static ObjectId createAtmosObject ( ObjectId myObjectId , EsuApi myEsuAPI ) { try { myObjectId = myEsuAPI . createObject ( null , null , null , \"application/octet-stream\" ); System . out . println ( \"Created object: \" + myObjectId . toString () ); } catch ( Exception e ) { System . out . println ( \"Create Object failed.\" ); System . out . println ( e ); //e.printStackTrace(); // JUnit Tests: Invalid Host, Port, SubtenantID, UID, Shared Secret, etc. } return myObjectId ; } private static void displayAtmosObject ( ObjectId myObjectId , EsuApi myEsuAPI ) { ObjectInfo myObjectInfo = null ; myObjectInfo = myEsuAPI . getObjectInfo ( myObjectId ); //System.out.println( \"ObjectInfo: \" + myObjectInfo.toString() ); //System.out.println( \"ObjectInfo as XML: \" + myObjectInfo.getRawXml() ); } private static void deleteAtmosObject ( ObjectId myObjectId , EsuApi myEsuAPI ) { try { System . out . println ( \"Trying to delete Server Object: \" + myObjectId . toString () ); myEsuAPI . deleteObject ( myObjectId ); System . out . println ( \"Test Object deleted on Server: \" + myObjectId . toString () ); } catch ( Exception e ) { System . out . println ( \"Delete Object \" + myObjectId . toString () + \" failed \" + e ); //e.printStackTrace(); } } } // end class Verify the bash commands Verify bash commands that will help extract the configuration: /bin/grep -i 'emcIpAddress=' /var/lib/tomcat6/webapps/storagegateway/WEB-INF/app.properties | cut -f 2 -d '=' Modify the perl console menu script start on stopped rc RUNLEVEL=[2345] stop on runlevel [!2345] respawn exec /sbin/getty -n -l /etc/init.d/CONSOLEMENU.pl 38400 tty1 sudo vi /etc/init.d/CONSOLEMENU.pl sub testatmosconnect () { system ( \"clear\" ); my $sourcefilename = \"/var/lib/tomcat6/webapps/storagegateway/WEB-INF/app.properties\" ; my $applicationfilename = \"/var/lib/tomcat6/oxygen-storagegateway/atmosconnect.jar\" ; if ( ( - e $sourcefilename ) and ( - e $applicationfilename ) ) { my $emcipaddress = qx( /bin/grep -i 'emcIpAddress=' $sourcefilename | cut -f 2 -d '=' ) ; chomp ( $emcipaddress ); my $emcportnumber = qx( /bin/grep -i 'emcPortNumber=' $sourcefilename | cut -f 2 -d '=' ) ; chomp ( $emcportnumber ); my $emcuid = qx( /bin/grep -i 'emcUid=' $sourcefilename | cut -f 2 -d '=' ) ; chomp ( $emcuid ); my $emcsharedsecret = qx( /bin/grep -i 'emcSharedSecret=' $sourcefilename | cut -f 2 -d '=' ) ; chomp ( $emcsharedsecret ); my @status = qx( /usr/bin/java -jar $applicationfilename $emcipaddress $emcportnumber $emcuid $emcsharedsecret ) ; print \"@status\\n\" ; } else { print \"$sourcefilename or $applicationfilename does not exist.\\n\" ; } } # end atmosconnect() Goal Achieved A user can self test if they've misconfigured the VM or there's missing conf files/app, etc. SUCCESS! Next steps: further unit and system tests, debug, refactor","tags":"programming","title":"Self configuration tests for scalability"},{"url":"https://blog.john-pfeiffer.com/technology-careers-part-1/","text":"A System Administrator is like a Firefighter Always available to fix an emergency, but without the glamour; after putting out a fire a SysAdmin has to report (with vague implications of guilt): what happened, why it happened, and how it can be prevented in the future. (Who's fault is it?) Good System Administrators are well paid but usually work on the invisible and complex \"back end\" and are on \"on call\". (Yes, your dinner, sleep, and other private activities will be interrupted.) A stable, in demand, career since it's hard to fire a good System Administrator but there's often a belief that the position could/should be outsourced. (Ever hear the story of the SysAdmin who was so good at automating server tasks and desktop deployments that he put himself out of a job?) The overly general term \"Operations\" is often applied. In non technology focused companies you might be managed by a Chief Operations Officer who is more concerned with facilities (buildings), logistics (trucks, ships, planes, etc.) or even janitorial services. If you're not careful, methodical, detailed, and cool headed then this may not be the right path for you. (Cynical is optional but it helps a lot). An IT Manager is like an Accountant Always asked to do more with less. In fairness an IT Manager gets to be more creative (legally) as innovation and change drive technological efficiency. Good IT Managers spend as little time as possible in meetings but can organize and automate the heck out of everything. Diagrams, Business Cases, Reporting, Schedules; not often a hands on role. Being in charge of IT resources is well paid and it's a stable career (you're in charge of the budget, right?) but when \"downsizing\" occurs you may be considered an acceptable loss (if your business cases prove you've saved/earned enough then it's usually a non issue). Gluing together services often means jugging time zones (remote contractors and services are the norm) and balancing/network internal departmental demands. If you're not good at understanding people or technology (yes you have to hire/fire people and purchase wisely) then this may not be the right path for you. A Developer is like an Artist Eccentric, misunderstood, and something rare for the Arts, well paid. Good Developers understand it's Art with a purpose and apply rigorous engineering principles to their work (since software runs things like airplanes and hospitals this is a good thing). It's well paid but sometimes unpredictable work (software project done = goodbye expensive labor), luckily there's an endless demand for the foreseeable future. (Interestingly it often takes awhile to discover a \"bad\" developer and even then employers are reluctant to let them go). If you're not creative and logical; unable to unriddle paradoxes (yes it's a required dependency but no we don't want it), or just don't want to spend most of your waking hours in front of a screen (human contact optional), then this might not be the right path for you.","tags":"it","title":"Technology Careers (part 1)"},{"url":"https://blog.john-pfeiffer.com/test-driven-development-introduction-and-qa-domains/","text":"Why Test Driven Development \"6% progress\" in engineering Cost: Organizations want to reduce the time and cost associated with releasing code including the post release support, debugging, and maintenance. Agility: TDD enables faster product reactions to market changes and more drastic product changes and continued extension of successful products Growth: it's a proven method for Organizations building a strong brand and looking for 6% compound growth, enables larger teams and rotation of personnel Individual Time: less time is spent in long debugging session or writing \"dead code\" (code that never reaches production for either Business reasons – i.e. no customer demand, or Engineering decisions – i.e. quality of the overall product never reaches a release point). Individual Careers: ability to work on more complex projects and clear contribution to business success Individual Satisfaction: confidence in what's been delivered and happier users Acceptance Test Driven Development Everyone agrees on done! Automated Tests create concrete artifacts for all stakeholders (i.e. PM, QA, Engineering, etc.) to understand scope, review progress, and agree on \"done\". This vastly improves inefficiencies in communication, scheduling, and manual testing. Design is codified into a common language of Tests: Specifications require less translation and have faster validation Deliverables are more accurate to the business needs and progress is more accurate Future maintenance and extendibility is built into the process Acceptance Tests do require more up front discussions (not coding!) and setup time for automation. Good tests must match production environments and requirements in order to be valid. Tests must be engineered well in order to give good results as they are a product too. TDD does not guarantee good designs, good thinking, or good code. It's up to people to make it work. (Unit) Test Driven Development Simple, easy, lean, fast, readable, and early. Testing should be easy so start small, ask questions, and get used to the world being upside down =) Tests must factor out Dependencies (helps design modularization and isolation) Tests must run fast (slow tests can be moved into Acceptance Testing) Consider writing tests in order of expected probability of occurrence (balanced by severity if coverage is missed) i.e. the most common usage is correct inputs generating correct output, next maybe 2% of invalid inputs will generate 85% of data corruption -\"Happy Path\" verifies that the code will fulfill the functionality specifically asked for so these are the \"high value\" tests. Next verify most likely invalid inputs and important exception handling The law of diminishing returns: more tests means more code writing and more tests that require maintenance Tests uncover assumptions, dependencies, tight coupling, and duplication. Every Developer must be able to read and maintain the tests; it is a shared effort at making a better product so clarity and readability are critical. TDD Workflow Write Acceptance Test -> Run Acceptance Test -> Passes = Next TDD feature please =) / &#94; AT Fails / | UT Passes / | Write Unit Test -> Run Unit Tests \\ &#94; UT Fails \\ | \\ | Write Code to make it pass Trivial Example using an \"Adding Positive Integers Only Calculator\" add( int a , int b) - testPositiveIntegerPlusPositiveInteger (\"happy path\") - testIntegerMinusPositiveInteger (invalid input) - testZeroPlusPositiveInteger (invalid input) - testStringPlusInteger (edge case invalid input: maybe the compiler does not catch this) \"Test Doubles\" using Stubs or Mocks (Factoring out dependencies) Using stubs or mocks we can focus testing only the code we've written, Stubs generally are simple hard coded ways to validate state. Source code examples of TDD add(int a, int b) { dependencyLibrary.adder(a, b) } Class dependencyLibraryStub extends dependencyLibrary { dependencyLibraryStub(expectedResult) Override adder(int a, int b) { return expectedResult } } Mocks are usually leveraged with a framework and can validate process, multiple interactions, as well as state. QA Domains Start Testing with the most used paths and most user visible areas. Find the most serious consequence -> force error handling or crash. Make it easy to test (invest in GUI / scripting, don't slow testing down by requiring careful command line typing) Automate testing (especially regression testing). :::text functionality communication / documentation command structure performance / load output (including error messages) compatible softare / hardware stack overflow garbage collection errors - boundary -math / time -startup -long running -pause & restart / resume -backup & restore -different data back & forth -race conditions -denial of resources Dependencies RAM full, Hard Drive full scenarios, cpu full, network slow locked files (e.g. OS is using) Error accessing media (slow disk, bad network, etc.) Other Shared Resource locks? Special Modes (i.e. airplane/offline mode, disconnected peripheral, etc. Remove/Rename files and folders the app depends on Corrupt one of the above hidden file? permissions changed... File Systems, Language, and Text Ascii vs UTF8 File size 0, negative? and very large Many small files, many large files Directory or Folder instead of File and vice versa Symbolic links and shortcuts Invalid paths (OS do the checks!) Max path length longest file name (symbols) reserved file names? try pasting (rather than typing) special control chars \\n <a href -1234567890-1234567890--1234567890--1234567890 Escape sequences :::text ‘\\'. ‘\\' = %5C = %255C = %%35%63 = %25%35%63 ) HTML Encoding check where applicable: ‘<' = < = &#x3C = &60 ! # $ byte boundaries 0 to 65,535 -2,147,483,648 to 2,147,483,647 Standard Test Cases Viewing Resolution 1024 x 768 but also try 800x600 and 1280 and strange ones Resizing? Excessive Requests performance (too slow might be unusable) Single item too long horizontal Single item too long vertical Too many items returned horizontal Too many items returned vertical Dropdown scrollbar = logical / physical batches of results? Web Page chrome, firefox(2,3), ie (6,7,8,9), safari (i.e. without SSO), opera? able to bookmark User Input Text Entry too few (blank) and too many chars funny chars (UTF8? and symbols !@#$%&&#94;*()[]\\|;':\",./<>? Special case = Email Address or Phone Number? Excessive Submits Login wrong username wrong password already logged in username does not exist? user does not have permission? back button Search (similar to above but also includes) case sensitive? (user notified?) wild card no data set exists to search (e.g. user search but no users?) Upload a file UTF8 file names, very long names, very short names, special characters (. / ? < >) case sensitivity, reserved key names upload normal expected extensions (jpg, gif, png etc.) try random or restricted extensions (.exe, .bat, no extension, super long chars, special chars) tiny file or really large file 0 byte file compressed (zip, 7z, rar, etc.) interrupt the upload (does it fail cleanly? resume?) race condition of two different sessions uploading the same file name (different content) Whitebox \"Glassbox\" Looking in the code InvalidSession Exception Offline Exception, No Permission Exception Could not retrieve from source Exception Unexpected Exception array boundary memory leaks / free race conditions / locked global variables (creates a hidden race condition or lock) Error Messages Do they appear when there's a problem? Are they accurate? Can they be understood? Do they direct the user on how to the correct the problem? What to log or print in an error Title = source of the error You cannot do \"action user tried\". Reason why Source is throwing error. \"MyComponent Driver Error\" You cannot reply to \"user@domain.com\" You do not have write permission on that file. For your consideration Stress \"thundering herd\" and exponential backoff cpu processing, data sizes, input volumes, frequency what breaks when it fails can it be disabled Time concurrency stale/null/indeterminate state archived/deleted Operations upgrades, migrations, backups/restore UX: new users and new features power users vs newbs vs admins, international, 3rd party devs, support logging, metrics, and analytics Platform variability in OS, browser, screens, devices, databases, connections external dependency handling? licensing and tier thresholds (i.e. group storage limit) SaaS vs cluster vs on premise considerations Usability consistency empty state, keyboard vs mouse vs ? customization? (saved preferences) what's the learning curve? too little/too much documentation? list UI: sorting, ordering, Security Access and Audit logs who should be able to access via Web UI, API, etc. XSS, XSRF where's the edge? tiered service layers (clearly defined boundaries for authorization) leaking information (what can anonymous or normal users see they should not) logging password/sensitive info","tags":"programming","title":"Test Driven Development Introduction and QA Domains"},{"url":"https://blog.john-pfeiffer.com/yes-change-your-password-regularly/","text":"Security is risk management The hundreds of articles about \"changing passwords doesn't improve security\" are just hype/noise without context. There is an absolute spectrum of password management that creates untenable risk (no password or simply the word \"password\") all the way to very low risk (100 random characters of every category changed every day) but at an unbearable overhead. While \"changing the highly complex password every 90 days\" is considered inefficient and draconian... What about at least changing the password once a year? While it's likely that an external attacker or disgruntled employee will use a compromised password immediately, it doesn't mean there isn't a good reason to choose a frequency of password change: If a sticky note attached to a laptop sold at a garage sale is still valid for the company's online bank account then you're in trouble. Reduce the risk, have a policy to manage that window of access to something you're comfortable with. Dependency Visibility The opportunity to improve your infrastructure is well worth the cost of finding every hard coded place that a password is embedded in your organization: The password was just changed You quickly find something is not working You either change the password back or refactor/reorganized and break the dependency Far better than: - a random event like a forgotten password reset by one individual in the organization - creates a mystery problem in mission critical systems for everyone else to track down But I use a password management tool! Password management tools have to store their passwords somewhere (hopefully encrypted). Over time there is a chance that a \"bad actor\" will end up with your local password store. Man in the middle attacks (NSA anyone?) can snatch credentials from secure channels. There is an even higher likelihood that the remote services you are using will be compromised (database or password hashes leak). Given time hackers can use rainbow tables and GPU based brute force attacks they will crack your password. Every time you change your password you reset the clock on every type of attack","tags":"it","title":"Yes, Change your Password regularly"},{"url":"https://blog.john-pfeiffer.com/using-find-grep-cut-awk-sort-dd-with-files-and-text/","text":"There are amazing linux command line utilities that make finding and manpiulating files very easy Assuming you understand the basics like: touch example.txt make an empty file named file.txt echo \"hi\" >> example.txt append the hi content into example.txt, note that a single > will overwrite the contents cat example.txt | tee -a example2.txt display the contents of the file to the output console and also pipe the result to the tee utility which appends it to another file - tee is better than >> :> example.txt truncate the file without disturbing any existing readers of that file (e.g. zero a log without messing up an applications ability to write to the file) cp -a example.txt example2.txt copy archive which preserves timestamp (but will also overwrite the target - in this example probably zero bytes) find find -name \"MyCProgram.c\" case sensitive, starts in the current directory find startdirectory -name 'partoffileordirname' e.g. find /home/joe -name '.tx' which would return txt's as well as txv?'s find / -iname \"MyCProgram.c\" case insensitive, starts from root find -maxdepth 1 -not -iname \"MyCProgram.c\" case insensitive, starts from current directory, will search subdirectory(ies) and list all items //that do NOT match the query find . -type f -exec ls -s {} \\; | sort -n -r | head -5 the largest 5 files find . -not -empty -type f -exec ls -s {} \\; | sort -n | head -5 the smallest not empty 5 files find . -type d all directories in the current directory find . -type f | wc count the number of files (can recurse subdirectories) find . -type f -iname '*.pyc' -exec mv {} /tmp/PYC/ \\; move all .pyc files (start from this directory and indefinitely recurse down) find . -name \"*api*\" -exec cat {} \\; find everything containing api and cat it find and exec to modify a set of files find . -type f -name \"*api*\" -exec cat {} \\; | grep objectid find all files that contain an api and output the contents but filter to only display lines that contain \"objectid\" find . -maxdepth 1 -type d -exec du -sh {} \\; only one level down if it's a directory show the disk usage summary (human sizes) find DIR1 DIR2 -maxdepth 1 -type f -exec basename {} \\; | sort | uniq -d lists all file names in directories, sorted, show only repeats (aka duplicates) find . -name '*.txt' -exec sh -c 'mv \"$0\" \" ${ 0 %. txt } .java\"' {} \\; find all .txt files and renames them to .java for f in ; do mv \" $ f \" \" $( echo $ f | sed 's/-/\\ /g' ) \"; done find . -type f -iname \".py\" | rename s/.py/.py.txt/ .py { } \\; only works on the current directory (no recursion?) find . -type f -iname '.py' | while read filename; do mv -v \" ${ filename } \" \"echo \" ${ filename } \" | sed -e 's/\\.py$/\\.py.txt/'\"; done a lot of extra work to achieve a recursive rename from .py to .py.txt find . -type f -iname \".java\" -exec grep -Hn \"fileSizeInMB < 100\" {} \\ ; find . -type f -iname \".java\" -exec grep -Hni \"case-insensitive-text\" {} \\ ; find . -type d -name directoryname * -exec ls -ahl {} \\ ; sudo find / var / www / java -type f -iname \".txt\" -exec chown root :www-data {} \\ ; sudo find / var / www / java -type f -iname \".txt\" -exec chmod 640 {} \\ ; sudo find / var / www / d -type d -iname \"web*\" -exec chmod 750 {} \\ ; find . -type f -iname \"*.sh\" -exec mv {} . \";\" find files ending in .sh and move them into the current directory find /dir/dir -type f -mtime +540 -mtime -720 -printf \\%p\\,\\%s\\,\\%AD\\,|%TD\\\\n > /dir/dir/output.csv find ~ -empty //check the home directory for empty files (size 0) find / -mindepth 3 -maxdepth 5 -iname passwd case insensitive, starts from root, will search subdirectory levels between 2 and 4 find / 3 -maxdepth 5 -iname passwd & case insensitive, starts from root, will search at most 4 subdir levels, will start in background note that you'll have to press enter once as the text results will scroll to interrupt your bash session ... once the job's done pressing enter will return you to the prompt find -iname \"MyCProgram.c\" -exec md5sum {} \\; interesting use: creating a md5sum of all of the results find -inum 16187430 -exec mv {} new-test-file-name \\; interesting - find a file by inode number (ls -i) and then rename/move it find / -perm 700 -type f find all files from root below, with permissions set exactly to 700, only regular files (-type f) find / -perm 700 -type f -exec ls -l {} \\; while the above just lists the files the below runs an ls -l to see everything about them... RUN \"man find\" IF YOU NEED TO FIND SOMETHING SPECIFIC ABOUT FILES AND PERMISSIONS any files newer than the one given find -newer file-i-made-yesterday search the home directory size equal to 100 MB, use +100MB for greater than and -100MB for less than find ~ -size 100M http://www.thegeekstuff.com/2009/03/15-practical-linux-find-command-examples/ find files by modified time There is an implied AND operator with find but for OR or NOT... find / -mmin -10 something modified 10 minutes ago find . -mtime 1 find files modified between 24 and 48 hours ago find . -mtime +1 find files modified more than 48 hours ago find . -mmin +5 -mmin -10 find files modifed between # 6 and 9 minutes ago find / -type f -mtime -7 | xargs tar -rf weekly_incremental.tar find files modified in the last 7 days and create a .tar file from them find / -name core -delete same if using Gnu find find / -user username find all of the files a user owns.. -mtime +60 means you are looking for a file modified 60 days ago. -mtime -60 means less than 60 days. -mtime 60 If you skip + or - it means exactly 60 days. find / -mtime 9 -mtime -10 24 hours grep grep is an amazing tool for getting efficiently finding text, http://www.gnu.org/software/grep/manual/grep.html grep parameters and examples explained cat access.log | grep -v \"bingbot\" exclude from output lines that match bingbot grep -r -i -w -n -A2 -B1 'hidden' /tmp search the /tmp directory and subdirectories recursively case insensitive only match the whole word, so \"thidden\" would not be returned as a match print the line number in the file where it was found print the two lines after the grep match print the one line before the grep match start the search in the /tmp directory command notes grep -c 'hidden' ./myfile only display the number of matches in the file grep -r -l 'hidden' /tmp recursively search /tmp and only display the file names which contain \"hidden\" grep \"hidden treasure\" /home/ubuntu/*.txt search only txt files grep ab.d file find a single character wildcard grep \"ab.*e\" file find a infinite repitions of a single character, word ends in e grep \"ab.*e.\" file find a infinite repitions of a single character, word ends with a single character grep \"ab[c-e]f\" file find with a wildcard of a subset of range of characters Useful parameters for grep -v = invert the match so do NOT show lines that match (typically | grep -v 'myexclude') -x = whole line match only -C 2 = print two lines before and two lines after a match grep ubuntu /etc/passwd | cut -d: -f3 only print the user id by piping the match to cut which delimits by colon and outputs the 3rd column ls -t -d -1 -r path/directory/ >> oldest.m3u list reverse order by timestamp ls -t -d -1 path/directory/ | grep -v DONOTLIKE >> newest.m3u list by timestamp (sort by modification time, newest first), list directories themselves, not their contents, only 1 level deep pipe to grep and ignore matches of DONOTLIKE, then append output to the newest.m3u file grep files without match grep -L 'foobar' * --files-without-match , display filenames that do not contain the string foobar cut ps auxwwww | grep someappname | tr -s [:space:] | cut -d\\ -f11- filter to only view someappname from all processes (wide) THEN shorten all whitespace to a single space THEN cut delimited by a single space (escaped by the slash) and then only prints all after the 11th field/column cat sometext.txt | cut -f1 -d\"[\" delimiter of square bracket , only take after the first \"field\" token, so essentially print everything after the first occurence of a left square bracket http://ss64.com/bash/cut.html cut to only display a part of a path #!/bin/bash # iterate through the list of subdirectories # cut out each subdirectory name (using forward slash delimiter) # compare a text file within to a similarly organized TEMP directory DIRECTORY = /var/lib/tomcat6/webapps/* for dir in $DIRECTORY do if [ -d $dir ] ; then echo $dir NAME = ` echo $dir | cut -d \"/\" -f 6 ` diff $dir /WEB-INF/app.properties /var/lib/tomcat6/TEMP/ $NAME /WEB-INF/app.properties fi done awk awk to parse columns of data , some overlap with cut awk -F\",\" '{ print $2 }' results.txt csv parsing , set the delimiter to a comma and print the second column awk ' { $ 1 = \"\" ; print $ 0 } ' results.txt assuming space delimited and remove the first column but print all else in results.txt ps aux | grep someappname | awk ' { $ 1 = $ 2 = $ 3 = $ 4 = $ 5 = $ 6 = $ 7 = $ 8 = $ 9 = $ 10 = \"\" ; print $ 0 } ' print everything after the nth (10th) column cat sometext.txt | cut -f1 -d\"(\" using cut can be more effective: deleting everything after the first occurence of a left parenthesis grep -r 'beta/dists/precise/main/binary-amd64' | grep -v 1.2.3.4 | grep -v AccessDenied | awk '{print $5}' | sort -u grep --exclude-dir=.git -r 'foo' . # recursively search this directory for foo but ignore the .git directory recursive search for a string, pipe the output to exclude lines that contain IP 1.2.3.4 , pipe to exclude AccessDenied, print the 5th column, sort for uniqueness If the 5th column of results.txt contains numbers then ... cat results.txt | awk '{total = total + $5} END{print total}' ls -tahl | awk '{print $5,$6,$7,$8}' awk <search pattern> {<program actions>} 1.5K 2009-07-14 12:14 backupCHECKUP.sh 5.2K 2009-07-14 12:03 email-backup.txt 4.0K 2009-07-14 10:06 . 330 2009-07-14 09:55 test.sh 253 2009-07-14 08:54 daily-backup-projects.sh 4.0K 2009-07-12 11:09 .. 3.9K 2009-03-18 16:01 mtrac.ini 5.0K 2008-11-25 11:50 CreateProject.sh awk '/2009/ {print $5,$6,$7,$8}' ls_output.txt Note that Awk recognizes the field variable $0 as representing the entire line, so this could also be written as: awk '/gold/ {print $0}' sed sed does string substitution sed regular expression 's=start/olditem/newitem/g=end' filename sed -e 's/ /\\t /g' email-backup.txt replace a space with a tab sed -i 's/\\x85/.../g' *.md replace a UTF-8 character (in this case the single character horizontal ellipsis) with three dots REMEMBER \\t = tab \\n = newline \\r = carriage return sed -e 's/$/\\r/' inputfile > outputfile # UNIX to DOS (adding CRs) sed -e 's/\\r$//' inputfile > outputfile # DOS to UNIX (removing CRs) perl -pe 's/\\r\\n|\\n|\\r/\\r\\n/g' inputfile > outputfile # Convert to DOS perl -pe 's/\\r\\n|\\n|\\r/\\n/g' inputfile > outputfile # Convert to UNIX perl -pe 's/\\r\\n|\\n|\\r/\\r/g' inputfile > outputfile # Convert to old Mac dd dd can delete things very quickly (dangerous!) But a useful tool for testing upload limits or compression or any other miscellaneous file tasks is to generate a file of a specified length: dd if=/dev/zero of=a.log bs=1M count=2 zero filled 2MB file dd if=/dev/urandom of=random.txt bs=1M count=2 random contents 2MB file hexdump random.txt | head Notes about randomness (on linux): /dev/urandom semi-random data generated by a PRNG which is fed by the trickle of real entropy from /dev/random (which blocks until the entropy pool has some randomness) watch -n 0 'cat /proc/sys/kernel/random/entropy_avail' cat /dev/random > /dev/null Drain the entropy from your system cpuid | grep -i rand Look for RDRAND http://en.wikipedia.org/wiki/RdRand cat /dev/urandom | rngtest -c 1000` how good is your non blocking urandom? more info https://en.wikipedia.org/wiki/Grep http://www.gnu.org/software/grep/manual/grep.html http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_04_02.html http://linux.die.net/man/1/cut","tags":"linux","title":"Using find grep cut awk sort dd with files and text"},{"url":"https://blog.john-pfeiffer.com/a-concise-summary-of-amazing-and-great-ideas/","text":"Test Driven Development (TDD) Test, Code, Refactor http://en.wikipedia.org/wiki/Test-driven_development Lean Lean: Identify Value -> Breakdown Steps -> Continuous Flow -> Reduce Waste Lean Startup: Measure, Learn, Build http://theleanstartup.com/principles Better than Free http://www.kk.org/thetechnium/archives/2008/01/better_than_fre.php immediacy personalization interpretation/support authenticity accessibility embodiment patronage findability Persuasion http://en.wikipedia.org/wiki/Robert_Cialdini reciprocity commitment/consistency social proof authority liking scarcity Competition and Profit http://en.wikipedia.org/wiki/Porter_five_forces_analysis Threat of new competition (barriers to entry, customer loyalty, desirability of that industry/biz model) Threat of substitute products or services (switching costs, quality, compatibility) Bargaining power of customers (switching costs, market options, dependency for other services) Bargaining power of suppliers (switching costs, supplier choice, supplier becoming competitor) Intensity of competitive rivalry (innovation, branding, economies of scale) Schelling's segregation model micromotives and macrobehavior , http://en.wikipedia.org/wiki/Thomas_Schelling#Models_of_segregation Granovetter threshold model for peer effect on collective behavior and Strength of Weak Ties (aka how LinkedIn gets you a new job) http://en.wikipedia.org/wiki/Mark_Granovetter Cognitive Biases Bandwagon Effect (Groupthink) Confirmation Bias: search for and interpret information and memories that support preconceptions Gambler's Fallacy: future probabilities are affected by previous outcomes Negativity Bias: paying more attention to bad news Neglect of Probability: disregarding probabilities when making a decision (risk of flying versus driving) Observational Selection Bias (Frequency Illusion): noting something previously ignored results in a misconception that it has increased in frequency Projection Bias: wrongly presuming others think like us Status Quo Bias: things should stay the same Logical Fallacies and Disinformation Appeal to probability: because something could happen it is inevitable that it will happen (see Gambler's Fallacy and Neglect of Probability) Silence, Indignant, Rumors, Straw Man, Ad Hominem, Hit and Run, Question Motives, Invoke Authority, Play Dumb, \"That's old news\", Confess to a lesser item and \"come clean\", Enigma, Rube Goldberg Logic, Demand a complete solution, Fit the facts to alternate conclusions, Remove witnesses/evidence, Change the subject, Antagonize, Ignore proof and demand impossible proof, False evidence/facts, Loudly call for a separate investigation (ideally either biased or with confidential findings), Manufacture a new truth, Larger distractions, Silence critics, Lie low http://www.nowandfutures.com/spew_tools.html Keys to being successful lists organize (categorize) prioritize schedule brainstorm goals & feelings: \"say what you're going to do and then do what you say\" Concentrate, Iterate, Automate, Validate, Appreciate Software version: core tech strengths & problem, quick releases, automate, test!, style? + recognize the contributions Military version: core strength and enemy weaknesses, rapid short executions, make excellence a reflex, check for brittleness, engender loyalty 7 habits of highly effective people be proactive \"begin with the end in mind\" (envision the goal) \"put first things first\" (order and prioritize) \"think win-win\" (good outcomes for everyone) \"Seek First to Understand, Then to be Understood\" (listen, then persuade) \"synergize\" (teamwork) \"sharpen the saw\" (sustainable balance) http://en.wikipedia.org/wiki/The_7_Habits_of_Highly_Effective_People Important Software Concepts Do Not Repeat Yourself (DRY) Model View Controller (MVC) Atomic Consistent Isolation Durability (ACID) Abstraction Polymorphism (overloading, inheritance, overriding interface) , Inheritance , Encapsulation Consistency Availability Partition tolerance vs Basically Available Soft-State with Eventual consistency Nondeterministic Polynomial ... NP-hard http://en.wikipedia.org/wiki/NP-hard NP-complete (subset sum problem can be verified) http://en.wikipedia.org/wiki/NP-complete co-NP (verifier of \"no\" answer\") http://en.wikipedia.org/wiki/Timeline_of_algorithms Amdahl's Law A system cannot be sped up by parallelization more than the inherently serial steps http://en.wikipedia.org/wiki/Amdahl%27s_law So benchmark your system, then determine what parts can be parallelized and how much that will improve the result and how much will it cost to do so Conway's Law The system design produced by an organization will reflect the organization's communication structure. http://www.melconway.com/Home/Conways_Law.html http://www.thoughtworks.com/insights/blog/demystifying-conways-law Possibly disastrous results when combined with Groupthink http://en.wikipedia.org/wiki/Groupthink Commonly referred to when considering how adding a new person or new team to organization will affect productivity Brooks' Law Adding resources (people) later in a project will make it even later http://en.wikipedia.org/wiki/Brooks%27s_law A decent observation given the above \"laws\": if a task has serial parts adding people (parallelization) will not speed it up AND every person will have to interface Moore's Law Computing power will double (or become cheaper by half) every two years http://en.wikipedia.org/wiki/Moore%27s_law Sustained in part by improvements in complimentary technologies like Memory, Storage, Cooling, etc. At a certain point in the future potentially only possible using parallel computing but with an increased coordination cost (including software that leverages parellization) Postel's Law Be conservative in what you do, be liberal in what you accept from others https://en.m.wikipedia.org/wiki/Robustness_principle Laws of Unix http://en.wikipedia.org/wiki/Unix_philosophy#Eric_Raymond.E2.80.99s_17_Unix_Rules Modularity: Write simple parts connected by clean interfaces. Clarity: Clarity is better than cleverness. Composition: Design programs to be connected with other programs. Separation: Separate policy from mechanism; separate interfaces from engines. Simplicity: Design for simplicity; add complexity only where you must. Parsimony: Write a big program only when it is clear by demonstration that nothing else will do. Transparency: Design for visibility to make inspection and debugging easier. Robustness: Robustness is the child of transparency and simplicity. Representation: Fold knowledge into data, so program logic can be stupid and robust. Least Surprise: In interface design, always do the least surprising thing. Silence: When a program has nothing surprising to say, it should say nothing. Repair: Repair what you can, but when you must fail, fail noisily and as soon as possible. Economy: Programmer time is expensive; conserve it in preference to machine time. Generation: Avoid hand-hacking; write programs to write programs when you can. Optimization: Prototype before polishing. Get it working before you optimize it. Diversity: Distrust all claims for one true way. Extensibility: Design for the future, because it will be here sooner than you think. (Or, to put it another way, your creations will last longer than you think!) Clean Code Source Code is for humans, make it easy to read and understand The code is the authoritative source (comments add context) Leave the campground cleaner than you found it Tests reveal what the code outputs; clean code runs all of the tests Meaningful Names Functions: A minimum number of parameters and the smaller the better Open - Close principle Single Responsibility (do one thing, and do it well) No Duplication (DRY) Objects allow modularity, Boundaries keep you sane Separate Constructing a System from Using it (and Initialization from Runtime) Fallacies of Distributed Computing https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing The network is reliable. Latency is zero. Bandwidth is infinite. The network is secure. Topology doesn't change. There is one administrator. Transport cost is zero. The network is homogeneous. How Complex Systems Fail http://web.mit.edu/2.75/resources/random/How%20Complex%20Systems%20Fail.pdf Complex systems are intrinsically hazardous systems Complex systems are heavily and successfully defended against failure Catastrophe requires multiple failures – single point failures are not enough Complex systems contain changing mixtures of failures latent within them Complex systems run in degraded mode Catastrophe is always just around the corner Post-accident attribution accident to a ‘root cause' is fundamentally wrong Hindsight biases post-accident assessments of human performance Human operators have dual roles: as producers & as defenders against failure All practitioner actions are gambles Actions at the sharp end resolve all ambiguity Human practitioners are the adaptable element of complex systems Human expertise in complex systems is constantly changing Change introduces new forms of failure Views of ‘cause' limit the effectiveness of defenses against future events Safety is a characteristic of systems and not of their components People continuously create safety Failure free operations require experience with failure Designers/Creators of Programming Languages Language Creator/Designer Year more info Fortran John Backus 1957 https://en.wikipedia.org/wiki/Fortran Lisp John McCarthy 1958 https://en.wikipedia.org/wiki/Lisp_(programming_language) C Dennis Ritchie 1972 https://en.wikipedia.org/wiki/C_(programming_language) C++ Bjarne Stroustrup 1983 https://en.wikipedia.org/wiki/C%2B%2B Perl Larry Wall 1987 https://en.wikipedia.org/wiki/Perl Python Guido van Roosum 1991 https://en.wikipedia.org/wiki/Python_(programming_language) Java James Gosling 1995 https://en.wikipedia.org/wiki/Java_(programming_language) PHP Rasmus Lerdorf 1995 https://en.wikipedia.org/wiki/PHP Javascript Brendan Eich 1995 https://en.wikipedia.org/wiki/JavaScript Ruby Yukihiro Matsumoto 1995 https://en.wikipedia.org/wiki/Ruby_(programming_language) Go Robert Griesemer, Rob Pike, Ken Thompson 2009 https://en.wikipedia.org/wiki/Go_(programming_language) A quick history of software (in ascii) hardcoded hardware (ENIAC) -> von neumann architecture (stored programs) -> mainframes with custom punch cards (assembly) -> procedural code (fortran, c) -> object oriented (simula, java) -> parallel programming -> Artificial Intelligence that writes self adapting Domain Specific Langauges for everything? Start by reading all of the following to nitpick how the above is fast and loose with history and the truth... http://en.wikipedia.org/wiki/Computer http://en.wikipedia.org/wiki/ENIAC http://en.wikipedia.org/wiki/Von_Neumann_architecture http://en.wikipedia.org/wiki/Programming_paradigm http://en.wikipedia.org/wiki/Object-oriented_programming#History http://en.wikipedia.org/wiki/Parallel_computing#Software http://en.wikipedia.org/wiki/Concurrent_computing Hacker's Jargon http://www.catb.org/jargon/oldversions/","tags":"puzzles","title":"A concise summary of amazing and great ideas"},{"url":"https://blog.john-pfeiffer.com/time-for-programmers/","text":"Computer (Unix / POSIX) time starts 1970-01-01 00:00:00 UTC http://en.wikipedia.org/wiki/Unix_time An excellent article about time, especially for java programmers, http://www.odi.ch/prog/design/datetime.php Inside the \"river of time\" measurement is absurd, but Physicists have spacetime, \"...cycles of radiation corresponding to the transition between the two electron spin energy levels of the ground state of the 133 Caesium atom\". 24 hours, UTC and NTP can synchronize the world (especially servers!), but days, calendars, time zones, weeks, etc. will drive you crazy, so think carefully and use the utility libraries! Java datetime timezone example import java.util.Date ; import java.util.Calendar ; import java.util.TimeZone ; import java.text.DateFormat ; import java.text.SimpleDateFormat ; // not thread-safe public static SimpleDateFormat dfm = new SimpleDateFormat ( \"yyyy-MM-dd\" ); DateFormat dfm = new SimpleDateFormat ( \"yyyy-MM-dd HH:mm:ss\" ); dfm . setTimeZone ( TimeZone . getTimeZone ( \"Europe/Zurich\" )); Date a = dfm . parse ( \"2007-02-26 20:15:00\" ); Yesterday in python from datetime import date yesterday = date . fromordinal ( date . today () . toordinal () - 1 ) . strftime ( '%Y-%m- %d ' ) dateutil and helper functions import dateutil.parser # http://labix.org/python-dateutil (for google app engine put the source directory at the root project level) myd = 'Thu, 11 Jul 2013 05:01:21 -0700' datetime_obj = dateutil . parser . parse ( myd ) def seconds_to_datetime ( t ): return datetime . datetime . fromtimestamp ( int ( t ) ) def datetime_string_to_seconds ( date_str ): datetime_obj = dateutil . parser . parse ( date_str ) # Thu, 11 Jul 2013 05:01:21 -0700 return Utility . datetime_to_seconds ( datetime_obj ) def datetime_to_seconds ( datetime_obj ): return int ( time . mktime ( datetime_obj . timetuple () ) ) pytz for timezones import pytz # sometimes requires complex installation, easy_install --upgrade pytz from datetime import datetime print datetime . datetime . now () utc = pytz . timezone ( \"UTC\" ) print utc date_utc = datetime . datetime . now ( pytz . timezone ( \"UTC\" ) ) . strftime ( \"%Y-%m- %d \" ) print date_utc time and datetime tuples import time mytime = time . strptime ( \"Mon Apr 07 13:05:55 PDT 2014\" , \"%a %b %d %H:%M:%S %Z %Y\" ) # time.struct_time(tm_year=2014, tm_mon=4, tm_mday=7, tm_hour=13, tm_min=5, tm_sec=55, tm_wday=0, tm_yday=97, tm_isdst=1) time . mktime ( mytime ) # 1396901155.0 print time . strftime ( \"%Y-%m- %d %H:%M:%S\" , mytime ) # 2014-04-07 13:05:55 time_tuple = ( 2008 , 11 , 12 , 13 , 51 , 18 , 2 , 317 , 0 ) print time . strftime ( \"%Y-%m- %d %H:%M:%S\" , time_tuple ) # 2008-11-10 17:53:59 import datetime date_object = datetime . datetime ( 2008 , 11 , 10 , 17 , 53 , 59 ) print date_object . strftime ( \"%Y-%m- %d %H:%M:%S\" ) # 2008-11-10 17:53:59 timestamp = 1226527167.595983 print repr ( datetime . fromtimestamp ( timestamp ) ) # repr prints with limits on sizes of objects import calendar time_tuple_utc = ( 2008 , 11 , 12 , 13 , 59 , 27 , 2 , 317 , 0 ) # time tuple in utc time to timestamp timestamp_utc = calendar . timegm ( time_tuple_utc ) print repr ( timestamp_utc ) #------------------------------------------------- time_tuple = ( 2008 , 11 , 12 , 13 , 51 , 18 , 2 , 317 , 0 ) datetime_object = datetime ( * time_tuple [ 0 : 6 ]) print repr ( datetime_object ) date_string = \"2008-11-10 17:53:59\" datetime_object = datetime . strptime ( date_string , \"%Y-%m- %d %H:%M:%S\" ) print repr ( datetime_object ) timestamp = 1226527167.595983 datetime_object = datetime . fromtimestamp ( timestamp ) # local time print repr ( datetime_object ) timestamp = 1226527167.595983 datetime_object = datetime . utcfromtimestamp ( timestamp ) print repr ( datetime_object ) #------------------------------------------------- # conversions to time tuples datetime_object = datetime ( 2008 , 11 , 10 , 17 , 53 , 59 ) time_tuple = datetime_object . timetuple () print repr ( time_tuple ) date_str = \"2008-11-10 17:53:59\" time_tuple = time . strptime ( date_str , \"%Y-%m- %d %H:%M:%S\" ) print repr ( time_tuple ) timestamp = 1226527167.595983 local_time_tuple = time . localtime ( timestamp ) # local time print repr ( local_time_tuple ) utc_time_tuple = time . gmtime ( timestamp ) # UTC print repr ( utc_time_tuple ) #------------------------------------------------- # conversions to timestamps # time tuple in local time to timestamp time_tuple = ( 2008 , 11 , 12 , 13 , 59 , 27 , 2 , 317 , 0 ) timestamp = time . mktime ( time_tuple ) print repr ( timestamp ) # time tuple in utc time to timestamp time_tuple_utc = ( 2008 , 11 , 12 , 13 , 59 , 27 , 2 , 317 , 0 ) timestamp_utc = calendar . timegm ( time_tuple_utc ) print repr ( timestamp_utc ) #------------------------------------------------- # results #------------------------------------------------- # 2008-11-10 17:53:59 # 2008-11-12 13:51:18 # datetime.datetime(2008, 11, 12, 13, 51, 18) # datetime.datetime(2008, 11, 10, 17, 53, 59) # datetime.datetime(2008, 11, 12, 13, 59, 27, 595983) # datetime.datetime(2008, 11, 12, 21, 59, 27, 595983) # (2008, 11, 10, 17, 53, 59, 0, 315, -1) # (2008, 11, 10, 17, 53, 59, 0, 315, -1) # (2008, 11, 12, 21, 59, 27, 2, 317, 0) # (2008, 11, 12, 13, 59, 27, 2, 317, 0) # 1226527167.0 # 1226498367","tags":"programming","title":"Time for Programmers"},{"url":"https://blog.john-pfeiffer.com/amazon-ses-on-ec2-free-tier-to-search-for-a-kitteh/","text":"Mission: hourly poll of a website ...to find out if the Kitteh is available for adoption and immediate email notification if Kitteh is found. Estimated time to complete: between 15 minutes and hours (depending on setting up your EC2 instance, SES service, etc.) Skills: Amazon EC2 setup, SSH, centos yum, bash, wget, cronjob Amazon Free services tier If you have an Amazon EC2 instance running (e.g. EC2 Linux Micro Instance in Free Tier = centos!) (And you're not running over the GET/POST upload/download free tier bandwidths) (If you don't know how to setup a quick Amazon Linux Micro Instance in the free tier search this blog for more info) Sign up for SES (then receive a verification email for your Amazon AWS Account) Account Security Credentials (for AWS access identifiers) Use nano or vi to create a file \"aws-credentials\" (Amazon's sample below) AWSAccessKeyId=022QF06E7MXBSH9DHM AWSSecretKey=kWcrlUX5JEDGM/LtmEENI/aVmYvHNif5zB+d9+ Download the example perl scripts via: http://aws.amazon.com/code/Amazon-SES wget http://aws-catalog-download-files.s3.amazonaws.com/AmazonSES-2011-02-02.zip unzip AmazonSES-2011-02-02.zip chmod +x /home/ec2-user/*.pl /home/ec2-user/bin/ses-verify-email-address.pl -k aws-credentials -v youreemail@example.com Amazon EC2 Missing some perl \"Can't locate XML/LibXML.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/local/share/perl5 /usr/lib64/perl5 /usr/share/perl5 /usr/share/perl5 /usr/lib64/perl5 /usr/share/perl5 /usr/local/lib64/perl5/site_perl/5.10.0/x86_64-linux-thread-multi /usr/local/lib/perl5/site_perl/5.10.0 /usr/lib64/perl5/vendor_perl/5.10.0/x86_64-linux-thread-multi /usr/lib/perl5/vendor_perl /usr/lib/perl5/site_perl .) at ./ses-verify-email-address.pl line 26. BEGIN failed--compilation aborted at ./ses-verify-email-address.pl line 26.\" THANKS AMAZON! Using their preconfigured Instance means they don't have all of the Perl packages installed... sudo yum install perl-XML-LibXML perl-digest-SHA sudo yum provides */SHA.pm tells me what other packages I might have missed... sudo yum search perl-Digest sudo yum install perl-Digest-SHA What a difference a D versus d makes! sudo yum perl-libxml-perl libxml2-devel perl-IO-Socket-SSL libxslt-devel ? Debian: sudo apt-get install libio-socket-ssl-perl libxml-libxml-perl tail /var/log/maillog for troubleshooting sendmail... Verify an SES linked email address by running the perl script /home/ec2-user/bin/ses-verify-email-address.pl -k aws-credentials -v youreemail@example.com Use the email account you gave above for verification and click on the link... You have successfully verified an email address with Amazon Simple Email Service. ~/amazonses/bin/ses-send-email.pl -k ~/amazonses/bin/aws-credentials -s \"Test AWS\" -f youremail@example.com youremail@example.com,secondemail@example.com < ~/kittysearch/result.txt Search.sh #/bin/bash wget -O ~/kittysearch/page1.htm 'http://adopt.hssv.org/search/searchResults.asp?task=search&searchid=&advanced=&s=adoption&animalType=2%2C15&statusID=3&state=&regionID=&submitbtn=Find+Animals' wget -O ~/kittysearch/page2.htm 'http://adopt.hssv.org/search/searchResults.asp?tpage=2&task=search&searchid=&advanced=&s=&animalType=2,15&statusID=3&state=&regionID=&submitbtn=Find+Animals' wget -O ~/kittysearch/page3.htm 'http://adopt.hssv.org/search/searchResults.asp?tpage=3&task=search&searchid=&advanced=&s=&animalType=2,15&statusID=3&state=&regionID=&submitbtn=Find+Animals' grep -i \"bandit\" ~/kittysearch/page1.htm ~/kittysearch/page2.htm ~/kittysearch/page3.htm > ~/kittysearch/result.txt if [ -s ~/kittysearch/result.txt ] ; then # must move to the directory to use the SES.pm cd ~/amazonses/bin ./ses-send-email.pl -k ~/amazonses/bin/aws-credentials -s \"Test AWS\" -f myemail@domain.com myemail@domain.com,secondrecipient@domain.com < ~/kittysearch/result.txt fi # http://docs.amazonwebservices.com/ses/latest/DeveloperGuide/ for full details about email.pl Trigger the search with cron crontab -e i key to enter input in vi 55 * * * * /home/ec2-user/kittysearch/kittysearch.sh escape key gets : then x to save and quit Troubleshooting \"Cannot locate SES.pm\" Running the script from a different directory or CRON gets the error: \"Can't locate SES.pm in @INC\" cp /home/ec2-user/amazonses/bin/SES.pm /home/ec2-user/kittysearch FIXED! must move to the directory in the script using cd first to have access to SES.pm SES Message Limit Yyou can send 2,000 messages for free each day when you call Amazon SES from an Amazon EC2 instance directly or through AWS Elastic Beanstalk. (Note bandwidth charges may still apply) More info Apparently since 2011 there has come along infrastructure like page2rss and ifttt that makes these kind of custom solutions less helpful (unless you need customization!)","tags":"linux","title":"Amazon SES on EC2 free tier to search for a kitteh!"},{"url":"https://blog.john-pfeiffer.com/command-line-dos-networking/","text":"Disk Operating System is still quite useful even in Windows XP/2003/Vista/7 if you know the commands (and parameters). Diagnostic and Networking Commands systeminfo.exe uptime, OS, originall install, ram, domain, logonserver, nic's (note that window98 had a gui, winipcfg from the Run prompt) systeminfo.exe /s computername /u domain\\\\username /p password ipconfig /all ipconfig /renew ipconfig /flushdns ping tracert pathping netstat net view \\\\10.0.0.13 net use x: \\\\10.0.0.13 net use /delete x: net use /delete \\\\10.0.0.13\\share net use * \\\\fileservername\\share net user username newpassword /domain net localgroup /add administrators \"domain users\" # prompts for new password net user username * /domain # Note: If you type these commands on a member server or workstation and # you don't add the /domain switch, the command will be performed on the # local SAM and NOT on the DC SAM. # Note: Non-administrators receive a \"System error 5 has occurred. Access is denied\" # error message when they attempt to change the password. nbtstat -a 127.0.0.1 nbtstat [-a RemoteName] [-A IPAddress] [-c] [-n] [-r] [-R] [-RR] [-s] [-S] [Interval] -a RemoteName : Displays the NetBIOS name table of a remote computer, where RemoteName is the NetBIOS computer name of the remote computer. The NetBIOS name table is the list of NetBIOS names that corresponds to NetBIOS applications running on that computer. -A IPAddress : Displays the NetBIOS name table of a remote computer, specified by the IP address (in dotted decimal notation) of the remote computer. -c : Displays the contents of the NetBIOS name cache, the table of NetBIOS names and their resolved IP addresses. -n : Displays the NetBIOS name table of the local computer. The status of Registered indicates that the name is registered either by broadcast or with a WINS server. -r : Displays NetBIOS name resolution statistics. On a Windows XP computer that is configured to use WINS, this parameter returns the number of names that have been resolved and registered using broadcast and WINS. -R : Purges the contents of the NetBIOS name cache and then reloads the #PRE-tagged entries from the Lmhosts file. -RR : Releases and then refreshes NetBIOS names for the local computer that is registered with WINS servers. -s : Displays NetBIOS client and server sessions, attempting to convert the destination IP address to a name. -S : Displays NetBIOS client and server sessions, listing the remote computers by destination IP address only. Interval : Redisplays selected statistics, pausing the number of seconds specified in Interval between each display. Press CTRL+C to stop redisplaying statistics. If this parameter is omitted, nbtstat prints the current configuration information only once. http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/nbtstat.mspx Windows Logon Ids 2 interactive 3 network 4 batch 5 service 7 unlock 8 network cleartext 9 RunAs 10 RemoteInteractive 11 CachedInteractive Monitor Logon Script Create your logon script and place it in the %SystemRoot%\\System32 folder. Run Regedt32.exe and go to the following value: HKEY_LOCAL_MACHINE\\Software\\Microsoft\\WindowsNT\\CurrentVersion \\Winlogon\\Appsetup After the last entry in the Appsetup value, place a comma and a space and then enter the name and extension of the logon script you placed in the %SystemRoot%\\System32 folder. For example, if the value of Appsetup is: Usrlogon.cmd, Rmvlinks.exe After adding an entry for Termlogon.cmd, the value would look like: Usrlogon.cmd, Rmvlinks.exe, Termlogon.cmd echo %computername% %username% %date% %time% >> %homedrive%\\%homepath%\\log\\log.txt Net Use net use /help NET USE [devicename | *] [\\\\computername\\sharename[\\volume] [password | *]] [/USER:[domainname\\]username] [/USER:[dotted domain name\\]username] [/USER:[username@dotted domain name] [/SMARTCARD] [/SAVECRED] [[/DELETE] | [/PERSISTENT:{YES | NO}]] NET USE {devicename | *} [password | *] /HOME NET USE [/PERSISTENT:{YES | NO}] NET USE connects a computer to a shared resource or disconnects a computer from a shared resource. When used without options, it lists the computer's connections. [devicename] Assigns a name to connect to the resource or specifies the device to be disconnected. There are two kinds of devicenames: disk drives (D: through Z:) and printers (LPT1: through LPT3:). Type an asterisk instead of a specific devicename to assign the next available devicename. \\\\computername Is the name of the computer controlling the shared resource. If the computername contains blank characters, enclose the double backslash (\\\\) and the computername in quotation marks (\" \"). The computername may be from 1 to 15 characters long. \\sharename Is the network name of the shared resource. \\volume Specifies a NetWare volume on the server. You must have Client Services for Netware (Windows Workstations) or Gateway Service for Netware (Windows Server) installed and running to connect to NetWare servers. password Is the password needed to access the shared resource. * Produces a prompt for the password. The password is not displayed when you type it at the password prompt. /USER Specifies a different username with which the connection is made. domainname Specifies another domain. If domain is omitted, the current logged on domain is used. username Specifies the username with which to logon. /SMARTCARD Specifies that the connection is to use credentials on a smart card. /SAVECRED Specifies that the username and password are to be saved. This switch is ignored unless the command prompts for username and password. /HOME Connects a user to their home directory. /DELETE Cancels a network connection and removes the connection from the list of persistent connections. /PERSISTENT Controls the use of persistent network connections. The default is the setting used last. YES Saves connections as they are made, and restores them at next logon. NO Does not save the connection being made or subsequent connections; existing connections will be restored at next logon. Use the /DELETE switch to remove persistent connections. NET HELP command | MORE displays Help one screen at a time. http://en.wikipedia.org/wiki/Net_use http://www.microsoft.com/resources/documentation/windows/xp/all/proddocs/en-us/net_use.mspx","tags":"it","title":"Command Line DOS Networking"},{"url":"https://blog.john-pfeiffer.com/eclipse-ide-c-wascana-on-windows-7/","text":"The Wascana IDE project was discontinued: http://speedydeletion.wikia.com/wiki/Wascana_Desktop_Developer which is probably why links no longer work Unfortunately I was trying to install the 64 bit version of everything below but could not find a reliable method of getting mingw 64 bit to work with Eclipse... See the very end for the workaround. (Note: having a 64 bit compiler should theoretically compile faster but with gcc 32 bit you can compile / target both 32 and 64 applications.) install JRE (64 bit) install Eclipse CDT (64 bit) install Wascana (mingw for eclipse) configure the path variable Install the Java Runtime Environment (JRE) A pre-requisite is to download the JRE (Java Runtime Environment, 5.0 or higher, newer is often better). FIRST, check if you have a 64 bit or older 32 bit system. If you have a 64 bit system, use a \"64 bit browser\" to go to the java page because otherwise it will keep giving you the 32 bit version to download... http://www.java.com/en/download/faq/java_win64bit.xml The link and filename should be something like: \"jre-6u22-windows-x64.exe\" Otherwise in Eclipse you may get the Error exit code=13 or \"failed to load the JNI shared library\"... This will most likely install in C:\\Program Files\\Java (or some variant). (with the \\bin\\javaw.exe) on Windows 7 the C:\\Program Files (x86)\\ directory contains 32 bit installations/applications Install Eclipse CDT (64 bit) The base Eclipse CDT supports integration with the GNU toolchain but may not come with a compiler... All Linux distributions include the GNU toolchain (but might not be installed by default...) MinGW provides the best integration support with the CDT due to it's direct support for the Windows environment. If you download the Eclipse IDE for C/C++ you'll get the \"CDT\" plugins along with the default Eclipse platform: http://www.eclipse.org/cdt/downloads.php The eclipse.ini file allows you to configure your program (e.g. specify the JRE location) (Notedpad2 or notepad++ handle the linux versus windows line breaks transparently...) e.g. insert the line to specify where your java run time environment is, maybe you have two... eclipse.ini -vm C:\\Java\\jre6\\bin\\javaw.exe Double clicking the eclipse.exe icon will start it with an empty Workbench (and use the default JRE) The first time you will be given the opportunity to choose your \"Workspace\" (aka directory where all of your files will be stored). I prefer to have it in the Eclipse folder but obviously in a multi user setup the \"My Docs\" or Network Folder would also make sense... or Dropbox? (DropBox -> Public might make Open Source Distribution even easier?) File -> New -> C Project Fill in the basics (you can choose the pre-made hello world app) Then click on the \"Go To Workbench\" so you can see the Project File Explorer, Code Editor, Console Window -> Preferences allows you to customize Eclipse e.g. disable usage statistics) Click on the hammer symbol (Build) to ensure that you create an object file (.o) before trying to test run an executable... gcc error Of course when you try to build you get an error... Internal Builder: Cannot run program \"gcc\" (in directory \"C:\\My Dropbox\\workspace\\hello\\Debug\"): CreateProcess error=2, The system cannot find the file specified Build error occurred, build is stopped Help -> Install New Software (previously Software Installer) Work With gets pasted the URL of the Wascana C/C++ compiler for Eclipse, then click ADD http://svn.codespot.com/a/eclipselabs.org/wascana/repo seems to have moved to Google Code but does not allow access, maybe http://sourceforge.net/projects/wascana/ Click on the Checkbox for \"Wascana C/C++ Developer for Windows, then NEXT (review items to be installed, e.g. wascana.core) NEXT, then Agree to the License Terms... After it downloads, installs, and restarts Eclipse you'll find the new mingw and msys directories in your Eclipse folder. Now you have to update the Path, in Windows it's usually under System Properties -> Advanced System Settings Environment Variables -> System Variables scroll area, highlight \"Path\" (click on the edit button) It should already have something like: %SystemRoot%\\system32;%SystemRoot%;%SystemRoot%\\System32\\Wbem; Append a semi colon to continue the long list and add: c:\\eclipse\\mingw\\bin Apparently some people feel eclipse does not autodetect unless it's c:\\mingw (Theoretically you could also try installing MingW directly from their website, which I've done, but it again is not 64 bit). Always 32 mingw gcc (oops) Unfortunately fundamental flaw - even when using Eclipse 64 bit the Install New Software gets the Wascana 32 bit mingw gcc. The Workaround is to use the Wascana Desktop as a single download/install (which includes 32 bit versions of: JRE 1.6.0 , Mingw 3.4.5 , Eclipse IDE) More info http://code.google.com/a/eclipselabs.org/p/wascana/ Apparently inaccessible due to strange Google Code permissions issues Also try: http://mclserver.eecs.ucf.edu/trac/courses/wiki/COP3402Spring2011/InstallEclipseCpp","tags":"programming","title":"Eclipse IDE C Wascana on Windows 7"},{"url":"https://blog.john-pfeiffer.com/drupal-security-tip-disabling-anonymous-access-to-cron/","text":"Drupal is a wonderful way of leveraging many open source advanced web features into one interface that conceivably can be handed off to a \"non developer\" to maintain. Along with all of the installation / implementation (often customized to fit the customers' needs) there are two further things that should be considered, Security and Useability. Here's some tips on security and maintenance. Drupal is a Content Management System that allows remote users to run scripts and access databases on your web server, this is a serious responsibility as Shared Hosting means your runaway/hacked scripts affects others, and Hackers/Spammers are always looking for new Zombies... Restrict access to PHP Scripts Restrict the PHP scripts access from ANONYMOUS USERS ON THE INTERNET! \"index.php\" should be allowed (it's your home page) but... Cron is the method in linux to run scheduled tasks. Drupal requires regular scheduled actions for maintenance (e.g. update content in search, cleaning up log files, checking for updates, etc.) http://drupal.org/cron.php (should not be) accessible but http://example.com/cron.php may be accessible to ANYONE as that's the default install =( To secure the cron.php file in .htacess, you have to do lock it down manually after installation. To block remote access to cron.php, in the server's .htaccess file or vhost configuration file: .htaccess Order Deny,Allow Deny from all Allow from localhost Allow from 127.0.0.1 Allow from xx.xx.xx.xx <-- your remote IP address Or protect update.php too in the .htaccess file: order deny,allow deny from all allow from 127.0.0 # add allowed specific remote IP addresses allow from a.b.c.d allow from a.b.c.d NOW ANONYMOUS ACCESS TO CRON.PHP should return either \"access denied\" or \"page not found\"... Running Drupal cron manually You can still run cron manually from either of the options below: Administer -> Reports -> Status http://example.com/admin/reports/status/run-cron There's even a way to schedule it to run against localhost or 127.0.0.1 (which is trusted in the .htaccess file we created above) Cron explained Cpanel -> Advanced -> Cron Jobs * * * * * http://example.com/cron.php (e.g. for rochen or bluehost cpanelx command should be the 8 char directory) php -q /home/yoursite/public_html/cron.php OR if you have multiple subdomains running different drupal installs: php -q /home/8chars/public_html/subdomain/cron.php Check using your drupal admin to ensure that the cron job has run Administer -> Reports -> Status This will allow you to test if your cpanel really has the correct permissions as Administer -> Reports -> Status should now show the cron job status as updated frequently! =) Here is a diagram of the general crontab syntax, for illustration: # +---------------- minute (0 - 59) # | +------------- hour (0 - 23) # | | +---------- day of month (1 - 31) # | | | +------- month (1 - 12) # | | | | +---- day of week (0 - 7) (Sunday=0 or 7) # | | | | | * * * * * command to be executed e.g. 59 23 31 12 * /bin/execute/this/script.sh the five stars (with a space in between each!) represent wildcards: when minute = 59 when hour = 23 when day = 31 when month = 12 every day (could be = 5 to limit it to only every friday) Update free access = FALSE /sites/default/settings.php should definitely have: $update_free_access = FALSE; Restricting file upload extensions Administer -> Site configuration -> File uploads \"Default permitted file extensions field\" for each role should be limited, because obviously you don't want ANONYMOUS users uploading .php files! (Or in INPUT FORMAT, .php code entered by an anonymous or hacked authenticated user!)","tags":"it","title":"Drupal Security Tip: disabling anonymous access to cron"},{"url":"https://blog.john-pfeiffer.com/drupal-6-wysiwyg-module-so-users-can-insert-images/","text":"Content manager made easy = WYSIWYG (what you see is what you get) (for Drupal 6) https://www.drupal.org/project/wysiwyg Download and install the Drupal 6 WYSIWYG module Download and install (aka get the .tgz and upload it to /sites/all/modules) and enable the module (in the main Drupal Modules menu) Enable it with: Administer -> Site building -> Modules Administer -> Site configuration -> Wysiwyg lists which editor modules you can use (and a helpful download link) e.g. TinyMCE (Download) Not installed. Extract the archive and copy its contents into a new folder in the following location: sites/all/libraries/tinymce Administer -> Site configuration -> Input formats Ensure that the \"Content Manager Role\" (or authenticated user?) has access to Full HTML Also you can set the default format to Full HTML (alternatively you can create a more limited input type role that matches your paranoia) Administer -> Site configuration -> Wysiwyg Input Format = Full HTML = TinyMCE 3.8.8 Edit the profile of your WYSIWYG editor to decide which buttons/functions are displayed Up until now (by default) you only get bold and inserting images that are already uploaded... Setting up IMCE (Image Manager) IMCE (image manager) (with the WYSIWYG bridge module) Download and install (aka get the .tgz and upload it to /sites/all/modules) and enable the module (in the main Drupal Modules menu) Administer -> Site building -> Modules Administer -> Site configuration -> IMCE -> User-1 Configure any specific settings, then give a role (e.g. Content Manager) permission Administer -> Site configuration -> IMCE Enable the Image button (as IMCE is accessed from the Image plugin). Enable the IMCE plugin in the plugins/buttons configuration of the wysiwyg profiles of your choice. (checkbox) Edit the profile of your WYSIWYG editor to decide which buttons/functions are displayed. You MUST include the checkbox IMCE... Permissions One of the common gotchas in Drupal is forgetting to set permissions (and having to dig through a ton of UI to find them) Ensure the future \"content manager\" role has create content permissions Administer -> User management -> Roles = Add Role Administer -> User management ->Permissions Then assign that role to the user Verify it all works Finally a user with the appropriate role (e.g. \"content manager\" above) can insert bold/underline/etc. and insert images (and upload photos using IMCE). Log In Create a new article/post In the UI you should see the menu has a lot more buttons","tags":"programming","title":"Drupal 6 wysiwyg module so users can insert images"},{"url":"https://blog.john-pfeiffer.com/css-3-column-liquid-layout-example/","text":"CSS is much better than HTML, but making a webpage look the way it ought to look can be very painful, frustrating, and time consuming. Not only do you have to create cross browser compatible code, but it has to look nice when you're done! This is just a basic example that you can experiment with and add to later, there are some comments but the code is mostly self explanatory: 3 column css source code < html xmlns = \"http://www.w3.org/1999/xhtml\" lang = \"en\" xml:lang = \"en\" > < head >< style type = \"text/css\" > /* required to kill off any extra \"helpful\" browser padding */ html , body { margin : 0 ; padding : 0 ; height : 99% ; } #container { min-height : 100% ; height : 100% ; margin : 0 ; border : 1px solid green ; } #column1 { float : left ; width : 33% ; height : 100% ; /* full length column */ position : relative ; border : 1px solid red ; } #column2 { float : left ; /* wraps the div around the left of the prev object */ width : 34% ; height : 100% ; position : relative ; border : 1px solid yellow ; } #column3 { float : right ; width : 33% ; margin-left : -1% ; /* prevent the right column from being pushed down! */ height : 100% ; position : relative ; border : 1px solid blue ; overflow : hidden ; } </ style > </ head > < body > < div id = \"container\" > < div id = \"column1\" > left left left left left left left left left left left left </ div > < div id = \"column2\" > center center center </ div > < div id = \"column3\" > right right right right right right right right right </ div > </ div > <!-- end div container --> </ body > </ html > 3 column css example /* required to kill off any extra \"helpful\" browser padding */html, body { margin: 0; padding: 0; height: 99%;} #container{ min-height: 100%; height: 100%; margin:0; border: 1px solid green;}#column1 { float: left; width: 33%; height: 100%; /* full length column */ position: relative; border: 1px solid red;}#column2 { float: left; /* wraps the div around the left of the prev object */ width: 34%; height: 100%; position: relative; border: 1px solid yellow;}#column3 { float: right; width: 33%; margin-left: -1%; /* prevent the right column from being pushed down! */ height: 100%; position: relative; border: 1px solid blue; overflow: hidden;} left left left left left left left left left left left left center center center right right right right right right right right right","tags":"programming","title":"CSS 3 column liquid layout example"},{"url":"https://blog.john-pfeiffer.com/tiny-core-linux-with-linksys-wireless-card-no-cd-required-installation/","text":"Tiny Core Linux is fast and works great... but it does take some effort to get setup... Here's how I got my Linksys wpc54g (v3) pci wireless card working with WPA - and I didn't burn a tiny core cd! You can't repartition a hard drive while actually using it so you'll most likely need GParted (ie from SystemRescueCD bootable cd) so that you can repartition / (root) and resize to have a spare linux partition... AND use: mke2fs -t ext3 /dev/hda3 (or wherever it is...) hda3 = third partition on the first hard drive, you may need to use fdisk -l or Start -> Control Panel -> Administrative Tools -> Computer Management -> Disk Management Prerequisites grub bootloader installed (preferrably to the MBR) tinycore.iso (cd image of tiny core installation/live cd) uniextract or isobuster (to open files from iso's) ext2fsd (winxp application that allows copying files in/out of an ext2/ext3 partition) tcz files from an FTP repository listed in the section \"Install on a Hard Drive Without Being Connected to the Internet\" from http://wiki.tinycorelinux.net/wiki:start#installing wireless-2.6.29.1-tinycore.tcz wireless_tools.tcz wpa_supplicant.tcz b43-fwcutter.tcz open-ssl-0.9.8m.tcz Getting the pieces ready Extract the files from the tinycore.iso (using IsoBuster or UniExtract)... OR if you have linux: mount -o loop /path-to-iso/image-filename.iso /mnt/custom We only need the bzImage and tinycore.gz files... CAPITAL I ON THE bzImage! USING Ext2 Volume Manager (ext2fsd) ... browse to your linux partition and create the following folder: /boot/tinycore Copy the \"bzimage\" and \"tinycore.gz\" files into the linux partition /boot/tinycore folder Also create the following text file: /tce/onboot.lst wireless-2.6.29.1-tinycore.tcz wireless_tools.tcz b43-fwcutter.tcz openssl-0.9.8m.tcz wpa_supplicant.tcz nano.tcz BE CAREFUL TO NOT HAVE ANY MISSPELLINGS OR EXTRA SPACES Finally, create the directory /tce/optional and copy the above .tcz files into it. TinyCore uses /tce/mydata.tgz to store your files in the /home and /opt directories. (Therefore you could sneak something in if you wanted to...?) ALSO, it uses .ashrc (e.g. not BASH command prompt) so any aliases are in /tce/mydata.tgz -> home/tc/.ashrc Modify your Grub (legacy) menu menu.lst title tinycore root (hd0,2) kernel /boot/tinycore/bzimage initrd /boot/tinycore/tinycore.gz Oh Wait, Wifi Drivers! BUT my wifi depends on this Linksys wpc54g (v3) pci wireless card AND wpa encryption... So I've got wl_apsta.o from my previous debian kernel 2.6.26 (with all of the linux-header and make and compiling commands to get that binary...) Without the correct fw5 (b43 firmware) dmesg will contain: b43-phy0 ERROR: firmware file \"b43/ucode5.fw\" kernel firmware unhappy with wrong linksys driver COPY wl_apsta.o into /mnt/hda3/tce (with EXT2FSD or usb stick or whatever) Rather than hack into my-data.tgz we'll wait until we've booted into Tiny Core... Booting into Tiny Core Linux BOOTING INTO TINY CORE IS VERY FAST... (fingers crossed about everything before) Right Click -> Control Panel or a funny icon in the middle with screwdriver = Control Panel First check that our \"onboot.lst\" hack worked: Apps Audit -> OnBoot -> Maintenance (could also use: nano /mnt/hda3/tce/onboot.lst) WHEN YOU CHOOSE TO SAVE/BACKUP (or when prompted when closing to Save a Backup) mydata.tgz is created and it includes any modifications to /opt/bootlocal.sh since things put in the bootlocal.sh script are run as root... my wifi hack works... nano bootlocal.sh mkdir /lib/firmware b43-fwcutter -w /lib/firmware /mnt/hda3/tce/wl_apsta.o wpa_supplicant -B -iwlan0 -c/mnt/hda3/tce/wpa_supplicant.conf udhcpc -H hostname -b -i wlan0 the /lib/firmware directory is necessary for the kernel to get the new drivers the b43-fwcutter firmware cutter gets the drivers to the directory wpa_supplicant starts in the background using wlan0 and the config file wpa_supplicant.conf wpa_passphrase ssid-network-name > wpa_supplicant.conf prompts for the wireless network password, after you type it in press enter udhcpc is busybox's dhcp client using \"hostname\", in the background on wlan0 An alternative configuration in bootlocal.sh for a static ip would be... ifconfig wlan0 10.0.0.99 netmask 255.255.255.0 up route add default gw 10.0.0.138 echo \"nameserver 10.0.0.138\" > /etc/resolv.conf Verify after reboot AFTER I REBOOTED ifconfig wlan0 //shows me my ip address ping 10.0.0.138 and ping http://kittyandbear.net ALL OK! Adding a browser to Tiny Core Linux Of course, now I have to install a browser... Since tinycore works from the core image and with then added modifications to be as lean and fast as possible you really need to explicitly choose what you want on your hard drive AND what you want started at boot time. Right Click -> AppsBrowser or funny icon on the bottom right (gears) File -> Install Local Extension (anything on your hard drive but not in onboot.lst) By default it lists your TCE/optional directory, double click on the one you want... (If it isn't onboot and it isn't \"installed\" by above then it's not on your tinycore yet!) OR File -> AppsBrowser ... when you choose to install something from the \"repository\" be prepared to wait for about 5 minutes for it to load the hundreds of packages... Miscellaneous Tiny Core Linux Notes Right Click -> Shells -> Shell Dark or funny icon on the bottom left (terminal with prompt) Right Click -> Control Panel -> System Stats OR funny icon in the middle with screwdriver = Panel -> System Stats dmesg TAB shows you the b43 stuff cpu is cpu type mem is RAM free net is network devices installed... loadkmap < /usr/share/kmap/uk.kmap","tags":"linux","title":"Tiny Core Linux with Linksys Wireless Card - no CD required installation"},{"url":"https://blog.john-pfeiffer.com/electric-car-rebate-why-not-go-european/","text":"I don't expect anybody to read this as Slashdot articles grow exponentially in comments each hour but... The price of petrol in London is about 116 pence per liter http://www.whatprice.co.uk/petrol-prices 1 US gallon = 3.78 liter 4.39 GBP (pounds) per gallon 1 pound = 1.5 dollars (exchange rates are always crazy) = 6.59 dollars per gallon miles driven / mpg ... -> total cost of gasoline? 195k / 49.5 = 3939.40 * $6.59 = $25,960 (hybrid) 195k / 30 = 6500 * $6.59 = $42,835 (fuel efficient car) 195k / 15 = 13000 * $6.59 = $85,670 (normal/big car) So, while many will argue that \"Europeans\" are \"controlling their oil consumption\" through taxes, I would argue that the world has been susidizing the oil industry. Additionally, many American vehicles get 20 mpg or even 15 mpg. PLEASE REMEMBER, money is fiction (pieces of paper), work is economic fiction, government is fiction, and the price of Gas/Fossil Fuels is fiction. We all agree to a system but the system can and should be changed towards improvement. IEA: To promote efficiency, cut fossil fuel subsidies http://www.cnet.com/news/iea-to-promote-efficiency-cut-fossil-fuel-subsidies/ http://www.iea.org/files/energy_subsidies.pdf http://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/publications/national_transportation_statistics/html/table_04_23.html Table 4-23: Average Fuel Efficiency of U.S. Light Duty Vehicles So perhaps instead of silly electric car rebates we should reduce Fossil Fuel susbidies and increase the tax on gas (yes, there are both Federal and State taxes on gas already so I'm not proposing some radical communist ideology), thereby making the market pay the actual costs (not even counting externalities like environmental impact!).","tags":"it","title":"Electric Car Rebate... Why not go European?"},{"url":"https://blog.john-pfeiffer.com/cant-delete-ftp-folder-in-drupal-filezilla-hidden-files/","text":"I couldn't delete a folder in FTP (which can be pretty frustrating) until I realized that Filezilla (my FTP/SFTP application with UI of choice) has an option to \"force showing hidden files\" In Filezilla 3 = Server -> Force showing hidden files I could then see the .htaccess file and delete it. (Right click on a folder/file and File Attributes shows me the permissions e.g. Read / Write / Execute)...","tags":"it","title":"Can't delete FTP folder in Drupal - Filezilla hidden files"},{"url":"https://blog.john-pfeiffer.com/how-to-install-a-drupal-tag-cloud/","text":"How to install a Drupal Cloud Tag Block First, in case you've stumbled on this by mistake (you were looking for Blocks of Drupal \"Cloud\" cheese with price tags?)... Drupal is a \"content management system\" - a fancy way of saying software that takes your text/photos and makes them pretty... automatically! Tag is an associated \"label\" - kind of like you might call a sandwich a \"cheese sandwich\", or \"lunch\", or a \"snack\". It's a neat free form way to categorize your thoughts without much effort (oh the wonders of Computers). The tagadelic module for Drupal creates a weighted \"cloud\" of the tags you have on your content nodes. If you create a \"block\" on your pages it is MUCH easier for you and any readers (if you have them?) to navigate... Download the module http://drupal.org/project/tagadelic Extract the tar.gz (izarc2go and 7zip) into a folder Upload the folder to /drupal-root/sites/all/modules (using FTP or preferrably SFTP) Administer -> Site building -> Modules -> List (It appears under Taxonomy ... Tagadelic , fill in the checkbox to Enable the module. Administer -> Site building -> Blocks -> List Blocks Tagadelic has already created the following Block parts for you: Tags in Blog Tags Use the dropdown to put the tag in the \"Right sidebar\" Then drag and drop it to determine it's order (ie below the search box) Finally click on \"configure\" next to it to select: customize the \"title\" above the cloud tag (e.g. Tag Cloud) what pages the block will appear. Note: Tags for the current post is an easy way to show \"related content\" These tags also may help your SEO but be careful to not overdo it! I personally like adding the other Block, \"Tags for this post\" as well because As per my very smart's wife very smart suggestion - an image... [![example-of-a-drupal-tagadelic-tag-cloud.gif]]","tags":"programming","title":"How to install a Drupal tag cloud"},{"url":"https://blog.john-pfeiffer.com/americas-great-recession-and-wealth-without-borders/","text":"Why do countries like Germany and Japan have such a high quality of life AND high productivity? Remember that their unemployed have free medical care, education, and unemployment benefits... http://www.oecdbetterlifeindex.org/countries/germany http://www.oecdbetterlifeindex.org/countries/japan http://www.oecdbetterlifeindex.org/countries/united-states Are they just smarter than Americans? ( While the US has the \"highest average income\" it comes at the price of the lower tiers not having the same opportunities for medical care and education, so maybe it's an active choice of American inequality? ) \"The economists Emmanuel Saez and Thomas Piketty examined tax returns from 1913 to 2008. They discovered an interesting pattern. In the late 1970s, the richest 1 percent of American families took in about 9 percent of the nation's total income; by 2007, the top 1 percent took in 23.5 percent of total income.\" Lower income taxes encourages risk taking and short term \"me first now\" gains. Higher income taxes motivates creating methods of long term wealth accrual, e.g. businesses with continual steady profits. Corporations and Wealthy Individuals do not have geographic or political boundaries. It is the mainstream and poor of a society that cannot simply pack up and move some place sunnier with low taxes (Monaco). So don't worry about the \"wealth moving away because of high taxes\"... it's already done so (e.g. Microsoft Ireland and Offshore bank accounts etc.) http://www.gpo.gov/fdsys/pkg/CHRG-110shrg45575/html/CHRG-110shrg45575.htm http://ctj.org/ctjreports/2014/05/dozens_of_companies_admit_using_tax_havens http://www.bloomberg.com/news/articles/2010-05-13/american-companies-dodge-60-billion-in-taxes-even-tea-party-would-condemn What's left? Businesses and Organizations that bring people together and give them a common purpose. They receive money for their time and work which is then spent in the local economies. I call it the Exponential Spending Effect of capitalism (as opposed to the failed Trickle Down Economic Policy). There's a lie that \"businesses are made for profit\" - If you own and work in a business (the quintessential small business) and you pay your employees, suppliers, contractors regularly and you receive a salary of $100,000 but your business doesn't show a profit... You can still have a wonderful life! and everyone in that interconnected ecology is healthy and stable. In fact, you're even contributing to the mythical GDP. http://www.economist.com/blogs/economist-explains/2014/03/economist-explains-26 Yet if policemen, firemen, nurses, teachers, etc. aren't able to work and spend then the quality of life for everyone will degrade into misery. The wealth gap will drive America's great experiment, a Democratic Capitalist Republic, into Depression and Oligarchy... http://en.m.wikipedia.org/wiki/Income_inequality_in_the_United_States Enrich your mind: Your vote makes a difference! http://www.usa.gov/Citizen/Topics/Voting/Register.shtml","tags":"puzzles","title":"America's Great Recession and Wealth Without Borders"},{"url":"https://blog.john-pfeiffer.com/outlook-rpc-over-http-with-a-non-standard-port/","text":"\"The proxy server you have specified is invalid. Correct it and try again.\" Oh, the wonderful error messages from Microsoft... So Outlook 2003 has HTTP and HTTPS hard coded to ports 80 and 443 (wonderfully modular thinking). Imagine you want to move your Outlook Web Access to a different port (security reasons? Or maybe just that another application is hard coded to port 443...) Now it's easy to tell people: https://mailserver.example.com:4430 BUT you might have to update any Blackberry using OWA to connect to Exchange users with the new port... and RPC over HTTP (s) is quite useful for the non VPN inclined ... The following unsupported workaround works, use at your own risk, registry editing is required... The UI Configuration In Control Panel -> Mail -> Email Accounts -> View or Change -> Change (button) -> More Settings (button) -> Connection (tab) Checkbox: Connect to my Exchange mailbox using HTTP Then the button: Exchange Proxy Settings https://mailserver.example.com:4430 Connect using SSL only (for the paranoid) Mutually authenticate... Yes, we need the following: msstd:mailserver.example.com (note that you may have to download and install the certificate from your mail server for RPC over HTTP to work, the name we've entered doesn't have a port because it's the name that's on the SSL certificate) Authentication (NTLM = SSL, Basic means anything - i.e. not encrypted) Click ok a million times and we've finished the easy part... Hacking the Windows Registry Start -> Run -> regedit (and hit the OK button) (HKCU = hkey_current_user) HKCU\\Software\\Microsoft\\Windows NT\\CurrentVersion\\Windows Messaging Subsystem\\Profiles\\ Browse down to the subkey: \"13dbb0c8aa05101a9bb000aa002fc45a\" (don't ask me why these settings are exactly there) Locate in the name column: \"001f6622\" of type REG_BINARY and double click on it... The \"Value Data\" will be in hex code (with a preview of the ascii on the right) 0000 6D 00 61 00 69 00 6C 00 m.a.i.l. 0008 65 00 72 00 2E 00 65 00 ......d. 0010 78 00 61 00 6D 00 70 00 o.m.a.i. 0018 6C 00 65 00 2E 00 63 00 n......c. 0020 6F 00 6D 00 2D 00 34 00 o.m.-.4. 0028 34 00 33 00 30 00 30 00 4.3.0.0. 0030 00 00 You'll see the 2D in the middle - that's the hex code for '-', we're going to change it to 3A (or the hex code for ':') (yes, we click inside, use the delete key to remove those two and type in 3A) 0020 6F 00 6D 00 3A 00 34 00 o.m.:.4. Click OK... Whew, hard part's over... Verify the Changes Now check that your change has taken by going to: Control Panel -> Mail -> Email Accounts -> View or Change -> Change (button) -> More Settings (button) -> Connection (tab) You should now see: https://mailserver.example.com:4430 Note that you won't be able to click OK (because Outlook detects that unpermitted colon) but hit Cancel a bunch of times and open up Outlook to try connecting to your Exchange Server! PLEASE NOTE: first ensure that you can get RPC over HTTP working with the default port 443 (e.g. no colons needed) BEFORE trying a non standard port as it is a little tricky to remember the certificate, the firewall port forwarding (if you need to), etc. ALSO, remember that to do this you've already changed your Exchange Mail Server IIS SSL port to the non standard 4430 AND you've fixed any firewall forwarding for your server so that 4430 goes to your mail server...","tags":"it","title":"Outlook RPC over HTTP with a non standard port"},{"url":"https://blog.john-pfeiffer.com/the-magical-million-3-helicopters-scratchcard-scam/","text":"The Magical Million scratch card scam... Magazines and newspapers get money from advertisers, some of which run contests and promotions. I was not surprised to see that Rupert Murdoch's \"Times\" had scratch cards for the Magical Million from PurelyCreative.com Considering the fake news from Fox, it's no surprise there's a long standing fake contest advertised in Murdoch's paper. A scatch card says \"3 identical symbols - you can claim a GUARANTEED CASH PAYOUT\". So our three helicopters... by calling the £1.50 per minute premium number (or premium text messages) you can find out how much you've won... which you then send by post (including a return stamp) and wait months. When you chase them up many times in many months you might receive your £10 award (in vouchers?!?!), or then again you might not Greed. Something for nothing. You're luckier than everyone else... If you are silly enough to insist on \"playing\" (because you already are a winner), then hopefully you'll recycle the scratch card and go buy a lottery ticket. Consider that £9 buys a lot of fun for a homeless person in your neighborhood, or supports your local charity (RSPCA, Red Cross, etc.) These companies are the lowest level of legal scam possible (eventually they graduate into selling derivatives, junk bonds, or insurance); if you are using a computer and the internet to read about this scam then you should be too smart to fall for it. Oh, do you want some \"research\", try http://blogs.mirror.co.uk/investigations/2010/01/watchdog-takes-scratchcard-fir.html Original link is too old for the Mirror to keep around so some updated ones: http://www.bbc.com/news/uk-19993931 http://www.mirror.co.uk/news/uk-news/king-scam-mail-conmen-adrian-3719698","tags":"puzzles","title":"The Magical Million 3 helicopters scratchcard scam"},{"url":"https://blog.john-pfeiffer.com/css-3-column-liquid-layout-with-background-image-stretch/","text":"CSS keeps improving but sometimes it takes some creativity to meet what might seem like obvious demands: a background image stretched in a column The following example gives two different methods of background image stretching, though I admit that the background image is just a color gradient and that this kind of stretching on a graphical image (in a liquid layout) could appear at best, \"funny\". I created a color gradient and then resized it to be 640x2 pixels, otherwise all of the code is below: 3 column liquid layout with background image stretch code < html xmlns = \"http://www.w3.org/1999/xhtml\" lang = \"en\" xml:lang = \"en\" > < head >< style type = \"text/css\" > /* required to kill off any extra \"helpful\" browser padding */ html , body { margin : 0 ; padding : 0 ; height : 99% ; } #container { min-height : 100% ; height : 100% ; margin : 0 ; border : 1px solid green ; } #column1 { float : left ; width : 20% ; height : 100% ; /* full length column */ position : relative ; border : 1px solid red ; /* bg-body-left is a 640 wide by 2 pixel tall image color gradient */ background-image : url('bg-body-left.png') ; background-repeat : repeat-y ; } #column2 { float : left ; /* wraps the div around the left of the prev object */ width : 60% ; height : 100% ; position : relative ; border : 1px solid yellow ; } #column3 { float : right ; width : 20% ; margin-left : -1% ; /* prevent the right column from being pushed down! */ height : 100% ; position : relative ; border : 1px solid blue ; overflow : hidden ; } /* a class is defined below called stretch to force an image to stretch */ .stretch { width : 100% ; height : 100% ; } </ style > </ head > < body > < div id = \"container\" > < div id = \"column1\" > left left left left left left left left left left left left </ div > < div id = \"column2\" > center center center </ div > < div id = \"column3\" > right right right right right right right right right < img src = \"bg-body-left.png\" class = \"stretch\" alt = \"\" /> </ div > </ div > <!-- end div container --> </ body ></ html > Please leave a comment if you've appreciated this or any other posts on my site! Thanks! -John 3 column liquid layout with background image stretch example /* required to kill off any extra \"helpful\" browser padding */ html, body { margin: 0; padding: 0; height: 99%; } #container { min-height: 100%; height: 100%; margin:0; border: 1px solid green; } #column1 { float: left; width: 20%; height: 100%; /* full length column */ position: relative; border: 1px solid red; /* bg-body-left is a 640 wide by 2 pixel tall image color gradient */ background-image: url('bg-body-left.png'); background-repeat: repeat-y; } #column2 { float: left; /* wraps the div around the left of the prev object */ width: 60%; height: 100%; position: relative; border: 1px solid yellow; } #column3 { float: right; width: 20%; margin-left: -1%; /* prevent the right column from being pushed down! */ height: 100%; position: relative; border: 1px solid blue; overflow: hidden; } /* a class is defined below called stretch to force an image to stretch */ .stretch { width:100%; height:100%; } left left left left left left left left left left left left center center center right right right right right right right right right","tags":"programming","title":"CSS 3 column liquid layout with background image stretch"},{"url":"https://blog.john-pfeiffer.com/french-toast-recipe-aka-purjeni-filiki/","text":"First put oil in a pan and heat it (we use temperature setting 3 of 6 on our electric stove). Then beat/whip two or three eggs in a bowl, then take slices of bread and dip them in the whipped eggs (or you can soak them if you want them more egg full) and put them on the frying pan to fry. The medium heat should cook the egg slowly (too hot and the egg turns white/yellow like fried eggs on bread), after about 5-10 minutes? (peek underneath and check if they're golden brown) you flip them and cook the other side. I sometimes add my secret ingredient, a slice of butter on top to melt, after flipping them over. When done you can have them with one of the following yummy toppings: soft cream cheese (Philadelphia cream cheese) kashkaval or mozarella and honey jam (raspberry/blueberry/strawberry you name it!) helva To quote Bobby \"Filiki are divine!\" or \"Helva na filiki e strahotno\".","tags":"puzzles","title":"French Toast recipe aka Purjeni Filiki"},{"url":"https://blog.john-pfeiffer.com/blackberry-enterprise-server-express-on-same-domain-as-bes-windows-and-exchange-2003/","text":"Goal: Blackberry Express Server install: Windows 2003 with Exchange Server 2003 with BES 5 already installed Why Blackberry Express? Well it's the core Blackberry experience (email + contacts + calendar) but only requires a data plan, not a special (expensive) Blackberry plan. The big item missing: Wireless activation is only available with BES dataplan. http://crackberry.com/blackberry-101-lecture-2-bes-and-bis-whats-difference The dilemma, if you already have a BES installation, can you setup a BESX too? \"the two BES servers cannot be in the same BES Domain, but can be in the same AD tree and access the same exchange server.\" \"provided BlackBerry Enterprise Server Express is introduced as a new deployment with its own BlackBerry domain, as defined by the BlackBerry configuration database... the BlackBerry Enterprise Server and BlackBerry Enterprise Server Express can run independently in the same Microsoft Exchange environment in a Microsoft Windows Domain and would be managed from separate BlackBerry Administration Service consoles.\" Apparently there is a difference between a BES Domain and a windows domain... Basically a BES Domain SHOULD only be the BES installation and Database, therefore you should be able to have multiple BES installations in the same Windows Domain (e.g. a large corporation with many exchange servers all in the same Windows Domain?). I will try it with my own twist - Not only a separate BESX installation on a windows 2003 server BUT also creating a separate BESADMIN for the new BESX installation. http://supportforums.blackberry.com/t5/BlackBerry-Professional-Software/Installing-BESX-when-BES-5-exists/m-p/488112 Windows Server and Exchange Server Setup Setup a Windows 2003 Server SP2 in the Domain (exchange sys mgmr must match version to exchange server!) NEEDS at least 1GB RAM - but give it more if you suffer performance issues! Install Exchange 2003 SP2 System Manager (Requires Exchange Install CD -> Deployment Tools) exch2k3\\setup\\i386\\setup.exe => Action = Custom Action = Install (next to MS Exch System Management Tools) ... NEXT... 112MB required, NEXT Download and Install Exchange Service Pack 2 (E3SP2ENG.exe for an old Exchange 2k3 cd) http://www.microsoft.com/downloads/details.aspx?FamilyID=535bef85-3096-45f8-aa43-60f1f58b3c40&displaylang=en E3SP2ENG\\setup\\i386\\update.exe (goes from version 6.5 to ? requiring 2 & 13 MB space) Ensure TCP port 3101 (outgoing) is open on your firewall Ensure your anti-spam is not blocking \"blackberry.net\" Permissions for a service account for BlackBerry Enterprise Server for Microsoft Exchange http://www.blackberry.com/btsc/viewContent.do?externalId=KB02276 CREATE a user BESADMIN for the Domain (in Active Directory Users and Computers) On the exchange server with the Mailbox enabled user creation dsa.msc -> right click = new user PERMISSION SEND AS: On the domain, dsa.msc From the Active Directory \"View\" option choose Advanced Features Right click on the root of the domain for Properties -> Security -> Advanced Add BESADMIN and Apply Onto = User Objects (dropdown) , Allow = Send As (checkbox) Maybe? more secure: only right click on each OU or user that will be using Blackberry and give the BESADMIN \"Send As\" permission PERMISSIONS Exch 2k3 System Manager -> Administrative Groups right click the Group for your BES (e.g. First Administrative Group) -> Delegate Control -> Add Browse -> Role = Exchange View Only Administrator PERMISSIONS Exchange Server: Start -> Programs -> Microsoft Exchange -> System Manager Administrative Groups -> First Administrative Group -> Servers -> right click SERVERNAME (properties) Security -> find the BESADMIN and enable checkboxes: Administer Information Store Send As Receive As (Click on Advanced and ensure that \"Allow inheritable permissions\" is checked) PERMISSIONS Local Admin: each server that will have Blackberry Enterprise Server Express components My Computer right click -> Manage -> Local Users and Groups -> Groups -> Administrators Add = BESADMIN My Computer right click -> Properties -> Remote -> Enable Remote Desktop -> Select RemoteDesktop Users => Add = BESADMIN PERMISSIONS Log on Locally and Log on as a Service Start -> Administrative Tools -> Local Security Settings => Local Policies -> User Rights Assignment double click Log on Locally & Log on as a Service and add BESADMIN Installing Blackberry (and Database) BLACKBERRY warn that you need the Microsoft hotfixes 823343 and 894470 , http://support.microsoft.com/kb/823343 , http://support.microsoft.com/kb/894470 Verify by c:\\exchsvr\\bin\\cdo.dll 708KB right click Version 6.5.7232 or later REBOOT the server (ok, just an old habit) Log in as the BESADMIN user C:\\Research In Motion\\BlackBerry Enterprise Server 5.0.1\\setup.exe Create a Blackberry Configuration Database (aka BES Domain?) Blackberry Enterprise Server with all components preinstallation checklist will show you everything is ready (or will be auto installed) Install MS SQL Server 2005 Express SP3 note that the extracted setup folder is very similar to the target install folder C:\\Research In Motion\\BlackBerry Enterprise Server 5.0.1 C:\\Program Files\\Research In Motion\\BlackBerry Enterprise Server\\ enter BESADMIN password and the NAME of the Server where the SQL Express will be installed (e.g. the name of the server you are installed Blackberry Express!) read the summary, click INSTALL ... watch and wait. You are prompted to restart the computer - do so and then log in again with the BESADMIN user. Installation continues with the Database Information ... just click Next The database BESMgmt doesn't exist, would you like to create it ... YES Enter Blackberry CAL Key e.g. besexp-123456-123456-123456-123456 SRP Host name: gb.srp.blackberry.com and port number: 3101 were already provided SRP identifier = (Serial Number from Blackberry download) S12345678 SRP authentication key = (Licence Key from Blackberry download) 1234-1234-1234-1234-1234-1234-1234-1234-1234-1234 CLICK VERIFY BUTTON 1 AND BUTTON 2 (should be successful and valid! NEEDS dashes - inbetween!) Microsoft Exchange Server popup - type in the Exchange Server Name Administration Settings (already filled in by default) CLICK NEXT (no, I don't want to use SSL between the Blackberry Admin and my LAN browser) Type in the BESADMIN password and click NEXT Advanced Administration = leave as windows default click NEXT click Start Services button BlackBerry Router has successfully started. BlackBerry Attachment Service has successfully started. BlackBerry Dispatcher has successfully started. BlackBerry MDS Connection Service has successfully started. BlackBerry Alert has successfully started. BlackBerry Administration Service - NCC has successfully started. BlackBerry Administration Service - AS has successfully started. BlackBerry Controller has successfully started. Make a note of the Web Admin address(es) https://SERVERNAME.DOMAIN.CO.LOCAL:3443/webconsole/login https://SERVERNAME.DOMAIN.CO.LOCAL:3443/webdesktop/login Test your Blackberry Express Installation Locally on the Server you can check services.msc (all BB services started) and eventvwr.msc (no Blackberry errors) Use a browser (IE8?) https://SERVERNAME.DOMAIN.CO.LOCAL:3443/webconsole/login (don't worry about the Browser \"Security Alert\" as it will be a self signed SSL certificate, you can install the certificate and add it to Trusted Sites too...) Install the RimWebComponents.cab Create a user (you just need their email address - user should only be on one BES Domain so not on BES 5 and BES X at the same time!) Create User with an Activation Password (e.g. something simple that times out in 4 hours) Wait until it gives you the OK message that the user was created (and activation email sent) Assign a device to a User http://docs.blackberry.com/en/admin/deliverables/14334 Using the Blackberry Administration Service (web) https://SERVERNAME.DOMAIN.CO.LOCAL:3443/webconsole/login On the left there's a DEVICES area -> Attached Devices (if it doesn't expand?) Connect the BlackBerry device to the computer. Click Manage current device -> Click Assign current device -> Search for a user account Users can activate their BlackBerry devices by connecting them to computers using a USB cable or Bluetooth connection and logging in (with a browser) to the BlackBerry Web Desktop Manager. https://SERVERNAME.DOMAIN.CO.LOCAL:3443/webdesktop/login When users complete the activation process, the BESX synchronizes through the BlackBerry Router. If a connection to the BlackBerry Router is interrupted, the data transfer continues over the wireless network. Blackberry Desktop Manager installed on a computer (and connected to their Outlook Profile) Attach/Connect to a Device Not likely to be enabled but theoretically after sending the Activation Password the BlackBerry Enterprise Server sends an email message with an etp.dat On the Device choose Options -> Advanced Options -> Enterprise Activation \"Activation request failed. A service connection is unvailable\" Tips Setup a default Password requirement Policy -> Manage IT Policies -> Edit IT Policy (default) -> Device Only Tab Increase the default synchronization of messages when activating Servers and components -> BlackBerry Solution topology -> BlackBerry Domain -> Component view -> Email Click on the \"instance\" (e.g. computername_EMAIL) -> click on Messaging tab Scroll down and click on \"Edit Instance\" Change Message prepopulation settings to 14 days and 750 messages scroll down and click SAVE ALL Useless Steps from Blackberry Install SQL Express Service Pack 3 (already included in the Blackberry Express Install!) - <http://www.microsoft.com/downloads/details.aspx?FamilyID=3181842a-4090-4431-acdd-9a1c832e65a6&displaylang=en> - OTHERWISE theoretically you could re-use an existing SQL database (maybe with old Blackberry data?) - Install CDO support (for Exch 2010) <http://support.microsoft.com/kb/917481> - Microsoft Exchange Server MAPI Client and Collaboration Data Objects 1.2.1 - <http://www.microsoft.com/downloads/details.aspx?FamilyID=4825F157-5816-4802-850D-67A0C5423770&displayLang=en> TEST your BESADMIN account access to User Accounts - double click on the self extracting BESX_express_5.0.1.exe but DO NOT run setup.exe) - `C:\\Research In Motion\\BlackBerry Enterprise Server 5.0.1\\tools\\IEMSTest.exe` - The setup application configures the startup type for the BlackBerry Mail Store Service, BlackBerry Policy Service, and BlackBerry Synchronization Service to manual. - You cannot activate a BlackBerry device that is associated with the BlackBerry Internet Service over the wireless network or over wifi.","tags":"it","title":"Blackberry Enterprise Server Express on same domain as BES (windows and exchange 2003)"},{"url":"https://blog.john-pfeiffer.com/is-programming-simple-contrasting-fizzbuzz-solutions-365-programming-project-day-forty-two/","text":"Simple Programming Challenge: FizzBuzz The following is an example of a simple programming \"challenge\": Write a program that prints the numbers from 1 to 100. But for multiples of three print \"Fizz\" instead of the number and for the multiples of five print \"Buzz\". For numbers which are multiples of both three and five print \"FizzBuzz\". Below I've thrown together a solution in less than 5 minutes, BUT, I thought to myself, experimentally, what would the code look like if I needed something more \"Best Practice\"... Quick and Dirty FizzBuzz Solution in C /* 2010-03-29:1800 john pfeiffer \"simple programming\" examples */ #include <stdio.h> int main ( int argc , char * argv [] ) { int i = 0 ; for ( i = 1 ; i <= 100 ; i ++ ) { if ( i % 3 == 0 ) { printf ( \"Fizz\" ); } if ( i % 5 == 0 ) { printf ( \"Buzz\" ); } if ( ( i % 3 != 0 ) && ( i % 5 != 0 ) ) { printf ( \"%d\" , i ); } printf ( \" \\n \" ); } return 0 ; } /* end main */ Imagine instead of a challenge it's a professional assignment, requiring scalability, portability, modular parts, future maintenance (by someone totally different who may not be very good at coding and/or not have a lot of time to understand the code)... Suddenly a simple answer transforms into the following: Complex \"Enterprise\" solution to FizzBuzz /* 2010-03-29 john pfeiffer \"simple programming\" examples */ #include <stdio.h> #include <stdlib.h> #include <string.h> #define BUFFERMAX 128 /* prototyping of functions */ void output ( char text [ BUFFERMAX ] ); void writeToBuffer ( char buffer [ BUFFERMAX ], char text [ BUFFERMAX ] , int datasize ); int isMultipleofThree ( int i ); int isMultipleofFive ( int i ); /* ------------ MAIN ------------------------------------ */ int main ( int argc , char * argv [] ) { int counter = 0 ; char buffer [ BUFFERMAX ]; char temp [ 8 ]; for ( counter = 1 ; counter < 101 ; counter ++ ) { /* clear the output buffer each time */ memset ( buffer , 0 , sizeof ( buffer ) ); if ( isMultipleofThree ( counter ) || isMultipleofFive ( counter ) ) { if ( isMultipleofThree ( counter ) ) { writeToBuffer ( buffer , \"Fizz\" , strlen ( \"Fizz\" ) ); } if ( isMultipleofFive ( counter ) ) { writeToBuffer ( buffer , \"Buzz\" , strlen ( \"Buzz\" ) ); } } else { sprintf ( temp , \"%d\" , counter ); writeToBuffer ( buffer , temp , strlen ( temp ) ); } output ( buffer ); printf ( \" \\n \" ); } /* end for i=1 to 100 loop */ return 0 ; } /* end main */ /* ------------- FUNCTION DEFINITIONS ---------------------- perhaps better to put all function definitions in an include file? The modular abstraction of creating more functions allows us to swap out an existing implementation, e.g. if there's a faster way of determining \"multiple of three\" It also improves portability because most of the code would remain the same except the output function writing to a win32 Device Context ... aka window Or we could quickly add functionality by calling a new \"write to log\" function... */ /* display a string to the stdout */ void output ( char text [ BUFFERMAX ] ) { printf ( \"%s\" , text ); } /* write data to the buffer */ void writeToBuffer ( char buffer [ BUFFERMAX ], char text [ BUFFERMAX ] , int datasize ) { int i = 0 ; for ( i = 0 ; counter < datasize ; i ++ ) { buffer [ i ] = text [ i ]; } } /* determine if the parameter is a multiple of three using \"modulo\", if true return 1, if false return 0 */ int isMultipleofThree ( intcounter ) { if ( counter % 3 == 0 ) { return 1 ; } else { return 0 ; } } /* determine if the parameter is a multiple of five using \"modulo\", if true return 1, if false return 0 */ int isMultipleofFive ( intcounter ) { if ( counter % 5 == 0 ) { return 1 ; } else { return 0 ; } } And that long convoluted giant isn't even modular enough! Obviously the two \"isMultiple\" functions could both rely on a common modulo wrapper function... I guess at a certain point it will start looking like Java or C# ... Where if you want to do anything you have to look it up in the manual and change the parameters and hope the designer of the function didn't do anything buggy...","tags":"programming","title":"Is Programming Simple? Contrasting FizzBuzz Solutions: 365 programming project day forty two"},{"url":"https://blog.john-pfeiffer.com/windows-ce-programming-writing-text-to-the-display-365-programming-project-day-forty-one/","text":"Building a program properly requires a lot of discipline define goals (what functionality will be achieved?) write up a high level flow/state chart create modular parts from the flow chart (e.g.functions) create tests - e.g. know what input goes in and what should come out fill in the functions with dummy information (e.g. always return constants) integrate and ensure that your \"demo\" version achieves your goal Note that all of this ignores the tools to be used, estimating time and cost, scheduling, etc. BUT you could just as easily use the above for your \"Life Plan for Success\"... Real life example of \"programming\" for success Goal: I want to play professional soccer find the position I am best at go to at least 3 tryouts Flow: fitness -> skills -> recognition Modular Parts: physical fitness skills networking and agent tryout special training camps + video of playing feedback from experts on my best position Tests: Must run 5km in under 18 minutes. Must sprint 40 yards in 5 seconds. Must be able to shoot the ball from 30 yards out into top quarter of the goal 10 out of 10 times Agent must have history of signing players to contracts Tryouts must show a history of players being brought into the team Sample Info: run 5km in 16 minutes sprint in 4.7 seconds 10 for 10 on shooting signed a contract with an agency who manages 100's of professional players scheduled 3 tryouts where players have been signed onto the first team every year Integration: If I attend a special training camp and give some professional coaches tapes of me playing in different positions I will receive suggestions on what is my best position (and possibly tips on how to improve at that position). Based on total \"dummy\" information + extra edge from sub goal (networking + expert advice)... YES, very high probability of success. Plan for a Successful Win CE Program Whew, let's get back to some programming! Goal: to put text on the screen Flow: WinMain -> get the text -> draw the screen -> draw the text Modular: drawtext function char to wchar function Tested: program ran with just quit button drawing text to the screen direct from Main using a wchar L\"string\" constant string drawing text to the screen from Main using a wchar[] array populated by a wsprintf moving the above to a function and calling it from main passing a char string to the conversion function and printing the resulting wchar string the \"dummy\" info was the use of constant wchar L\"string\" but I also printed the sizeof and strlen and wcstrlen numbers I defined some sub functions so that I could use the char string functions instead of constantly referring to the Windows functions... I'd hoped it would be more portable but that's something I discuss at the end... Anyways, Win CE code for writing text to the display /* 2010-01 john pfeiffer writing text to the display */ #define WIN32_LEAN_AND_MEAN #include <windows.h> #include <windowsx.h> #include <commctrl.h> #include <aygshell.h> #define IDC_ExitButton 40099 /* wchar[] must be cleared empty first! */ void stringToWchar ( char string [ 128 ], wchar_t longstring [ 128 ] ) { int i = 0 ; for ( i = 0 ; i < strlen ( string ); i ++ ) { longstring [ i ] = ( WCHAR ) string [ i ]; } } /* convert a char string to wchar and Display it on the screen */ /* here we take the handle to device context (aka logical buffer about the screen and begin painting it - we then draw a single line of text (windows wide character format but first converting the character string to wchar string)... The end paint matches the begin paint and without them the text will flicker constantly */ VOID APIENTRY drawText ( HWND hwnd , char text [ 128 ], int x , int y ) { HDC hdc ; PAINTSTRUCT ps ; wchar_t outputtext [ 128 ]; hdc = BeginPaint ( hwnd , & ps ); hdc = GetDC ( hwnd ); /* good practice to zero things before using them */ memset ( outputtext , 0 , sizeof ( outputtext )); stringToWchar ( text , outputtext ); ExtTextOut ( hdc , x , y , NULL , NULL , outputtext , _tcslen ( outputtext ), NULL ); ReleaseDC ( hwnd , hdc ); EndPaint ( hwnd , & ps ); } VOID APIENTRY initializeBackground ( HWND hwnd ) { HDC hdc ; PAINTSTRUCT ps ; hdc = BeginPaint ( hwnd , & ps ); EndPaint ( hwnd , & ps ); } /* our big message loop with all sorts of interrupt options */ LRESULT CALLBACK MenuWndProc ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ) { switch ( message ) { case WM_LBUTTONDOWN : break ; case WM_CHAR : break ; case WM_KEYDOWN : break ; case WM_COMMAND : switch ( LOWORD ( wParam )) { case IDC_ExitButton : PostQuitMessage ( 0 ); break ; default : break ; } break ; case WM_DESTROY : PostQuitMessage ( 0 ); break ; case WM_PAINT : drawText ( hwnd , \"press quit button to quit\" , 40 , 40 ); break ; case WM_CREATE : initializeBackground ( hwnd ); break ; default : return DefWindowProc ( hwnd , message , wParam , lParam ); break ; } return 0 ; } /* end function MenuWndProc */ int APIENTRY WinMain ( HINSTANCE hInstance , HINSTANCE hPrevInstance , LPTSTR lpCmdLine , int ncmdshow ) { HWND hwnd = NULL ; HWND ExitButton = NULL ; MSG msg ; WNDCLASS wc ; wc . style = CS_HREDRAW | CS_VREDRAW ; wc . lpfnWndProc = ( WNDPROC ) MenuWndProc ; wc . cbClsExtra = 0 ; wc . cbWndExtra = 0 ; wc . hInstance = hInstance ; wc . hIcon = 0 ; wc . hCursor = 0 ; wc . hbrBackground = ( HBRUSH ) GetStockObject ( WHITE_BRUSH ); wc . lpszMenuName = NULL ; wc . lpszClassName = ( LPTSTR ) L\"App\" ; if ( ! RegisterClass ( & wc )) { MessageBox ( NULL , TEXT ( \"errors \" ), L\"IMPORTANT\" , MB_OK ); return 0 ; } /* Make sure the window uses the Menu App Class name defined above! */ hwnd = CreateWindow ( L\"App\" , L\"menu demo\" , WS_VISIBLE , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , ( HWND ) NULL , NULL , hInstance , ( LPSTR ) NULL ); /* -------- -------- -------- -------- */ ExitButton = CreateWindow ( L\"BUTTON\" , L\"quit\" , WS_CHILD | WS_VISIBLE | BS_PUSHBUTTON , 0 , 0 , 30 , 30 , hwnd , ( HMENU ) IDC_ExitButton , hInstance , NULL ); ShowWindow ( hwnd , ncmdshow ); UpdateWindow ( hwnd ); while ( GetMessage ( & msg , NULL , 0 , 0 )) { TranslateMessage ( & msg ); DispatchMessage ( & msg ); } return msg . wParam ; } /* end WinMain */ Convert to a Desktop Application As always I try to give as much info as possible, therefore to convert this to a Windows Desktop application you must change the following: Line 32: _tcslen( outputtext ), NULL); changed to: wcslen( outputtext ), NULL); All of the explicit conversions to \"Long\" that are necessary for Win CE (16 bit) have to be removed... which just means getting rid of those pesky 'L' s wc.lpszClassName = (LPTSTR) L\"App\"; For this to really work in Windows Desktop you'd have to replace my custom char to wchar string conversion with Microsoft's MultiByteToWideChar OR... you would probably prefer using wsprintf() to write any text to a wchar_t string and then ExtTextOut is very happy...","tags":"programming","title":"Windows CE Programming - writing text to the display: 365 programming project day forty one"},{"url":"https://blog.john-pfeiffer.com/drupal-in-2d-pictures-of-pixels/","text":"I've finally got a Drupal WYSIWYG editor (FCK Editor) working with Image Assist? Or just plain Upload? Anyways, here's a kitty for my Kitty! ![cyber-kitty][]","tags":"programming","title":"Drupal in 2D - pictures of pixels"},{"url":"https://blog.john-pfeiffer.com/windows-ce-programming-a-win32-api-button-365-programming-project-day-forty/","text":"Win CE uses a subset of the win32 API which is neat because technically anything written for it can be compiled for a \"full\" windows as well. Even though the Win32 API is very outdated I prefer the concept of building blocks and getting your hands dirty with implementation - that's how you really learn how things work. C# and .Net, especially with a super GUI IDE, make it easy to quickly build something but also obscure why performance might be slow, why different parts aren't integrating together, and really require you to build based on the vision of the platform designers - for better or for worse. This is probably why many programs written in C that are speed critical have important parts hand written in assembly. So here's a Windows API button for Win CE... Don't forget the batch file for pocket gcc: \\pgcc\\cc1plus \\pgcc\\source-code.txt -o \\pgcc\\cwms.s -I \\pgcc\\include -include \\pgcc\\fixincl.h -fms-extensions \\pgcc\\as \\pgcc\\cwms.s -o \\pgcc\\cwmo.o \\pgcc\\ld \\pgcc\\cwmo.o -o \\pgcc\\cwme.exe -L \\pgcc\\lib -l cpplib -l corelibc -l coredll -l aygshell -l runtime -l portlib /* 2010-03-06 john pfeiffer wince quit button */ #define WIN32_LEAN_AND_MEAN #include <windows.h> /* these three are included for pocketgcc compatibility */ #include <windowsx.h> #include <commctrl.h> #include <aygshell.h> /* not only do we have those complex included Windows headers but we have to define a special numeric ID for our buttons */ #define IDC_ExitButton 40099 #define IDC_clearScreen 40098 /* HDC = handle to device context - a logical buffer of the screen. You \"write\" things to it and then it can write to the display in a chunk. A paint structure contains the info about the area being painted. Direct from MSDN... typedef struct tagPAINTSTRUCT { HDC hdc; BOOL fErase; RECT rcPaint; BOOL fRestore; BOOL fIncUpdate; BYTE rgbReserved[32]; } PAINTSTRUCT, *PPAINTSTRUCT; */ VOID APIENTRY initializeBackground ( HWND hwnd ) { HDC hdc ; PAINTSTRUCT ps ; hdc = BeginPaint ( hwnd , & ps ); /* prepares the window for painting */ EndPaint ( hwnd , & ps ); /* done painting */ } /* This is the prototype to the \"do everything\" window message processing switch function */ LRESULT CALLBACK MenuWndProc ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ); int APIENTRY WinMain ( HINSTANCE hInstance , HINSTANCE hPrevInstance , LPTSTR lpCmdLine , int ncmdshow ) { HWND hwnd = NULL ; HWND ExitButton = NULL ; /* each button gets a window handle */ HWND clearScreenButton = NULL ; MSG msg ; WNDCLASS wc ; RECT rc ; wc . style = CS_HREDRAW | CS_VREDRAW ; wc . lpfnWndProc = ( WNDPROC ) MenuWndProc ; wc . cbClsExtra = 0 ; wc . cbWndExtra = 0 ; wc . hInstance = hInstance ; wc . hIcon = 0 ; wc . hCursor = 0 ; wc . hbrBackground = ( HBRUSH ) GetStockObject ( WHITE_BRUSH ); wc . lpszMenuName = NULL ; wc . lpszClassName = ( LPTSTR ) L\"Menu App\" ; if ( ! RegisterClass ( & wc )) { MessageBox ( NULL , TEXT ( \"errors \" ), L\"IMPORTANT\" , MB_OK ); return 0 ; } /* create our main window letting windows decide the placement & size */ hwnd = CreateWindow ( L\"Menu App\" , L\"quit button app\" , WS_VISIBLE , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , ( HWND ) NULL , NULL , hInstance , ( LPSTR ) NULL ); /* ----------- ----------- ----------- ----------- */ /* here we get the coordinate dimensions of the main window */ GetWindowRect ( hwnd , & rc ); /* this makes a quit button at the bottom of the screen */ ExitButton = CreateWindow ( L\"BUTTON\" , L\"Quit\" , WS_CHILD | WS_VISIBLE | BS_PUSHBUTTON , 0 , ( rc . bottom - ( rc . top + ( rc . right / 6 ) )), /* button top left corner x,y */ rc . right / 6 , rc . right / 6 , /* width & height */ hwnd , ( HMENU ) IDC_ExitButton , hInstance , NULL ); clearScreenButton = CreateWindow ( L\"BUTTON\" , L\"Clear\" , WS_CHILD | WS_VISIBLE | BS_PUSHBUTTON , /* button top left corner x,y */ 50 , ( rc . bottom - ( rc . top + ( rc . right / 6 ) )), /* width & height */ rc . right / 6 , rc . right / 6 , hwnd , ( HMENU ) IDC_clearScreen , hInstance , NULL ); ShowWindow ( hwnd , ncmdshow ); UpdateWindow ( hwnd ); while ( GetMessage ( & msg , NULL , 0 , 0 )) { TranslateMessage ( & msg ); DispatchMessage ( & msg ); } return msg . wParam ; } /* end WinMain */ /* ----------- ----------- ----------- ----------- */ LRESULT CALLBACK MenuWndProc ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ) { /* here we test for what events happened/the user might have done */ switch ( message ) { case WM_CREATE : initializeBackground ( hwnd ); /* draw the main window */ break ; case WM_DESTROY : PostQuitMessage ( 0 ); break ; case WM_LBUTTONDOWN : /* left button pressed / tap on screen */ break ; case WM_MOUSEMOVE : /* mouse pointer is moving */ /* if(wParam & MK_LBUTTON)*/ break ; case WM_COMMAND : switch ( LOWORD ( wParam )) { case IDC_ExitButton : /* quit button pressed */ PostQuitMessage ( 0 ); break ; case IDC_clearScreen : /* clear button pressed */ InvalidateRect ( hwnd , NULL , TRUE ); break ; /* wipe the main window */ default : break ; } /* end case command */ break ; case WM_PAINT : initializeBackground ( hwnd ); break ; default : return DefWindowProc ( hwnd , message , wParam , lParam ); break ; } /* end case message */ return 0 ; } As you can see a button is just a predefined \"window\" object... but hopefully I'll figure out how to design my own button infrastructure so that I better understand the challenges, have increased portability, and maybe even enhanced functionality! ... Further research when compiling using GCC on Windows XP gave warnings about the \"long\" formatting of some text, e.g. the \"L\" MessageBox(NULL, TEXT(\"errors \"), L\"IMPORTANT\", MB_OK); If you don't remove the \"L\" designation then your buttons won't appear in your binary compiled for Windows XP... clearScreenButton = CreateWindow(L\"BUTTON\", L\"Clear\", WS_CHILD Should be: clearScreenButton = CreateWindow(\"BUTTON\", \"Clear\", WS_CHILD If you forget to remove the L from wc.lpszClassName = (LPTSTR) L\"AppClass\"; Then your program will have \"undefined behavior\" =)","tags":"programming","title":"Windows CE programming: A win32 api button: 365 programming project day forty"},{"url":"https://blog.john-pfeiffer.com/how-to-customize-a-drupal-zen-theme-primary-links-into-horizontal-365-programming-project-day-thirty-nine/","text":"I'm squeezing my brain to get Drupal theme customization and CSS working as quickly as possible (while working a more than a full time job and doing something besides computers every once in a while)... \"Creating\" a Zen sub theme is relatively easy, mostly a lot of copying, replacing STARTERKIT, and uploading again to a different directory... Web Admin configs But to make significant changes in the layout you have to modify layout.css Before that! FIRST MODIFY THE SETTINGS THAT ARE GIVEN VIA THE WEB ADMIN e.g. Theme configuration http://example.com/admin/build/themes/settings/ Here you can choose whether to display the \"Theme\" Primary Links (appears just below the header section)... (I think it's best to uncheck it and use Blocks instead...) http://example.com/admin/build/block Block layouts is where you can put the Primary links up at the top and Secondary links at the bottom. Firefox Firebug Plugin Allows you to \"peek\" at the html and css source and figure out exactly what code controls what... http://getfirebug.com Tools -> AddOns -> disable when you're not working on a website CSS Modifying layout.css LOOK AT THE SOURCE CODE FROM YOUR UNMODIFIED SITE (with Firebug too...) e.g. line 37 of the home page WE FIND \"menu-primary\" in zen.css To \"override\" the default zen.css all we have to do is create our own version in layout.css #block-menu-primary-links /* \"Primary links\" block */ { font-size : 16px ; } To take advantage of the \"nested\" control principle, so any links in div \"block-menu-primary-links\" will now be red, we add the following to layout.css (with comments explaining it too, of course!) #block-menu-primary-links a :link { color : green ; } #block-menu-primary-links a :visited { color : red ; } TRY UPLOADING layout.css and forcing your browser to refresh... neat! Horizontal menu in layout.css SOME ADDITIONAL EXAMPLES (finally the horizontal menu!) /* \"Primary links\" block */ #block-menu-primary-links { margin : 0 ; padding : 0 ; } /* this means by default ordered and unordered lists and anchors have no underlines */ #block-menu-primary-links li ul , #block-menu-primary-links li , #block-menu-primary-links a { text-decoration : none ; } /* specifically \"anchors\" or links will appear as blocks */ #block-menu-primary-links a { display : block ; } #block-menu-primary-links a :links { color : #008000 ; } #block-menu-primary-links a :visited { color : #CCBA22 ; } Unfortunately haven't figured out why the drupal menu width, when set, makes them appear to go down instead of widening to the right... material for a future post I suppose... Scratch that, 30 frantic minutes later, a solution has arrived... Horizontal menu in layout.css with the width workaround /* FORCING THEM TO BE HORIZONTAL? 2010-02-22 JOHNPFEIFFER */ #block-menu-primary-links /* overriding the zen.css \"Primary links\" block */ { margin : 0 ; /* remove any previously defined margins or padding */ padding : 0 ; } /* this means by default ordered and unordered lists and anchors have no underlines */ #block-menu-primary-links li ul , #block-menu-primary-links li , #block-menu-primary-links a { text-decoration : none ; /* no underlining links */ list-style-type : none ; /* no bullet points */ list-style-image : none ; /* no custom bullet images */ text-align : center ; /* I prefer my text to be neat and centered */ } /* specifically \"anchors\" or links will horizontal */ #block-menu-primary-links a { display : block ; /*inline; /* blocks force a newline but inline just uses a little width */ width : 70px ; /* wide enough for the longest element, unless you want words on two lines */ } #block-menu-primary-links a :links { color : #008000 ; } /* light blue */ #block-menu-primary-links a :visited { color : #CCBA22 ; } /* light brown */ #block-menu-primary-links li { float : left ; position : relative ; } How to create a zen subtheme (part one) Note - you should only create custom themes in a test environment, leave your production website alone until you've got everything working! Download the newest version: http://drupal.org/project/zen Extract the files into a directory (e.g. tar -xzvf zen-6.x-1.1.tar.gz or winzip and extract) Upload the files using FTP (or better yet, SFTP) into the /drupal-root/sites/all/themes Upload the STARTERKIT directory that's inside your zen-6.x-1.1 folder from your pc into the /drupal-root/sites/all/themes Rename both the pc version and online STARTERKIT directory to your_subtheme_name (lowercase and underscores only) Inside both the pc version and the online STARTERKIT directory, rename the STARTERKIT.info.txt to your_subtheme_name.info On your pc, open the your_subtheme_name.info file and find and replace every \"STARTERKIT\" with \"your_subtheme_name\" inside your_subtheme_name.info find the following lines: ; The name and description of the theme used on the admin/build/themes page. name = Zen Sub-theme Starter Kit Replace the \"zen Sub-theme Starter Kit\" with your_subtheme_name Repeat step 7 for the \"template.php\" file Repeate step 7 for the \"theme-settings.php\" file Upload all 3 modified files (overwrite) to your /drupal-root/sites/all/themes/your_subtheme_name directory Using your FTP download the following files from: /drupal-root/sites/all/themes/zen-6.x-1.1/zen/zen html-elements.css, layout-fixed.css, layout-liquid.css, print.css, zen.css Upload all of the above files into your directory: /drupal-root/sites/all/themes/your_subtheme_name 12a. If you want a fixed (ie 1024px) css layout then rename layout-fixed.css to layout.css 12b. If you want a resizable css layout then rename layout-liquid.css to layout.css As the Drupal administrator login to your site and enable your new sub theme: Administer -> Site building -> Themes http://example.com/admin/build/themes/select you'll probably want to set it as default Apparently Drupal 6 is smart enough that when you click save: For easier theme development, the theme registry is being rebuilt on every page request. It is extremely important to turn off this feature on production websites. More info You may have to: run cron manually (or you should be) so that the new options appear on the Drupal Menus. Reload after you save the new theme as enabled and default","tags":"programming","title":"How to customize a Drupal Zen theme Primary Links into Horizontal: 365 programming project day thirty nine"},{"url":"https://blog.john-pfeiffer.com/a-better-css-3-column-header-footer-layout-365-programming-project-day-thirty-eight/","text":"It's messy as some of the code could be removed but it gets you most of the way there - a footer at the bottom (even if the content doesn't fill the page)... a header at the top that's full width, and 3 columns... with a little weird bug on the right column... <!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\"> < html xmlns = \"http://www.w3.org/1999/xhtml\" lang = \"en\" xml:lang = \"en\" > < head > < style type = \"text/css\" > /* required to kill off any extra \"helpful\" browser padding */ html , body { margin : 0 ; padding : 0 ; height : 99% ; } #header { height : 60px ; overflow : hidden ; z-index : 0 ; border : 1px solid purple ; text-align : center ; } #container1 { min-height : 100% ; height : auto ; height : 100% ; overflow : hidden ; margin : 0 ; position : relative ; border : 1px solid green ; } #container2 { border : 1px solid green ; position : relative ; } #container3 { position : relative ; } #column1 { float : left ; width : 20% ; overflow : hidden ; border : 1px solid red ; position : relative ; } #column2 { float : none ; width : 60% ; overflow : hidden ; border : 1px solid blue ; position : relative ; } #column3 { float : right ; width : 20% ; overflow : hidden ; border : 1px solid blue ; position : relative ; } #footer { height : 60px ; overflow : hidden ; z-index : 0 ; border : 1px solid purple ; text-align : center ; bottom : 0 ; width : 99% ; position : absolute ; } </ style > </ head > < body > < div id = \"container1\" > < div id = \"header\" > header </ div > < div id = \"column1\" > left left left left left left left left left left left left </ div > < div id = \"column2\" > standard 3 column layout with header and footer content and columns are ordered by SEO priority </ div > < div id = \"column3\" > right right right right right right right right right < img src = \"image.png\" > </ div > </ div > < div id = \"footer\" > footer </ div > <!--div id=\"container1\"> <div id=\"container2\"> <div id=\"c3\"> </div> </div--> </ body > </ html >","tags":"programming","title":"A better CSS 3 column header footer layout: 365 programming project day thirty eight"},{"url":"https://blog.john-pfeiffer.com/how-to-center-with-css-365-programming-project-day-thirty-seven/","text":"This is a tiny post, but it covers a very important part of how a CSS layout might look: how do you center something? The \"Centering CSS Blocks Trick\" forces the text (or image too, hopefully, depending on a post 2006 browser)... CSS center (horizontal) CSS #footer { text-align : center ; bottom : 0px ; margin-left : auto ; margin-right : auto ; width : 30% ; z-index : 0 ; position : relative ; } HTML < div id = \"footer\" > footer text goes here </ div > Note that the text-align instruction is perhaps redundant but it's better to be safe (and more universally compatible). CSS centering vertically To VERTICALLY CENTER things takes a bit of creativity, basically you must pretend that an \"inner div\" is actually a table cell and use the \"new\" property of vertical-align (alot like html table cell valign)... < html >< head > < style type = \"text/css\" > #contentsContainer { /* height must be a fixed number */ height : 200px ; width : 100% ; border : 1px solid green ; text-align : center ; display : table-cell ; vertical-align : middle ; position : relative ; } #contents { border : 1px solid blue ; /* height must be a fixed number */ height : 40px ; position : relative ; } </ style > </ head >< body > < div id = \"contentsContainer\" > CONTENTS Lots of contents Vertically and horizontally centered CONTENTS < div id = \"contents\" > test more content (this is horizontally centered too) </ div > </ div > </ body ></ html > Inline CSS centered < div style = \"text-align: center; width: 100%; margin-left: auto; margin-right: auto;\" > < img alt = \"Logo\" src = \"/img/icons/logo_256.png\" > < h2 > Welcome </ h2 > </ div > Whew, back to work!","tags":"programming","title":"How to center with CSS: 365 programming project day thirty seven"},{"url":"https://blog.john-pfeiffer.com/c-programming-windows-clock-v4-failure-is-only-feedback-365-programming-project-day-thirty-six/","text":"It seems I've bitten off more than I thought with regularly updating a Window every second... I need to go back and learn more about how WM_PAINT works in windows because my current version is very funky... though it does work! A couple of obvious other things: WM_TIMER and perhaps strcpy instead of get_current_time again... The reason I've chosen the awkward system of getting the system time over and over instead of the \"convenient\" windows timer is that I'm trying to learn and understand what I can do with programming, not how to copy and paste someone else's function. The \"modular\" aspect of get_current_time returning a string becomes very interesting as theoretically I could modify it to get time from an atomic clock or the internet and the application wouldn't know the difference. Anyways, here's some source code that does compile (but I think it has a very slow memory leak so don't leave it running all night... LOL) /* 2010-02-12 john pfeiffer, MS windows clock v4 (updating time) todo: wm_paint, wm_timer, strcpy instead of get_current? */ #include <stdlib.h> #include <stdio.h> #include <time.h> #include <string.h> #include <windows.h> #define IDC_ExitButton 40001 LRESULT CALLBACK WindowProcedure ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ); void get_current_time ( char current_time [ 128 ] ); int WINAPI WinMain ( HINSTANCE hThisInstance , HINSTANCE hPrevInstance , LPSTR lpszArgument , int ncmdshow ) { char current_time [ 128 ]; char temp_time [ 128 ]; HWND hwnd ; /* The handle for our window */ HWND ButtonPushed = NULL ; MSG messages ; /* Messages to the application */ WNDCLASS wc ; /* Data structure for our defined windowclass */ wc . style = 0 ; wc . lpfnWndProc = WindowProcedure ; /* This function is called by windows */ wc . cbClsExtra = 0 ; /* No extra bytes after the window class */ wc . cbWndExtra = 0 ; /* structure or the window instance */ wc . hInstance = hThisInstance ; /* handle to the owner */ wc . hIcon = NULL ; /* no special application icon */ wc . hCursor = LoadCursor ( NULL , IDC_ARROW ); /* default cursor */ wc . hbrBackground = ( HBRUSH ) GetStockObject ( LTGRAY_BRUSH ); wc . lpszMenuName = NULL ; /* No menu */ wc . lpszClassName = \"WindowsApp\" ; /* the name of the windows class */ /* Register the window class, if fail quit the program with an error value */ if ( RegisterClass ( & wc ) == 0 ) { return - 1 ; } /* The class is registered, let's instantiate our window */ hwnd = CreateWindowEx ( 1 , \"WindowsApp\" , \"Windows Title\" , WS_OVERLAPPEDWINDOW , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , NULL , NULL , hThisInstance , NULL ); /* create button and store the handle */ ButtonPushed = CreateWindow ( \"button\" , /* class name */ \"Push to Quit\" , /* button caption */ WS_CHILD | WS_VISIBLE | BS_PUSHBUTTON , /* the styles */ 0 , 0 , /* the left and top coordinates */ 150 , 50 , /* width and height */ hwnd , /* parent window handle */ ( HMENU ) IDC_ExitButton , /* the ID of your button */ hThisInstance , /* the instance of your application */ NULL ) ; /* unnecessary extra */ ShowWindow ( hwnd , ncmdshow ); /* Make the window visible on the screen */ UpdateWindow ( hwnd ); /* update with changes */ get_current_time ( temp_time ); /* get the current time initially */ /* Run the message loop. It will run until GetMessage( ) returns 0 */ while ( GetMessage ( & messages , NULL , 0 , 0 ) ) { /* nasty polling business, should be done with WM_TIMER */ /* if strings aren't the same then update the window */ get_current_time ( current_time ); if ( strcmp ( current_time , temp_time ) ) { /* debugging - am I getting the time comparison? */ MessageBox ( hwnd , current_time , temp_time , 0 ); /* theoretically the rest of this forces the window to refresh */ UpdateWindow ( hwnd ); ShowWindow ( hwnd , ncmdshow ); /* update the new \"old time\" */ get_current_time ( temp_time ); } TranslateMessage ( & messages ); DispatchMessage ( & messages ); } /* The program return-value is 0 - The value that PostQuitMessage( ) gave */ return messages . wParam ; } /* This function is called by the Windows function DispatchMessage( ) */ LRESULT CALLBACK WindowProcedure ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ) { char current_time [ 128 ]; PAINTSTRUCT ps ; /* a structure for a paint job (see below */ RECT rect ; /* a struct to hold rectangle values (e.g. x,y coordinates) */ HDC hdc ; /* handle to a DC (buffer) for the screen */ switch ( message ) /* handle the messages */ { case WM_COMMAND : switch ( LOWORD ( wParam )) /* find out what's been clicked */ { case IDC_ExitButton : PostQuitMessage ( 0 ); break ; default : break ; } break ; case WM_PAINT : GetClientRect ( hwnd , & rect ); /* get the size of our window */ InvalidateRect ( hwnd , NULL , TRUE ); hdc = BeginPaint ( hwnd , & ps ); /* begin painting to the buffer */ get_current_time ( current_time ); DrawText ( hdc , TEXT ( current_time ), - 1 , & rect , DT_CENTER | DT_VCENTER | DT_SINGLELINE ); EndPaint ( hwnd , & ps ); return 0 ; case WM_DESTROY : PostQuitMessage ( 0 ); break ; default : /* for messages that we don't deal with */ return DefWindowProc ( hwnd , message , wParam , lParam ); } return 0 ; } /* end of WinMain */ /* Get the current time from the system and update the time string */ void get_current_time ( char current_time [ 128 ] ) { time_t tempTime ; /* initialize the variable, otherwise only returns 1970 date */ tempTime = time ( NULL ); strcpy ( current_time , asctime ( localtime ( & tempTime ))); }","tags":"programming","title":"C programming windows clock v4 (failure is only feedback): 365 programming project day thirty six"},{"url":"https://blog.john-pfeiffer.com/drupal-basic-site-configuration-the-search-block/","text":"A basic Drupal theme has \"regions\" where you can put things... header left side right side content footer Blocks are the \"extra parts\" that can appear in addition to your \"node\" (stuff in the Content only) While it is easy to get advanced functionality with a few clicks, it can be a chore to remember the order and location of those clicks. Enable Site Search Administer -> Site Building -> Modules -> List Click the checkbox next to SEARCH to enable it. Hit the SAVE button ( way down at the bottom ). Site Search Permissions Now you have to give Anonymous users (or just Authenticated Users, or maybe your custom category of LOLcatz?) permission to actually see/use the Search. Administer -> User Management -> Permissions Click the check boxes next to Search Module for the Anonymous and Authenticated User columns (note you can also give access to Advanced Search... cool.) Hit the SAVE button ( way down at the bottom ). NOTE that the \"administrator/root\" user isn't a column (and therefore you can't remove your own permission to administer the site... not that anyone'd ever be so silly...) For Garland, specifically, when you add the Search it will make a \"double\" so you have to disable the one that's built into the theme... Administer -> Site Building -> Themes -> Configure -> Global Settings Whew! Almost there... Adding a Site Search Block Administer -> Site Building -> Blocks -> List From here you can either drag and drop (or use the \"dropdown\" next to the Block name) to get the Search into the region of the page you want. Hit the SAVE button (way down at the bottom) before doing anything else. (Very frequently I forget to press save and all of my changes don't get saved... must be a bug.) THEN click on \"Configure\" next to the Search Block to specify some details. (e.g. Search only appears on the page, or the title above the Search Block will be FIND, or only show the Search Block to certain user roles...) OK, hit your SAVE button one last time... not only can you, super root administrator see Search but hopefully Anonymous users can now find your secret buried treasure... Are you ready for URL's and page titles?","tags":"programming","title":"Drupal Basic Site Configuration: the Search Block"},{"url":"https://blog.john-pfeiffer.com/c-programming-windows-clock-v3-365-programming-project-day-thirty-five/","text":"My windows clock project is slowly progressing... Now I can create a window with a QUIT button and display the current system time (only once)... If you want to know more about the project from the beginning search the site for winclock (or click on the time tag). I've stripped out most of the project header stuff - perhaps in the future I'll just have it as an \"include\" so that it's in the project but since it doesn't change much I only have to look at it when I refer back to it (instead of at the top of the file ALL THE TIME!) Ok, enough planning and review, here's the code: notepad gcwin.bat gcc.exe -o %1.exe %1 -Iinclude -Llib -Wall -ansi -mwindows winclock-v3.c /* 2010-02-08 john pfeiffer, MS windows clock v3 (the windows) */ #include <time.h> #include <string.h> #include <windows.h> #define IDC_MyExitButton 40001 /* case sensitive! random high number to keep windows happy */ LRESULT CALLBACK WindowProcedure ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ); void get_current_time ( char current_time [ 128 ] ); int WINAPI WinMain ( HINSTANCE hThisInstance , HINSTANCE hPrevInstance , LPSTR lpszArgument , int ncmdshow ) { HWND hwnd ; /* The handle for our window */ HWND ButtonPushed = NULL ; MSG messages ; /* Messages to the application */ WNDCLASS wc ; /* Data structure for our defined windowclass */ wc . style = 0 ; wc . lpfnWndProc = WindowProcedure ; /* This function is called by windows */ wc . cbClsExtra = 0 ; /* No extra bytes after the window class */ wc . cbWndExtra = 0 ; /* structure or the window instance */ wc . hInstance = hThisInstance ; /* handle to the owner */ wc . hIcon = NULL ; /* no special application icon */ wc . hCursor = LoadCursor ( NULL , IDC_ARROW ); /* default cursor */ wc . hbrBackground = ( HBRUSH ) GetStockObject ( LTGRAY_BRUSH ); wc . lpszMenuName = NULL ; /* No menu */ wc . lpszClassName = \"WindowsApp\" ; /* the name of the windows class */ /* Register the window class, if fail quit the program with an error value */ if ( RegisterClass ( & wc ) == 0 ) { return - 1 ; } /* The class is registered, let's instantiate our window */ hwnd = CreateWindowEx ( 1 , \"WindowsApp\" , \"Windows Title\" , WS_OVERLAPPEDWINDOW , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , NULL , NULL , hThisInstance , NULL ); /* create button and store the handle */ ButtonPushed = CreateWindow ( \"button\" , /* class name */ \"Push to Quit\" , /* button caption */ WS_CHILD | WS_VISIBLE | BS_PUSHBUTTON , /* the styles */ 0 , 0 , /* the left and top coordinates */ 150 , 50 , /* width and height */ hwnd , /* parent window handle */ ( HMENU ) IDC_MyExitButton , /* the ID of your button */ hThisInstance , /* the instance of your application */ NULL ) ; /* unnecessary extra */ ShowWindow ( hwnd , ncmdshow ); /* Make the window visible on the screen */ UpdateWindow ( hwnd ); /* update with changes */ /* Run the message loop. It will run until GetMessage( ) returns 0 */ while ( GetMessage ( & messages , NULL , 0 , 0 ) ) { TranslateMessage ( & messages ); DispatchMessage ( & messages ); } /* The program return-value is 0 - The value that PostQuitMessage( ) gave */ return messages . wParam ; } /* This function is called by the Windows function DispatchMessage( ) */ LRESULT CALLBACK WindowProcedure ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ) { char current_time [ 128 ]; PAINTSTRUCT ps ; /* a structure for a paint job (see below */ RECT rect ; /* a struct to hold rectangle values (e.g. x,y coordinates) */ HDC hdc ; /* handle to a DC (buffer) for the screen */ switch ( message ) /* handle the messages */ { case WM_COMMAND : switch ( LOWORD ( wParam )) /* find out what's been clicked */ { case IDC_MyExitButton : PostQuitMessage ( 0 ); break ; default : break ; } break ; case WM_PAINT : GetClientRect ( hwnd , & rect ); /* get the size of our window */ hdc = BeginPaint ( hwnd , & ps ); /* begin painting to the buffer */ get_current_time ( current_time ); DrawText ( hdc , TEXT ( current_time ), - 1 , & rect , DT_CENTER | DT_VCENTER | DT_SINGLELINE ); EndPaint ( hwnd , & ps ); return 0 ; case WM_DESTROY : PostQuitMessage ( 0 ); /* send a WM_QUIT */ break ; default : /* for messages that we don't deal with */ return DefWindowProc ( hwnd , message , wParam , lParam ); } return 0 ; } /* Get the current time from the system and update the time string */ void get_current_time ( char current_time [ 128 ] ) { time_t tempTime ; /* initialize the variable, otherwise only returns 1970 date */ tempTime = time ( NULL ); strcpy ( current_time , asctime ( localtime ( & tempTime ))); }","tags":"programming","title":"C programming windows clock v3: 365 programming project day thirty five"},{"url":"https://blog.john-pfeiffer.com/customize-your-linux-bash-console-bashrc-aliases-colors-hotkeys-history-length/","text":"The hidden file .bashrc in each user's home directory (~ or or /home/username or /root) controls the configuration of how the console (and certain commands) behave. NOTE CENTOS/REDHAT also uses .bash_profile The shell is the program that interfaces between the user and the Linux kernel. There are different shells with different features. Ash is a reimplementation of the System V shell May 6, 1989 GNU Bourne Again Shell Bash is an sh-compatible command language interpreter that executes commands read from the standard input or from a file. Bash also incorporates useful features from the Korn and C shells (ksh and csh) June 18, 1999 Zsh is a command interpreter which mostly resembles the Korn shell. Includes a command-line editor and many other enhancements over the other shells. Favorite Aliases Here are my favorite aliases that are in my .bashrc: command explanation alias ls='ls --color=auto' color highlighting of a directory listing alias ll='ls -ahlF --color=auto' directory listing: all, hidden too, long format, show type with symbols: dir/ alias la='ls -A' directory listing almost all so exclude . and .. alias l='ls -CF' list in columns with trailing type symbols: dir/ alias free='free -m' free RAM in megabytes alias df='df -h' disk free in human sizes alias nano='nano -c -S -u' simple editor with cursor position, smooth scroll, and undo alias gp='git pull' pull remote changes with less typing alias gt='git status' current git status with less typing alias gw='git whatchanged' git history with less typing alias gd=\"GIT_PAGER='' git diff\" git history with less typing alias rm='rm -i' extra prompt before deleting a file or directory alias cp='cp -i' extra prompt before overwriting a file or directory with a copy alias mv='mv -i' extra prompt before overwriting a file or directory with a move export PATH=$PATH:~/bin your own local scripts are loaded in the console set bell-style none no more annoying bash beeps! xset -b no more annoying bash beeps for x windows alias ssh='ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no' ssh will not check nor store remote server signatures: insecure! unalias free or \\free NOTE to escape or override the alias To enable the new aliases immediately go to the directory type one of the following: . ./bashrc or source ~/.bashrc nano /home/username/.bashrc or /root/.bashrc export PATH=$PATH:~/bin .bashrc or .profile are where your shell (BASH) gets it's initial settings: root:~# ls -al total 20 drwxr-xr-x 2 root root 4096 Mar 29 21:56 . drwxr-xr-x 21 root root 4096 Mar 29 22:26 .. -rw------- 1 root root 183 Mar 29 22:31 .bash_history -rw-r--r-- 1 root root 2225 Mar 29 22:31 .bashrc -rw-r--r-- 1 root root 141 May 15 2007 .profile if a lot of commands start failing try a cat /etc/passwd and see if you are using /sbin/bash or /bin/sh - the older and less functional shell command explanation whoami display what user is logged in or is being impersonated by su echo $HOME display the current user's home directory (which should match what's in /etc/passwd) echo $PATH list what binary executable directories are accessible by default normal centos user has /usr/local/bin:/bin:/usr/bin:/home/username/bin changing to root with: su - /usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin NOTE that if you simply use \"su\" you will only get the normal user path... increasing bash history length .bash_profile allows you to customize your command history size... (sometimes it's a section in .bashrc) if it doesn't exist (e.g. ubuntu can't find it \".bash_profile doesn't exist\"), create it and make it executable and then add the following lines... echo \"#defaults are 500\" >> .bash_profile echo \"HISTFILESIZE=10000\" >> .bash_profile echo \"HISTSIZE=10000\" >> .bash_profile chmod 700 ~/.bash_profile HISTFILE is where the history saves to (/dev/null = no history file), the default is: ~/.bash_history. HISTFILESIZE how many commands to keep in HISTFILE (default 500) HISTSIZE how many commands to keep in the current session (default 500) HISTIGNORE Controls which commands to ignorea nd not save. The variable takes a list of colon separated patterns. Pattern & matches the previous history command. cut -f1 -d\" \" .bash_history | sort | uniq -c | sort -nr | head -n 30 what command you've typed the most cut -f1 -d\" \" /root/.bash_history | sort | uniq -c | sort -nr | head -n 30 what command root has typed the most cut -f1 /root/.bash_history | sort | uniq -c | sort -nr | head -n 30 what command + parameters root has typed the most Example default .bash_profile # Get the aliases and functions if [ -f ~/.bashrc ]; then . ~/.bashrc fi # User specific environment and startup programs PATH= $ PATH : $ HOME /bin export PATH # note this last command only exists in the /root/.bash_profile unset USERNAME nano .bashrc uncomment (remove the leading # from the two lines with --color=auto) # enable color support of ls and also add handy aliases if [ \" $TERM \" != \"dumb\" ]; then eval \"`dircolors -b`\" alias ls='ls --color=auto' #alias dir='ls --color=auto --format=vertical' #alias vdir='ls --color=auto --format=long' fi # ALSO, uncomment the following line to get a color prompt: # Comment in the above and uncomment this below for a color prompt #PS1=' ${ debian_chroot : + ( $ debian_chroot ) } \\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[$ And comment out the two lines above it... hotkeys command explanation ctrl-r to search through your command history in reverse (newest to oldest) ctrl-k to clear all the text after cursor ctrl-u to clear all the text before cursor ctrl-a move to beginning of the current ctrl-e move to end of the current ctrl-f move forward one char ctrl-b move backward one word Example recycle bin for root Now you can do fun things like have a \"recycle bin\" for root: mkdir /recycle-bin echo \"mv $1 /recycle-bin\" > del.sh chmod 700 del.sh AND THEN VERIFY WITH: touch test.txt ./del.sh test.txt ls -al /recycle-bin bashrc - xterm - ANSI escape color codes .bashrc alias xterm1='xterm -fg black -bg white' nano /home/username/.icewm/toolbar prog xterm xterm1 x-terminal-emulator nano /home/username/.icewm/keys key \"Ctrl+Alt+j\" xterm -fg black -bg white FIRST print out what colors are available... dircolors -p > dircolors.txt after reading through that and being thoroughly mystified... echo -e \"\\033[44;37;5m ME \\033[0m COOL\" if you put the above into your bash shell you'll see that the ANSI control of colors is basically what controls BASH/TERMINALS... Attribute codes: 00=none 01=bold 04=underscore 05=blink 07=reverse 08=concealed Text color codes: 30=black 31=red 32=green 33=yellow 34=blue 35=magenta 36=cyan 37=white Background color codes: 40=black 41=red 42=green 43=yellow 44=blue 45=magenta 46=cyan 47=white Black 0;30 Dark Gray 1;30 Blue 0;34 Light Blue 1;34 Green 0;32 Light Green 1;32 Cyan 0;36 Light Cyan 1;36 Red 0;31 Light Red 1;31 Purple 0;35 Light Purple 1;35 Brown 0;33 Yellow 1;33 Light Gray 0;37 White 1;37 echo -e \"\\e[1;34mThis is a blue text.\\e[0m\" so \\e[attribute code; text color code0m framed by the [ --- 0m note that the \\033 in the first example has been replaced by \\e echo -e \"\\e[30;470mtest\" FOREGROUND WHITE (in case you accidentally set it to black on black) echo -e \"\\033[37m\\]\" BACKGROUND WHITE echo -e \"\\033[47m\\]\" FOREGROUND BLACK echo -e \"\\033[1;30m\\]\" changing the foreground and background color of your bash shell (and font?) TEMPORARY SOLUTION xterm -fg *color* -bg *color* PERMENANT SOLUTION .Xdefaults file in your home directory","tags":"linux","title":"Customize your linux bash console: bashrc, aliases, colors, hotkeys, history length"},{"url":"https://blog.john-pfeiffer.com/drupal-themes-customization-intro-365-programming-project-day-thirty-four/","text":"Drupal is meant to be a dynamic website platform (that runs quite well out of the box) yet customizable. Despite the good advice to create a \"sub theme\" by copying and pasting the current files into a new folder i went ahead and hacked at the core php and css... While Garland itself (I'm using the \"configurable sub theme Minelli\" which must be modified through Garland) is very hard to work with (I will probably end up using Zen to get my final customized effect)... FTP into your website hosting and ... /drupal-root/themes/garland Modifying page.tpl.php with my trusty notepad2... Line 66 has the very interesting terms: From there you have to also modify \"style.css\" to the related terms (that's the way css works, to abstract the content & functionality in html/php and the design in css)... FUNNILY ENOUGH, THINGS WORK BACKWARDS... body .sidebars { /* min-width: 980px; */ min-width : 680px ; } So as you can see, I cleverly commented out the original and modified it to be smaller (right?)... but it actually made the sidebars bigger... so I'll try the reverse... but changes don't really appear... Admin -> Flush All Caches is supposed to help make the theme changes appear! After enough fiddling to understand it I've reset style.css to it's default in Garland... Apparently the \"Minnelli\" sub folder minnelli.css overrides the style.css so that's what I need to work on... /drupal-root/themes/garland/minnelli/minnelli.css body #wrapper #container { /* width: 560px; */ width : 960px ; } body .sidebars #wrapper #container { /* width: 980px; */ width : 480px ; } body .sidebar-left #wrapper #container , body .sidebar-right #wrapper #container { /* width: 770px; */ width : 570px ; } Except that once again, the settings above have SHRUNK the body... body .sidebar-left #wrapper #container , body .sidebar-right #wrapper #container { /* width: 770px; */ width : 870px ; } The above is the only bit of code needed to widen the body slightly, all of the rest is the garland/minnelli default. Ironically the changes only appear so far in the Admin User logged on screen (not for anonymous users)... But that's enough for today's post! More info: https://www.drupal.org/theme-guide/6-7 https://www.drupal.org/node/225125","tags":"programming","title":"Drupal Themes customization intro: 365 programming project day thirty four"},{"url":"https://blog.john-pfeiffer.com/javascript-form-validation-lots-of-work-365-programming-project-day-thirty-three/","text":"I'm working too much which is why these posts are more infrequent (though I will keep numbering them sequentially and hopefully one day I'll have enough time to backfill all 365 before the 1 year deadline)... Javascript is meant to easily add functionality to a webpage. Unfortunately some people don't really test it enough, case in point, an email newsletter went out with a link to an online competition page BUT the submit button \"didn't work\"... Here comes John to save the day! From the HTML everything appeared fine: < form name = \"f\" method = \"post\" action = \"send_enquiry_displays.asp\" > < td nowrap > First Name:* </ td > < td >< input class = \"pTextBox1\" name = \"txFirstName\" type = \"text\" value = \"\" ></ td > More form stuff here... Notice that the radio button only has one choice... < input type = \"radio\" name = \"rdDisplaySize\" value = \"5.7 VGA TFT\" ></ td > < td nowrap > Wireless LAN: </ td > < td >< input type = \"checkbox\" name = \"ckWirelessLAN\" value = \"1\" ></ td > < td valign = \"top\" >< input type = \"hidden\" name = \"whereDidYouHear\" value = \"WUNL0309#1\" ></ td > < td >< a href = \"javascript:checkSingupForm()\" onClick = \"this.blur()\" onmouseover = \"genericRollover('elImgSubmit','images/buttons/submit_over.gif'); window.status='Submit Form';return true;\" onmouseout = \"genericRollover('elImgSubmit','images/buttons/submit.gif'); window.status='';return true;\" > < img src = \"images/buttons/submit.gif\" name = \"elImgSubmit\" alt = \"Submit Form\" width = \"66\" height = \"22\" hspace = \"10\" border = \"0\" > </ a ></ td > </ table > </ td > </ tr > </ form > < script language = \"JavaScript\" > function checkSingupForm (){ var f = document . forms [ \"f\" ]; //this array should contain every text field you require to be filled in var arr = [ [ \"txFirstName\" , \"First Name\" ] ,[ \"txLastName\" , \"Last Name\" ] ,[ \"txCompany\" , \"Company\" ] ,[ \"txTown\" , \"Town\" ] ,[ \"txEmail\" , \"E-mail\" ] ]; //for loop checks each value if it is blank \"\" ... then popup alert and changes focus for ( i = 0 ; i < arr . length ; i ++ ){ if ( f . elements [ arr [ i ][ 0 ]]. value == \"\" ){ alert ( \"Please fill in \" + arr [ i ][ 1 ] + \".\" ) f . elements [ arr [ i ][ 0 ]]. focus (); return ; } } //a regular expression check to ensure the email is in a valid email format var emailRE = /&#94;([a-zA-Z0-9_\\.\\-])+\\@(([a-zA-Z0-9_\\-])+\\.)+([a-zA-Z0-9]{2,4})+$/ if ( ! emailRE . exec ( f . elements [ \"txEmail\" ]. value ) ){ alert ( \"Please fill in a valid e-mail address.\" ); f . elements [ \"txEmail\" ]. focus (); return ; } //ensures that the display size have been filled out... rdDisplaySizeValid = - 1 ; for ( i = 0 ; i < f . elements [ \"rdDisplaySize\" ]. length ; i ++ ) { if ( f . elements [ \"rdDisplaySize\" ][ i ]. checked ) { rdDisplaySizeValid = 1 ; } } if ( rdDisplaySizeValid == - 1 ) { alert ( \"Please choose a Display size.\" ); f . elements [ \"rdDisplaySize\" ][ 0 ]. focus (); return ; } //ensures that the embeddedconfiguration field has been checked at least once... if ( ! f . elements [ \"rdEmbeddedConfiguration\" ][ 0 ]. checked && ! f . elements [ \"rdEmbeddedConfiguration\" ][ 1 ]. checked ) { alert ( \"Please choose a Embedded Configuration.\" ); f . elements [ \"rdEmbeddedConfiguration\" ][ 0 ]. focus (); return ; } f . submit (); } //--> </ script > After downloading the ASP file and creating a backup copy... So what was the problem? The incorrect validation of radio button rdDisplaySize had to be commented out. I also added the nifty \"default CHECKED\" option as the form only gave one Radio Button option (but it was a Required field!) <input type=\"radio\" name=\"rdDisplaySize\" value=\"5.7 VGA TFT CHECKED\"></td> Whew, another crisis averted, customers now able to register for the competition and turn themselves into Leads for our company!","tags":"programming","title":"Javascript Form Validation, Lots of work: 365 programming project day thirty three"},{"url":"https://blog.john-pfeiffer.com/replace-windows-xp-sp3-notepad-with-notepad2/","text":"Each XP service pack seems to make the process of replacing notepad.exe with notepad2.exe even more complex (somebody at Microsoft really likes the original notepad)... Updated steps for SP3: Download a replacement text editor: http://www.flos-freeware.ch/notepad2.html (If you have the \\servicepackfiles\\i386 folder...) rename C:\\WINDOWS\\ServicePackFiles\\i386\\notepad.exe C:\\WINDOWS\\ServicePackFiles\\i386\\notepad.exe.bak Now rename notepad2.exe notepad.exe and copy it into: C:\\WINDOWS\\ServicePackFiles\\i386\\ C:\\WINDOWS\\system32\\dllcache C:\\WINDOWS\\system32\\notepad.exe C:\\WINDOWS Check that your new notepad is in place (the filesize change from 68k to 243k)... (In service pack 2 it would complain with 2 popups and you would just hit cancel both times... as \\system32 files were immediately replaced by the \"original\" from dllcache) ADDITIONALLY, the \"file type\" may get messed up so you might have to have notepad2.exe in the C:\\ and when you double click a .txt from windows explorer you'll have to choose Open With other -> Browse -> c:\\notepad2.exe","tags":"it","title":"Replace Windows XP SP3 notepad with notepad2"},{"url":"https://blog.john-pfeiffer.com/debugging-and-accidental-difficulties-with-getchar-and-loops-365-programming-project-day-thirty-two/","text":"So for fun I tried to \"port\" my code (of winclockv2.c) into Linux and compile it with gcc. Remarkably easy since most Linux distributions come with GCC installed (in case you need to build a new application from source code... it sounds scary until you've done it once or twice and then it's easy). Just open up a text editor, paste it in, save it (getchar-loop.c). The only thing to change was my Windows \"batch\" file, touch gc.sh chmod +x gc.sh nano gc.sh #!/bin/bash gcc -o $1 .exe $1 -Wall -ansi ./gc.sh getchar-loop.c So today's entry is an offshoot program I wrote to investigate why my previous version loop control wasn't working correctly. A little googling showed me that this particular \"getchar() buffer problem\" is a classic... /* 2010-02-01 john pfeiffer getchar() only takes one character from the buffer, but when a user presses \"enter\"... that's another character in the buffer... */ #include <stdio.h> int main () { char c = 'n' ; char buffer ; printf ( \"This program will take in one character you type\" ); printf ( \" and display it back to you. \\n \" ); printf ( \"GeekSpeak = Demo the extra \\\"\\n\\\" in the\" ); printf ( \" getchar() from user \\\" loop dilemma \\\"\\n \" ); do { printf ( \"Please enter one character and press enter...\" ); printf ( \"(y to quit)... Do not attempt to type in a word or else! \\n \" ); c = getchar (); printf ( \"%c \\n \" , c ); } while ( c != 'y' ); /* we must clear the stdin buffer of extra char's and the \\n for the y!*/ do { buffer = getchar (); } while ( buffer != '\\n' ); printf ( \"Ha ha, to quit press 'y' again\" ); printf \"((this corrected version will only display the first char entered). \\n \" ); do { printf ( \"Press a key or enter a word, then press enter (use y to quit): \\n \" ); c = getchar (); do { buffer = getchar (); } while ( buffer != '\\n' ); printf ( \"%c \\n \" , c ); } while ( c != 'y' ); return 0 ; } /* end of main */","tags":"programming","title":"Debugging and \"Accidental Difficulties\" with getchar and loops: 365 programming project day thirty two"},{"url":"https://blog.john-pfeiffer.com/c-programming-command-line-clock-continued-winclockv2-365-programming-project-day-thirty-one/","text":"Slowly working towards the final product, indeed I do see that I have a working executable at every stage (even if the steps are small and the tests are numerous)... (The looping part does not quite work yet...) /* 2010-01-31 john pfeiffer, MS windows clock PROGRAM DESIGN MS window with X, quit button, and current system time displayed Hour:minute:second (hh:mm:ss) HIGH LEVEL FUNCTIONS update current time see if user clicked button quit if button clicked display current time on window ORGANIC ITERATIVE BUILDS 1. build a program to show current time (in c/dos) then exits immediately *TEST: should show correct system time each time user presses enter show new current time (ctrl+c to exit) TEST: should show correct system time on each click 2. build a windows \"quit button app\" (can reuse previous work) TEST: program should quit cleanly 3. windows with current time (once) and quit button TEST: program should show correct system time and when button clicked quit 4. windows with current time constantly updating and quit button (will the processor be overloaded while waiting? need semaphores?) */ #include <stdio.h> #include <time.h> #include <string.h> /* stdio.h is for displaying output to command line time.h is for time() string.h is to help format any strings created */ /* returns a string with the current time */ void get_current_time ( char current_time [] ); void display_time ( char current_time [] ); void clear_current_time ( char current_time [] ); int main ( int argc , char * argv []) { char c = 'n' ; char current_time [ 128 ]; /* show that the string has garbage that is cleaned out */ printf ( \" \\n DISPLAY current time variable (initial garbage) \\n \" ); display_time ( current_time ); /* Loop depending on the user to continue updating */ do { printf ( \" \\n EMPTY current time variable. \\n \" ); clear_current_time ( current_time ); printf ( \" \\n DISPLAY current time variable: \\n \" ); display_time ( current_time ); printf ( \" \\n UPDATE current time variable. \\n \" ); get_current_time ( current_time ); /* DEBUGGING printf(\"%s\\n\", current_time); */ printf ( \" \\n DISPLAY current time variable: \\n \" ); display_time ( current_time ); c = 'n' ; /* ensure that the user must force a continuance */ printf ( \" \\n PRESS y to update the current time variable again: \\n \" ); c = getchar (); } while ( c == 'y' ); return 0 ; } /* end main */ /* begin function definitions */ /* Get the current time from the system and update the time string */ void get_current_time ( char current_time [ 128 ] ) { time_t tempTime ; /* initialize the variable, otherwise only returns 1970 date */ tempTime = time ( NULL ); /* DEBUGGING printf(\"%s\\n\", asctime(localtime(&tempTime))); */ strcpy ( current_time , asctime ( localtime ( & tempTime ))); /* DEBUGGING printf(\"%s\\n\", current_time); */ } /* output the current time string ... to the command line */ void display_time ( char current_time [ 128 ] ) { printf ( \"CURRENT TIME: %s \\n \" , current_time ); } void clear_current_time ( char current_time [ 128 ] ) { memset ( current_time , 0 , sizeof ( current_time )); } To Be Continued...","tags":"programming","title":"C programming command line clock continued (winclockv2): 365 programming project day thirty one"},{"url":"https://blog.john-pfeiffer.com/c-programming-display-the-current-time-365-programming-project-day-thirty/","text":"I am trying to follow best practice as I've learned from the Mythical Man Month (Fred Brooks), the Cathedral and the Bazaar's (Eric Raymond)... C programming seems natural to me but I always want to stretch myself a little bit so here's a \"C Clock\" program that will eventually evolve into a \"Windows Clock\" program. So here is a modular design, released early with plenty of comments, debugging ability, and grown locally and organically: /* 2010-01-31 john pfeiffer, MS windows clock PROGRAM DESIGN MS window with X, quit button, and current system time displayed Hour:minute:second (hh:mm:ss) HIGH LEVEL FUNCTIONS get current time display current time on window update current time see if user clicked button quit if button clicked ORGANIC ITERATIVE BUILDS 1. build a program to show current time (in c/dos) then exits immediately TEST: should show correct system time each time user presses enter show new current time (ctrl+c to exit) TEST: should show correct system time on each click 2. build a windows \"quit button app\" (can reuse previous work) TEST: program should quit cleanly 3. windows with current time (once) and quit button TEST: program should show correct system time and when button clicked quit 4. windows with current time constantly updating and quit button (will the processor be overloaded while waiting? need semaphores?) */ #include <stdio.h> #include <time.h> #include <string.h> /* stdio.h is for displaying output to command line time.h is for time() string.h is to help format any strings created */ /* returns a string with the current time */ void get_current_time ( char current_time [] ); void display_time ( char current_time [] ); void clear_current_time ( char current_time [] ); int main ( int argc , char * argv []) { char current_time [ 128 ]; /* show that the string has garbage that is cleaned out */ printf ( \"Current Time variable garbage: \\n \" ); display_time ( current_time ); clear_current_time ( current_time ); printf ( \" \\n Current Time variable is EMPTY: \\n \" ); display_time ( current_time ); printf ( \" \\n Current Time variable is FULL: \\n \" ); get_current_time ( current_time ); return 0 ; } /* end main */ /* begin function definitions */ void get_current_time ( char current_time [ 128 ] ) { time_t tempTime ; /* initialize the variable, otherwise only returns 1970 date */ tempTime = time ( NULL ); printf ( \"%s \\n \" , asctime ( localtime ( & tempTime ))); } void display_time ( char current_time [ 128 ] ) { printf ( \"CURRENT TIME: %s \\n \" , current_time ); } void clear_current_time ( char current_time [ 128 ] ) { memset ( current_time , 0 , sizeof ( current_time )); } Of course this isn't a finished product - but its a solid foundation that outlines what the next few posts will be about...","tags":"programming","title":"C programming display the current time: 365 programming project day thirty"},{"url":"https://blog.john-pfeiffer.com/wince-custom-include-to-modularize-functions-365-programming-project-day-twenty-nine/","text":"I've again used some \"empty\" time in the London Tube to stretch my brain a little bit by doing some more WinCE Windows Programming. While it may not seem like much of an accomplishment, using my fingernail on the screen keyboard to eke out code on a moving underground train requires a certain zen attitude... wait... wait... hit the key... oh... the wrong thing showed up (because obviously I pressed the right key)... ok, backspace and do it again... oh, that wasn't backspace, that was ]... and sometimes it looks like ]]]]]. I'm trying to not only read/learn best practice, but practice best practice. Once again I am relying on PGCC (pocket GCC) though it does apparently have the limitation of only doing WinMain (not c's usual main); I suppose a \"big goal\" I might have for WinCE programming would be to one day use it to compile GCC on my IPAQ (overnight?). The ever mysterious \"c.bat\" (yes, the filename is very short for onscreen typing challenged fingernails) \\pgcc\\cc1plus \\pgcc\\cwm.txt -o \\pgcc\\cwms.s -I \\pgcc\\include -include \\pgcc\\fixincl.h -fms-extensions \\pgcc\\as \\pgcc\\cwms.s -o \\pgcc\\cwmo.o \\pgcc\\ld \\pgcc\\cwmo.o -o \\pgcc\\cwme.exe -L \\pgcc\\lib -l cpplib -l corelibc -l coredll -l aygshell -l runtime -l portlib CWM.TXT CWM.TXT #define WIN32_LEAN_AND_MEAN #include #include \"func.h\" /* MODULAR BY INCLUDE Function definitions hidden in includesabove I've told the compiler to use the func.h file as well as windows.h */ /* declare the function before main */ int outputText (); int APIENTRY WinMain ( HINSTANCE hInstance , HINSTANCE hPrevInstance , LPTSTR lpCmdLine , int ncmdshow ) { printf ( \"pre function \\n \" ); /* this is the big moment, calling a function defined in another file! */ outputText (); printf ( \"post function \\n \" ); /* So that if I run it in the Windows Explorer I can still see the output */ printf ( \"Press return to quit\" ); getchar (); return 0 ; } func.h /* function definitions go here */ int outputText () { printf ( \"function include \\n \" ); return 0 ; }","tags":"programming","title":"WinCE Custom Include to Modularize Functions: 365 programming project day twenty nine"},{"url":"https://blog.john-pfeiffer.com/pancakes-from-scratch-recipe-v2/","text":"You're hungry, you want something \"home cooked\", and you have the following: 1 egg 1.5 cups of flour .5 cup of sugar 1 pinch of baking soda 1 tsp of salt 3 tsp of melted butter At least 2 cups of Milk (or water) Stir and then pour in enough milk, while beating vigorously, until the mixture is smooth enough to pour. Try not to leave any flour unmixed in the batter. Hot frying pan (with oil in the bottom) for about 5 mins a side (hopefully you'll have a hotter frying pan/cooker and it will take less time). Usually you flip when the bottom is golden brown (you'll notice bubbles start appearing in the cooking batter. Shapes of hearts, letters, and animals are recommended. Possible spreads (separate or altogether): Jam, chocolate, honey, cheese with honey, philadelphia cream cheese, feta cheese, fruits, peanut butter, ?","tags":"puzzles","title":"Pancakes from Scratch Recipe v2"},{"url":"https://blog.john-pfeiffer.com/maintaining-an-old-asp-website-with-javascript-downloads-365-programming-project-day-twenty-eight/","text":"Our company has an outdated website which was a semi-custom \"product catalog\" from 2003 (or earlier) that uses SQL and ASP to create dynamic pages and a \"webadmin\" area to update the catalog (advertised as user friendly because no source code needed). Modern Content Management Systems (Drupal anyone?) have made this obsolete but like many corporations, \"If it's only broken a little and employees bend over backwards to make it work then don't fix or replace it until there's a disaster.\" I have had the masochistic pleasure of learning the system: everything in one folder of .asp files calling lots of other ASP files (and a couple of .js files) and calls to the SQL database with obscure 4 digit codes. Here's an example of me using my programming skills during the day for what might seem a simple request, \"Please add another PDF to a page on the Website\"... The first step was to upload the PDF into the database (so that it would be magically assigned a 4 digit number in the SQL). The webadmin was typically user unfriendly: Login -> Menu with a blank page -> click Catalog tab and finally see the left navigation bar: Catalog Main Catalog Structure Catalog Products Table Headers hmmm... I got lucky and on my second attempt found Catalog Structure -> Displays -> Intelligent Display Platform Up Arrow, Down Arrow, Edit, Delete Given the options I picked the obvious (no, not delete!) Stuffed in about twenty hot linked words and options I then found \"Product Family Files\" Counter-intuitively clicking on a \"...\" button to access a popup window to upload a new file... A byzantine system: scrolling through what appears to be every PDF file in the system to verify that my 5.7\" file isn't already there... Then using a Browse button (on the bottom of the screen) to see the PDF on my desktop, then seperately clicking the Upload button. Finally clicking on a \"SELECT\" button (at the top of the screen) to finally add it to the database (without any friendly message, just a refresh, so I scrolled through all 300+ items to make sure it was listed). Bored yet? Oh, at this point you've got the PDF selected but you still have to click ADD to actually put it in a table for that Product Family. So... where's the Javascript? Well this particular page that needs to be updated is actually a custom job that appears instead of the default... and it may even be that I did the work! So each Product Family is reached by clicking on a link on the homepage, but those links are the inscrutable type: http://protect-the-guilty/displays_products.asp?prodFamID=4148 I \"hacked\" the scripts.js file (which contains all of the Javascript functions, so maybe they got one thing right)... /* MODIFIED FUNCTION TO GO TO SPECIFIC PAGE INSTEAD OF PULLING FROM DATABASE FOR CERTAIN PRODUCTS*/ function GoToProductLine ( prodLineID , partOfFilePath ){ if ( ( prodLineID != 4136 ) && ( prodLineID != 4536 ) ) { location . href = partOfFilePath + \"_families.asp?prodLineID=\" + prodLineID ; } else { location . href = \"umr.asp\" ; } } So to locate the \"actual\" magic 4 digit ID of the PDF I had to go to the original (non redirected) webpage... http://protect-the-guilty/displays_products.asp?prodFamID=4148 View the Source Find the following bit of text: < td >< a href = \"javascript:void(0)\" class = \"regular\" onclick = \"DownloadFile(1245);\" > Example_Datasheet_Rev_1.0 </ a ></ td > Whew, almost there! Next, download the \"customized.asp\" file and save a backup copy (ALWAYS SAVE A BACKUP!). Then update the \"customized.asp\" file by adding your special code to the right place... < td >< a href = \"javascript:void(0)\" class = \"regular\" onclick = \"DownloadFile(1245);\" > Example_Datasheet_Rev_1.0 </ a ></ td > Once you've uploaded and tested it you can look at the clock and happily say, \"Uploading 1 pdf took me 2 frickin hours!\"","tags":"programming","title":"Maintaining an old ASP website with Javascript downloads: 365 programming project day twenty eight"},{"url":"https://blog.john-pfeiffer.com/dos-batch-file-using-a-for-loop-to-test-a-vpn-with-ping-365-programming-project-day-twenty-seven/","text":"Once again my work demands creative programming solutions. I have a new ISP with a brand new modem and I want to know if a VPN will stay connected overnight... BUT it's not a good test without some traffic... Enter the DOS/Windows Batch file... like a linux script or VBS, a .bat file is a series of commands which allow a creative programmer to do quite a bit. Using a text editor create your first .bat file: ping-test-vpn.bat (Yes, I realize we skipped the steps using echo \"hello world\", that a % sign means a variable, defining that a mapped network drive to a folder on another computer... oh well, break this one into little pieces if it's too much to swallow at once.) REM batch program to test and log the stability/uptime of a remote REM computer using ping and copying a file REM is a \"remark\" or a comment that the computer will ignore REM create a time stamp and append it to the end of the file echo %date% %time% >> ping-log.txt REM see if we can reach the remote computer and append to the log file ping -n 1 192.168.1.30 >> ping-log.txt REM Map network drive where we will copy files back and forth net use z: \\192.168.1.30\\groups\\shared REM copy a 1.8 MB file to the remote computer (DOS overwrites by default) copy install_flash_player.exe z:\\ REM list the files in the remote directory (including timestamp) and log it dir z:\\install* >> ping-log.txt REM remove our mapped network drive net use z: /delete echo \"-------------------------------------------------------------\" >> ping-log.txt REM ping 6 times takes about 5 seconds - like a (324/6) *5 = 4.5 mins \"pause\" REM redirect to NUL sends the output nowhere to not fill the screen ping -n 324 127.0.0.1 >NUL Once you've saved your .bat file, rather than just double click it (which works), I prefer to use Start -> Run -> cmd.exe to open up a DOS command line prompt. From there I cd to c:\\directory\\ and find the .bat file... then I type in: ping-test-vpn.bat Watching the output can be fascinating - programming (and debugging) is certainly the most engaging when it's interactive. With the above file using the bandwidth of the VPN to copy a file and pause we then need to repeat this all night long. REM forloop counting variable IN (start,step,end) DO command FOR /L %%i IN (1,1,2) DO ping-test-vpn.bat for-loop-counter.bat High level network testing is fundamentally the same, though they obviously want to introduce variables like large packets, lost packets,random intervals, and contention issues. Wow, another day and another program (useful even!)","tags":"programming","title":"DOS batch file using a for loop to test a vpn with ping: 365 programming project day twenty seven"},{"url":"https://blog.john-pfeiffer.com/javascript-validation-of-an-html-form-365-programming-project-day-twenty-six/","text":"On the 26th I missed my 365 entry because I was being Mr. Corporate IT Hero at our annual Company Meeting but I actually have a number of things from the past that I can comment on and insert... Everybody who has an HTML form would like some software \"intelligence\" to guide the User to fill in the \"required\" fields, or help direct the User if they've not entered a valid email address, etc... I'm not a fan of Javascript if only because it runs on the client and can be a big security hole. If it's merely written poorly it can crash the browser or confuse the heck out of the user (rendering your HTML Form useless). But here's the source code on how to do the most basic user input validation: < html >< head >< title > javascript form validation </ title ></ head > < body > <!-- onSubmit tells the browser that there is a javascript function to run when the user hits the submit button --> < FORM ACTION = \"javascript-form-validation.htm\" NAME = \"testform\" onSubmit = \"return validateMyForm()\" > Starting X Point: < input name = \"startx\" type = \"text\" >< br /> Starting Y Point: < input name = \"starty\" type = \"text\" >< br /> Email Address: < input id = \"email\" maxlength = \"80\" name = \"email\" size = \"20\" type = \"text\" /> < br /> < br /> < input type = \"submit\" /> </ form > < script type = \"text/javascript\" language = \"javascript\" > function validateMyForm () { if ( document . getElementById ( 'startx' ). value == '' ) { alert ( 'Please enter a Starting X value (integer)' ); document . getElementById ( 'startx' ). focus (); return false ; } if ( document . getElementById ( 'starty' ). value == '' ) { alert ( 'Please enter a Starting Y value (integer)' ); document . getElementById ( 'starty' ). focus (); return false ; } if ( document . testform . email . value == '' ) { alert ( 'Please enter your email' ); document . myForm . email . focus (); return false ; } //if we've passed all of the above checks return true ; } </ script > </ body > </ html > There is a ton more that you can do with Javascript (including for loops on checkboxes minimum 2 out of 5, regular expressions on email addresses, etc.) so hopefully I'll get to it this year! P.S. You must have Javascript enabled on your browser to see the example and some browsers will pop up \"ActiveX Warnings\" - if you're paranoid only run Javascript that you've coded yourself!","tags":"programming","title":"Javascript validation of an html form: 365 programming project day twenty six"},{"url":"https://blog.john-pfeiffer.com/php-user-input-html-sanitization-and-math-365-programming-project-day-twenty-five/","text":"HTML forms are quick way to get user data but PHP requires a PHP server. Luckily I have one and together it's quite easy to create a page that gets some info from a user and then does some calculations (in this case nothing fancy). I've done a little more User Input Sanitization than usual - basically the rule is: \"If you'll display it, clean up the HTML output, if you'll send it to a linux script, strip the slashes, and if you send it to a database, clean up any MySQL stuff\"... <html> <head> </head> <body> <?php if ( ! isset ( $_POST [ 'startx' ]) || empty ( $_POST [ 'startx' ]) || ! isset ( $_POST [ 'starty' ]) || empty ( $_POST [ 'starty' ]) || ! isset ( $_POST [ 'endx' ]) || empty ( $_POST [ 'endx' ]) || ! isset ( $_POST [ 'endy' ]) || empty ( $_POST [ 'endy' ]) ) { print '<form action=\"' . $_SERVER [ 'PHP_SELF' ] . '\" method=\"post\">' ; print \" \\n <br \\>\" ; print 'Starting X Point<input name=\"startx\" type=\"text\">' ; print \" \\n <br \\>\" ; print 'Starting Y Point<input name=\"starty\" type=\"text\">' ; print \" \\n <br \\>\" ; print 'End X Point<input name=\"endx\" type=\"text\">' ; print \" \\n <br \\>\" ; print 'End Y Point<input name=\"endy\" type=\"text\">' ; print \" \\n <br \\>\" ; print '<input type=\"submit\" /></form>' ; print \" \\n <br \\>\" ; } else { $startx = ( int ) htmlentities ( $_POST [ 'startx' ] ); $starty = ( int ) htmlentities ( $_POST [ 'starty' ] ); $endx = ( int ) htmlentities ( $_POST [ 'endx' ] ); $endy = ( int ) htmlentities ( $_POST [ 'endy' ] ); print \" \\n <br />\" ; print $startx . \",\" . $starty . \" \" . $endx . \",\" . $endy ; print \" \\n <br />\" ; print \"width: \" . ( $endx - $startx ); print \" \\n <br />\" ; print \"height: \" . ( $endy - $starty ); print \" \\n <br />\" ; print \"acos(startx): \" . acos ( $startx ); print \" \\n <br />\" ; } //end of if-else user filled in textarea ?> </body></html> Further reference about how easy it is to manipulate user numbers: http://php.net/manual/en/book.math.php","tags":"programming","title":"PHP User Input HTML Sanitization and Math: 365 programming project day twenty five"},{"url":"https://blog.john-pfeiffer.com/javascript-on-ms-crm-forms-365-programming-project-day-twenty-four/","text":"Programming isn't always fun and games. Here's an example from me putting in extra time on a weekend... At my work We use Microsoft Dynamics CRM 4.0 which is a customizable CRM web interface on top of MS SQL Server. While the built in functionality is pretty good a business always needs some more customization to get things \"just right\"... MS CRM has \"onload\" and \"onsave\" functionality for each entity that allows a developer to stick in some custom javascript. Below is the JS that controls the calculations on the Products section of a Quote for Customer Form. This improves the end user experience as user changes update the different fields instantly and automatically (though they still have to press SAVE to keep those changes). Note: Javascript isn't related to Java, it's a \"client side\" (runs on your computer, not the server) piece of code frequently used to modify how things look on your screen. // if the Tax field is null (ie a new quote is being created) we fill in a default tax if ( crmForm . all . new_taxpercentage . DataValue == null ) { crmForm . all . new_taxpercentage . DataValue = 17.5 ; } //when Loading the Quote Form and on certain fields I have the following: crmForm . all . quantity . FireOnChange (); //tells the system to pretend that the Quantity field has just changed //(and run it's jscripts) //this allows for one central place to control all of the calculations -------------------------------------------------------------------------------- //this forces even \"disabled\" fields to update values crmForm . all . new_taxpercentage . ForceSubmit = true ; //Calculate the BASE amount - MS CRM multicurrency required for money crmForm . all . baseamount_base . DataValue = crmForm . all . quantity . DataValue * crmForm . all . priceperunit_base . DataValue //Calculate the baseamount. crmForm . all . baseamount . DataValue = crmForm . all . quantity . DataValue * crmForm . all . priceperunit . DataValue ; //Only calculate the discount if the user has actually filled in the discount field if ( crmForm . all . new_manualdiscountpercentage . DataValue >= 0 ) { //Calculate the manual BASE discount amount. crmForm . all . manualdiscountamount_base . DataValue = crmForm . all . quantity . DataValue * crmForm . all . priceperunit_base . DataValue * ( crmForm . all . new_manualdiscountpercentage . DataValue / 100 ); //Calculate the manual discount amount. crmForm . all . manualdiscountamount . DataValue = crmForm . all . quantity . DataValue * crmForm . all . priceperunit . DataValue * ( crmForm . all . new_manualdiscountpercentage . DataValue / 100 ); } //Tricky piece of math with lots of ( and * and / ... //Calculate the new BASE Tax Amount crmForm . all . tax_base . DataValue = (( crmForm . all . quantity . DataValue * crmForm . all . priceperunit_base . DataValue ) - ( crmForm . all . manualdiscountamount_base . DataValue )) * ( crmForm . all . new_taxpercentage . DataValue / 100 ); //Calculate the new Tax Amount crmForm . all . tax . DataValue = (( crmForm . all . quantity . DataValue * crmForm . all . priceperunit . DataValue ) - ( crmForm . all . manualdiscountamount . DataValue )) * ( crmForm . all . new_taxpercentage . DataValue / 100 ); //Calculate the new Extended BASE Amount crmForm . all . extendedamount_base . DataValue = ( crmForm . all . baseamount_base . DataValue - crmForm . all . manualdiscountamount_base . DataValue ) + ( crmForm . all . tax_base . DataValue ); //Calculate the new Extended Amount crmForm . all . extendedamount . DataValue = ( crmForm . all . baseamount . DataValue - crmForm . all . manualdiscountamount . DataValue ) + ( crmForm . all . tax . DataValue );","tags":"programming","title":"Javascript on MS CRM Forms: 365 programming project day twenty-four"},{"url":"https://blog.john-pfeiffer.com/vbscript-windows-shell-script-programming-for-loop-365-programming-project-day-twenty-three/","text":"Programming obviously requires logic and discipline. Less well known but just as important, it also requires creativity and elasticity. Windows Script programming has a significantly different syntax (rules of how the code must be written to be valid) than the previous Linux Script, C Programming, HTML, or CSS. How many people do you know who speak multiple languages, much less create functional artificial constructs with them? You can copy and paste the text below into an empty notepad and save it as test.vbs A .vbs file can be dangerous as it is executing commands on your computer but in this case there are no surprises as you can see all of the commands explained in the comments. REM John Pfeiffer's windows vbscript 2010-01-22 REM In Visual Basic Script a REM stands for \"remark\", which is a comment REM (something ignored by the computer) REM we must declare what variables we want but without saying what type REM in this case we've used the good practice of naming the variables REM as they're intended to be used: i = programming standard for counting, REM astring is a string of characters (though in a real program it would be REM better named \"username\" or \"address_street\") and array is a list of items.dim i, astring, array REM assigning a literal piece of text to a string variable is really easyastring = \"this,should,be,interesting,csv,\" REM built in functions do most of the hard work - like splitting up a string REM into an array of strings based on a \"splitting character\"array = split(astring, \",\") REM wscript.echo displays message boxes, REM the & symbol concatenates strings and variables to display together wscript.echo astring & \" BECOMES => \" & array(4) & array(3) & array(2) & array(1) & array(0) REM The for loop counts from the first (lbound) element to the uppermost this REM \"object oriented\" technique of a method/function to access the attributes REM (in this case size/bounds) of a variable, rather than predefined symbol or number, REM prevents a careless programmer error or an unforseen change from crashing REM the program by accessing (or writing!) outside of the defined variable space REM Lines between the \"for\" and \"next\" are executed as many times as the for loop REM iteratesfor i = lbound(array) to ubound(array) wscript.echo array(i)nextREM the \"for each\" is a special case of a for loop which will do something for REM every item in the array - better than the above for this specific exampleREM as it is easier to read and understand what it is doing (with even lessREM chance of an error) for each i in array wscript.echo inextREM REMEMBER, 50-80% of your time will be spent debugging, recompiling, REM fixing, updating! More important than getting it right the first timeREM is making it easy to figure out where/why it went wrong wscript.echo \"the end\"","tags":"programming","title":"VBScript Windows Shell Script (Programming) For Loop: 365 programming project day twenty three"},{"url":"https://blog.john-pfeiffer.com/script-programming-linux-expect-script-passwd-365-programming-project-day-twenty-two/","text":"What the heck is a Script? Is writing a Script still programming? A script is a written set of instructions for a platform (usually an Operating System) that makes the computer (or hardware) do something. Sounds a lot like a Program! Well... A computer software program is usually source code (written by a human) that is then \"compiled\" into \"machine code\" which a machine can understand - except that there's also a linker and loader to figure out exactly what places in memory (and for what piece of hardware) the Program will run - finally turning it into Object code and then an Executable (usually called binary because it's in 1's and 0's directly in the language of the hardware). A script gives hardware a set of commands indirectly through another piece of software (it's environment/platform, e.g. DOS, Python, Windows OS, GNU Linux OS, etc.). The software then executes those commands (which often then tell an Operating System which then tells Hardware to to do something - like clearing the screen or adding some numbers). So maybe html is even less Programming like than a script! Since it only tells a browser (a specific application) how to format/display data!?!?! Ah, enough semanticarating! What happens if you're running a script and somewhere it requires user input? \"Expect\" is a piece of software that \"waits\" for input from the linux operating system command line... apt-get install expect to install it on linux (ubuntu) yum install expect to install it on linux centos/redhat touch test-script.sh create the file chmod +x test-script.sh make the file an executable in linux nano test-script.sh edit the file to enter our script commands, alternatively use vim test-script.sh 1 2 3 4 5 6 7 #!/usr/bin/expect set timeout 1 send \"passwd USER\\n\" expect \"Password: \" send \"password\\n\" sleep 1 interact First we tell the operating system what program (environment platform) will actually be able to read and execute the instructions (normally linux uses BASH /bin/bash but in this case it's EXPECT). Setting the timeout means that we will wait at least and at most 1 second before a non responding command is skipped to execute the next command. Send tells the operating system something (e.g. we want to run the change USER's password program) The expect command tells the script to wait for the operating system to prompt the user to type in a password. Once the operating system responds (with exactly what we \"expected\") the script continues by sending (just as if a user typed it in) some predetermined password picked by the script writer. Finally we tell our script to sleep for one second after all of it's hard work. Oh, and it's very important to \"interact\" , which returns the command prompt back to the user/operating system. Whew... that's just the tip of the iceberg of how \"expect\" greatly extends the functionality of Linux scripts! Later I hope to demonstrate some VBScript (for windows) and maybe even a script on how to run a \"non interactive FTP session\" (as most FTP programs require \"interaction\" aka human at the keyboard)...","tags":"linux","title":"Script Programming, Linux expect script passwd: 365 programming project day twenty two"},{"url":"https://blog.john-pfeiffer.com/windows-programming-in-wince-on-an-hp-ipaq-365-programming-project-day-twenty-one/","text":"As the number of portable computers (we might call them mobile phones or smart phones or pda's etc.) explodes they need software. One thing Microsoft got right is that Windows CE, the stripped down version of Windows for Mobile Devices, uses most of the same basic programming platform/language as \"normal\" desktop windows programming. The following is a very interesting learning experience I've had (and a useful way to pass otherwise idle waiting time in the \"Tube\") programming Windows Applications in WinCE. A Compiler for WinCE: pocketgcc The first thing you need is a WinCE compiler - my choice was pocketgcc (even though it hasn't been updated since 2003), because a port of GCC to WinCE means a stable tool that I already know (sorta) how to use. http://pocketgcc.sourceforge.net/ The next hurdle after downloading and copying them onto my device (HP Ipaq) and double tapping (clicking) to install them was to use the CMD to cd \\pgcc Then I used a text editor (either Word or pocketnotepad) to create a batch file (yes, technically I suppose a batch file is a program too!) with the following cryptic lines... \\pgcc\\cc1plus \\pgcc\\source-code.txt -o \\pgcc\\cwms.s -I \\pgcc\\include -include \\pgcc\\fixincl.h -fms-extensions \\pgcc\\as \\pgcc\\cwms.s -o \\pgcc\\cwmo.o \\pgcc\\ld \\pgcc\\cwmo.o -o \\pgcc\\cwme.exe -L \\pgcc\\lib -l cpplib -l corelibc -l coredll -l aygshell -l runtime -l portlib The cc1plus.exe (compiler?) outputs the assembly code file but also includes the \"fixincl.h\" file and uses the -fms-extensions option (no, I haven't actually yet learned exactly why). The as.exe program turns the assembly into object code. Finally the loader turns the object code into an executable with the appropriate libraries. The following source code should look very familiar to my earlier Windows Programming examples. I've added comments here that are not in my \"production\" IPAQ environment source code because the screen is too small with lots of scrolling already... #define WIN32_LEAN_AND_MEAN #include <windows.h> #include <windowsx.h> #include <commctrl.h> #include <aygshell.h> #define IDC_ExitButton 40099 /* the IDC object requires a number, I just give them high unused ones */ /* the above includes are a bit of a mystery to me but the program doesn't work without them and I figure they must be specific to WinCE */ /* instead of function stubs, main, then function declarations I do them all before main() ... but of course I have to remember to always declare before using... so do as I say: always declare functions (and then remember to update them and the stub), not as I do... */ VOID APIENTRY HandlePaint ( HWND hwnd ) { HDC hdc ; /* handle to device context */ PAINTSTRUCT ps ; RECT rc ; /* rectangle structure */ TCHAR tszOut [] = TEXT ( \"hello!\" ); hdc = BeginPaint ( hwnd , & ps ); GetClientRect ( hwnd , ( LPRECT ) & rc ); /* the cryptic stuff below outputs a text string in a rectangle */ DrawText ( hdc , tszOut , _tcslen ( tszOut ), ( LPRECT ) & rc , DT_VCENTER | DT_CENTER | DT_SINGLELINE ); EndPaint ( hwnd , & ps ); } /* below is the most important function after main() where the \"large windows switch\" figures out what the user has done and responds */ LRESULT CALLBACK MenuWndProc ( HWND hwnd , UINT message , WPARAM wParam , LPARAM lParam ) /* w & l param's are extra data like x,y */ { switch ( message ) { case WM_COMMAND : /* there's a command from the user */ switch ( LOWORD ( wParam )) /* to find out what command */ { case IDC_ExitButton : /* the user pressed the button */ PostQuitMessage ( 0 ); break ; default : break ; } break ; case WM_DESTROY : PostQuitMessage ( 0 ); /* closes the program */ break ; case WM_PAINT : HandlePaint ( hwnd ); /* calls a \"modular\" drawing function */ break ; default : return DefWindowProc ( hwnd , message , wParam , lParam ); break ; } return 0 ; } int APIENTRY WinMain ( HINSTANCE hInstance , HINSTANCE hPrevInstance , LPTSTR lpCmdLine , int ncmdshow ) { HWND hwnd = NULL ; HWND ExitButton = NULL ; /* a handle to our button \"window\" */ MSG msg ; WNDCLASS wc ; HMENU hMenu ; wc . style = CS_HREDRAW | CS_VREDRAW ; wc . lpfnWndProc = ( WNDPROC ) MenuWndProc ; wc . cbClsExtra = 0 ; wc . cbWndExtra = 0 ; wc . hInstance = hInstance ; wc . hIcon = 0 ; wc . hCursor = 0 ; wc . hbrBackground = ( HBRUSH ) GetStockObject ( WHITE_BRUSH ); wc . lpszMenuName = NULL ; wc . lpszClassName = ( LPTSTR ) L\"Menu App\" ; /* WinCE seems to require strings cast as LONG? */ if ( ! RegisterClass ( & wc )) { MessageBox ( NULL , TEXT ( \"errors \" ), L\"IMPORTANT\" , MB_OK ); return 0 ; } /* the size and placement of our program window */ hwnd = CreateWindow ( L\"Menu App\" , L\"2nd prog hello\" , WS_VISIBLE , 0 , 30 , 200 , 150 , ( HWND ) NULL , NULL , hInstance , ( LPSTR ) NULL ); /* the size and placement of our button child of hwnd \"window\" */ ExitButton = CreateWindow ( L\"BUTTON\" , L\"quit\" , WS_CHILD | WS_VISIBLE | BS_PUSHBUTTON , 0 , 30 , 100 , 100 , hwnd , ( HMENU ) IDC_ExitButton , hInstance , NULL ); ShowWindow ( hwnd , ncmdshow ); UpdateWindow ( hwnd ); while ( GetMessage ( & msg , NULL , 0 , 0 )) { TranslateMessage ( & msg ); DispatchMessage ( & msg ); } return msg . wParam ; } /* end WinMain */","tags":"programming","title":"Windows programming in WinCE on an HP IPAQ: 365 programming project day twenty one"},{"url":"https://blog.john-pfeiffer.com/a-simple-windows-button-365-programming-project-day-six/","text":"Windows programming is a bit ugly - you have to selectively ignore the stuff \"you know\". On the positive side I've figured out how to insert code as preformatted (Drupal Input Filter -> extending the \"safe HTML\") so this should be easier to copy paste. Also, Notepad2 has a handy \"turn tabs into spaces\" that I shall start using more often... I've built on the previous Windows Program that put some text on the screen so most of it should look familiar. Remember, this was compiled with a gcc port (see earlier post for the link to the binary) gcc.exe -o %1.exe %1 -Iinclude -Llib -Wall -ansi -mwindows OK, here's the code for an exit button, some of the old comments (which you've \"learned\" have been removed)... /* john pfeiffer basic MS windows program with button 2009-01-20 */ #include <windows.h> #define IDC_MyExitButton 40001 /* case sensitive! random high number to keep windows happy */ /* This function is called by the Windows function DispatchMessage( ) */ LRESULT CALLBACK WindowProcedure ( HWND hwnd , /* Handle of window that received the msg */ UINT message , /* The message */ WPARAM wParam , /* Extra parameter ( e . g . mouse x ) */ LPARAM lParam ) /* Extra parameter (e.g. mouse y) */ { PAINTSTRUCT ps ; /* a structure for a paint job (see below */ RECT rect ; /* a structure to hold rectangle values (e.g. x,y coordinates) */ HDC hdc ; /* handle to a DC (buffer) for the screen */ switch ( message ) /* handle the messages */ { case WM_COMMAND : switch ( LOWORD ( wParam )) /* find out what's been clicked */ { case IDC_MyExitButton : PostQuitMessage ( 0 ); break ; default : break ; } break ; case WM_PAINT : GetClientRect ( hwnd , & rect ); /* get the size of our window */ hdc = BeginPaint ( hwnd , & ps ); /* begin painting to the buffer */ DrawText ( hdc , TEXT ( \"Press button to quit\" ), - 1 , & rect , DT_CENTER | DT_VCENTER | DT_SINGLELINE ); EndPaint ( hwnd , & ps ); return 0 ; case WM_DESTROY : PostQuitMessage ( 0 ); /* send a WM_QUIT */ break ; default : /* for messages that we don't deal with */ return DefWindowProc ( hwnd , message , wParam , lParam ); } return 0 ; } int WINAPI WinMain ( HINSTANCE hThisInstance , /* Handle to the current instance */ HINSTANCE hPrevInstance , /* Handle to the previous instance */ LPSTR lpszArgument , /* pointer to command line arguments */ int ncmdshow ) /* show state of the window */ { HWND hwnd ; /* The handle for our window */ HWND ButtonPushed = NULL ; MSG messages ; /* Messages to the application */ WNDCLASS wc ; /* Data structure for our defined windowclass */ wc . style = 0 ; wc . lpfnWndProc = WindowProcedure ; /* This function is called by windows */ wc . cbClsExtra = 0 ; /* No extra bytes after the window class */ wc . cbWndExtra = 0 ; /* structure or the window instance */ wc . hInstance = hThisInstance ; /* handle to the owner */ wc . hIcon = NULL ; /* no special application icon */ wc . hCursor = LoadCursor ( NULL , IDC_ARROW ); /* default cursor */ wc . hbrBackground = ( HBRUSH ) GetStockObject ( LTGRAY_BRUSH ); wc . lpszMenuName = NULL ; /* No menu */ wc . lpszClassName = \"WindowsApp\" ; /* the name of the windows class */ /* Register the window class, if fail quit the program with an error value */ if ( RegisterClass ( & wc ) == 0 ){ return - 1 ; } /* The class is registered, let's instantiate our window */ hwnd = CreateWindowEx ( 1 , \"WindowsApp\" , \"Windows Title\" , WS_OVERLAPPEDWINDOW , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , CW_USEDEFAULT , NULL , NULL , hThisInstance , NULL ); /* create button and store the handle */ ButtonPushed = CreateWindow ( \"button\" , /* class name */ \"Push Button\" , /* button caption */ WS_CHILD | WS_VISIBLE | BS_PUSHBUTTON , /* the styles */ 0 , 0 , /* the left and top coordinates */ 150 , 50 , /* width and height */ hwnd , /* parent window handle */ ( HMENU ) IDC_MyExitButton , /* the ID of your button */ hThisInstance , /* the instance of your application */ NULL ) ; /* unnecessary extra */ ShowWindow ( hwnd , ncmdshow ); /* Make the window visible on the screen */ UpdateWindow ( hwnd ); /* update with changes */ /* Run the message loop. It will run until GetMessage( ) returns 0 */ while ( GetMessage ( & messages , NULL , 0 , 0 ) ) { TranslateMessage ( & messages ); /* Translate virtual-key messages into character messages */ DispatchMessage ( & messages ); /* Send messages to WindowProcedure */ } /* The program return-value is 0 - The value that PostQuitMessage( ) gave */ return messages . wParam ; }","tags":"programming","title":"A Simple Windows Button: 365-programming project day six"},{"url":"https://blog.john-pfeiffer.com/php-and-css-on-a-radio-button-board-365-programming-project-day-five/","text":"Overview I'm a little sick so this one took longer than it should have. Futher exploration of the theme of PHP + CSS + HTML as a very simple way to get user input and then do something fun with it. The beginnings of trying to \"modularize\" code into functions earlier in the process (in later, more complex programs it is a necessity). You may be asking, \"How is this useful?\" but I think it's a popular misconception that Programs should be useful... I will try to break the code into chunks (that you will have to re-assemble) but with better piece by piece explanation: Defining CSS Style in the Head Best practice is to have many php or html pages linked to a single CSS definition file so that a single update is more efficient. Efficiency in this case means all of the code in one easily visible file. The CSS here removes the anchor (hyperlink) underline and makes it black BUT if hovered the text will turn red. < html >< head > < style type = \"text/css\" > <! -- removing text decoration removes the underlines -- > #chosen a { text-decoration : none ; color : black ; } #chosen a :hover { text-decoration : none ; color : red ; } </ style ></ head > Body of PHP functions If you have an HTML head then you should have a body. The majority of the body is the definition of a custom function called \"display_board\". Again I'm using the trick that if the user has entered data (and therefore something's in the $_POST) then we get to show the user that we've done something fun with what they gave us. PHP functions are easy to define and have the usual parameters and return value responsibilities (though parameters don't need any typing which allows the programmer to focus on the concepts, not the implementation - hopefully not confusing the variables up in the process). Much like a CSS you can have a file with the definitions of your php functions and include it somewhere with include(\"footer.php\"); On one hand you remove the implementation details which can make prototyping and modularization faster and easier, on the other hand you have to search through/open a number of different files (or even include definitions to functions you don't use) and of course, it's great to ignore the implentation details right until something goes wrong. function display_board <body>Click on one square, then submit, then hover your choice!<br /> <br /> <?php function display_board ( $chosen_square ) { print '<form action=\"' . $_SERVER [ 'PHP_SELF' ] . '\" method=\"post\">' ; print \" \\n \" ; //newline character in the HTML code for readability print '<div id=\"chosen\" align=\"center\"\\>' ; print \" \\n \" ; I don't think the indentations show up well in the blog but the nested loops and formula are just a fancy way of counting from 1 to 64 with 8 items per line. The magic \"redefined css anchor\" is buried in there if the user has chosen a square. nested for loops for($x=0; $x<8; $x++) { for($y=0; $y<8; $y++) { if( ((8*$x) + $y+1) == $chosen_square ) { print '<input type=\"radio\" name=\"board_square\" value=\"'; print ((8*$x) + $y+1) . '\">'; print '<a href=\"#\"> '; print ((8*$x) + $y+1); print \"</a>\\n\"; //the nested for loops formula creates 64 consecutive values } else { print '<input type=\"radio\" name=\"board_square\" value=\"'; print ((8*$x) + $y+1) . '\"> ' . ((8*$x) + $y+1) . \"\\n\"; } } print \"<br \\>\\n\"; } print '</div><input type=\"submit\" value=\"Press Me\" /></form>'; user input stripslashes is best practice when dealing with user data though I'm not sure how a $_POST of a radio button value could end up as a root command; better safe than sorry. The ever present \"if the $_POST variable is set AND the $chosen_square variable is NOT EMPTY\" allows to only print something that exists (instead of foolishly printing non-existent stuff). Finally we call our function (with a parameter). So if the CSS and PHP function definition were other files this \"program\" would be very compact and very easy to read (but then again you wouldn't know what the program did unless you had those other files). $chosen_square = stripslashes( $_POST['board_square'] ); if( !empty( $chosen_square ) && isset( $chosen_square ) ) { print $chosen_square; } display_board( $chosen_square ); ?> </body> </html>","tags":"programming","title":"PHP and CSS on a radio button board: 365 programming project day five"},{"url":"https://blog.john-pfeiffer.com/first-real-windows-program-365-programming-project-day-four/","text":"Windows Programming has a steep curve in the beginning as there are many things to memorize (oops I mean learn and understand) as theoretically they've created the data structures and functions to get stuff on the screen. Again I've used gcc 2.95 windows port with the special command on my wndclass-hello.c source code. gcc.exe -o %1.exe %1 -Iinclude -Llib -Wall -ansi -mwindows The code is extra commented as this is the foundation (of understanding) for future programs. Basically the windows program loops waiting for some input (e.g. clicking an OK button or the X to close the window)... in the mean time I've written a simple text in the middle. Most of the code is actually just trying to setup the data structure of a window (there are many options and variations that we might use later to extend our window's functionality). /* john pfeiffer basic MS windows program with comments 2009-01-18 */ # include /* this includes the whole Windows API */ /* Declare the Windows procedure */ LRESULT CALLBACK WindowProcedure ( HWND , UINT , WPARAM , LPARAM ) ; int WINAPI WinMain ( HINSTANCE hThisInstance , /* Handle to the current instance */ HINSTANCE hPrevInstance , /* Handle to the previous instance */ LPSTR lpszArgument , /* pointer to command line arguments */ int ncmdshow ) /* show state of the window */ { HWND hwnd ; /* The handle for our window */ MSG messages ; /* Messages to the application */ WNDCLASS wc ; /* Data structure for our defined windowclass */ wc . style = 0 ; wc . lpfnWndProc = WindowProcedure ; /* This function is called by windows */ wc . cbClsExtra = 0 ; /* No extra bytes after the window class */ wc . cbWndExtra = 0 ; /* structure or the window instance */ wc . hInstance = hThisInstance ; /* handle to the owner */ wc . hIcon = NULL ; /* no special application icon */ wc . hCursor = LoadCursor ( NULL , IDC_ARROW ) ; /* default cursor */ wc . hbrBackground = ( HBRUSH ) GetStockObject ( LTGRAY_BRUSH ) ; /* the window background color */ wc . lpszMenuName = NULL ; /* No menu */ wc . lpszClassName = \"WindowsApp\" ; /* the name of the windows class */ /* Register the window class, if fail quit the program with an error value */ if ( RegisterClass (& wc ) == 0 ) { return - 1 ; } /* The class is registered, let's instantiate our window */ hwnd = CreateWindowEx ( 1 , /* Extended possibilites for variation */ \"WindowsApp\" , /* Classname */ \"Windows App\" , /* Application Title Text */ WS_OVERLAPPEDWINDOW , /* default window */ CW_USEDEFAULT , /* default x position on the screen */ CW_USEDEFAULT , /* default y position on the screen */ CW_USEDEFAULT , /* initial window width */ CW_USEDEFAULT , /* initial window height */ NULL , /* no Parent window */ NULL , /* No menu */ hThisInstance , /* Program Instance handler */ NULL /* No pointer for passing parameters */ ) ; ShowWindow ( hwnd , ncmdshow ) ; /* Make the window visible on the screen */ UpdateWindow ( hwnd ) ; /* update with changes */ /* Run the message loop. It will run until GetMessage( ) returns 0 */ while ( GetMessage (& messages , NULL , 0 , 0 ) ) { TranslateMessage (& messages ) ; /* Translate virtual-key messages into character messages */ DispatchMessage (& messages ) ; /* Send messages to WindowProcedure */ } /* The program return-value is 0 - The value that PostQuitMessage( ) gave */ return messages . wParam ; } /* This function is called by the Windows function DispatchMessage( ) */ LRESULT CALLBACK WindowProcedure ( HWND hwnd , /* Handle of window that received the msg */ UINT message , /* The message */ WPARAM wParam , /* Extra parameter (e.g. mouse x) */ LPARAM lParam ) /* Extra parameter (e.g. mouse y) */ { PAINTSTRUCT ps ; /* a structure for a paint job (see below */ RECT rect ; /* a structure to hold rectangle values (e.g. x,y coordinates) */ HDC hdc ; /*handle to a DC (buffer) for the screen */ switch ( message ) /* handle the messages */ { case WM_PAINT : GetClientRect ( hwnd , & rect ) ; /* get the size of our window */ hdc = BeginPaint ( hwnd , & ps ) ; /* begin painting to the buffer */ DrawText ( hdc , TEXT ( \"Hello\" ), - 1 , & rect , DT_CENTER | DT_VCENTER | DT_SINGLELINE ) ; /* the above actually draws the text, h & v centered on a single line, to the buffer */ EndPaint ( hwnd , & ps ) ; return 0 ; case WM_DESTROY : PostQuitMessage ( 0 ) ; /* send a WM_QUIT to the message queue */ break ; default : /* for messages that we don't deal with */ return DefWindowProc ( hwnd , message , wParam , lParam ) ; } return 0 ; } /* typedef struct tagPAINTSTRUCT { HDC hdc; // A handle to the display DC to be used for painting. BOOL fErase; //Indicates whether the background must be erased. RECT rcPaint; //specifies upperleft and lowerright corners where to be painted //in device units relative to the upper-left corner of the client area. BOOL fRestore; BOOL fIncUpdate; BYTE rgbReserved[32]; }PAINTSTRUCT, *PPAINTSTRUCT; */ Whew, another day done.","tags":"programming","title":"First real Windows program: 365 programming project day four"},{"url":"https://blog.john-pfeiffer.com/php-is-mad-cool-with-css-365-programming-project-day-three/","text":"PHP is similar to C which makes it easy for me to do interesting things. The php manual webpage ( http://php.net/manual/en ) is brilliant! Don't you wish everybody had a full public API, easy menus, working examples, searchable and with the best user comments that really flesh out the bugs? CSS is so much better than HTML tables - it really revolutionizes the simplicity, maintainability, and power of displaying things on the web. Put them together and you have miracles like Drupal. The following project took a little more time than usual and it might even be useful one day, but what I really like about it is that it's FUN... I apologize about the poor formatting of the source code, I'll see if I can find a nicer way to enter into the Blog or break it up into \"this is how you do\" sections... Many thanks to my debugger Bobby! php-list-to-diagram.php <html> <head> </head> <body> This page takes a list of objects, one object per page, and displays them in a list using css. <br />&nbsp;<br /> Please fill in the box below: <br />&nbsp;<br /> <?php //The POST variable is an array with each form item as an item in the index //if the object list variable is not filled out then we ask the user to fill it in /* Clicking the submit button does the form action: the same page again with the POST data the ' quotations will ignore the \" quotations which is useful when outputing HTML but sometimes very hard to read (or debug, along with spaces between HTML options) the \" quotations allow the \\n newline to be output to the HTML code spacing */ print '<form action=\"' . $_SERVER [ 'PHP_SELF' ] . '\" method=\"post\">' ; print \" \\n \" ; //making extra spacing obvious improves readability and debugging print '<textarea name=\"objectlist\" cols=\"40\" rows=\"20\">' ; if ( ! isset ( $_POST [ 'objectlist' ]) || empty ( $_POST [ 'objectlist' ] ) ) { print \" \\n </textarea><br />\" ; print \" \\n \" ; print '<input type=\"submit\" /></form>' ; print \"<br />\" ; /* The else allows the user to see what they typed in last time BUT for security no slashes! */ } else { print stripslashes ( $_POST [ 'objectlist' ]); print \"</textarea><br />\" ; print '<input type=\"submit\" /></form>' ; print \"<br />\" ; $object_list = $_POST [ 'objectlist' ]; //one string for all of the user entered items $object_lines = explode ( \" \\n \" , $object_list ); //break the string into lines print_r ( $object_lines ); //dump the array for debugging print \"<br />&nbsp;<br />\" ; /* the for loop takes each item in the array and copies it into the $value variable and it assigns the index to the $key variable. rtrim() removes the newline the user entered in the form. */ foreach ( $object_lines as $key => $value ) { print '<div style=\"position: absolute; left: ' . ( 40 + $key ) . '%; top: ' . ( $key + 2 ) . 'em;\">' ; print $value ; print \"</div> \\n \" ; } } //end of if-else user filled in textarea ?> </body> </html>","tags":"programming","title":"PHP is mad cool with CSS: 365 programming project day three"},{"url":"https://blog.john-pfeiffer.com/windows-messagebox-365-programming-project-day-two/","text":"Overview Programming in Windows is like building with legos. It can be easy and fun but at the same time very few of us have houses or cars made of legos. Of course, if you're writing an application for 90% of the computer users it will probably have to run on Windows. I've programmed with Borland, Mingw, Visual Studio and my favorite, GCC. The story of the GNU C Compiler (and Richard Stallman) is fascinating and reading \"Free as in Freedom\" really opened my eyes to the details of the shoulders of the giants that I'm standing upon. This tiny program opens up a message box that you can immediately close. While not very useful it very often inspires new programmers with the power that they can quickly wield to get their ideas into a working program. Anyways, here's the win32 source code... Windows Text Editor Open up your favorite text editor (I very much like Notepad2 or Bluefish)... Windows MessageBox /* include all of the prebuilt Windows librarys to the fun stuff like GUI */ # include /* the \"main\" will return a 1 or 0 depending on how the application terminates */ int WINAPI WinMain ( HINSTANCE hThisInstance , /* Handle to the current instance */ HINSTANCE hPrevInstance , /* Handle to the previous instance */ LPSTR lpszArgument , /* pointer to command line arguments */ int ncmdshow ) /* show state of the window */ { /* call the messagebox function, no \"parent\", text to include, title bar text, and ? */ MessageBox ( NULL , \"Yet another program by John Pfeiffer\" , \"Close by clicking X or OK\" , NULL ) ; return 0 ; /* the \"main\" function now returns a good result, 0 */ } As you can see, lots of nice comments. I compiled the above with a windows port of GCC that I like because I can zip and move it around and it still always works. gcc compile windows app I DID have to come up with the funny batch file to actually make the GCC compile it on windows: gcc.exe -o %1.exe %1 -Iinclude -Llib -Wall -ansi -mwindows","tags":"programming","title":"Windows MessageBox: 365 programming project day two"},{"url":"https://blog.john-pfeiffer.com/translate-this/","text":"As I can speak bulgarian, french, and english (and almost speak italian and spanish) I realized that having my site entirely in english is limiting it's popularity. Therefore I recommend the following options to ensure that you can understand life, the universe, and the meaning of happiness: http://translate.google.com http://www.babelfish.com p.s. the above links will not only allow you to translate a website but you can write emails to people in foreign languages full of computer generated errors, WOW much more efficient!","tags":"puzzles","title":"Translate This!"},{"url":"https://blog.john-pfeiffer.com/first-time-baking-bread-version-116/","text":"I can cook pancakes, cookies, and cake even... from the pre-made mixes. Just add water, eggs,stir, heat in oven, eat... I've tried making pancakes from scratch (next time I'll post the recipe) with ok results. My first \"bread\" attempt didn't actually make dough that you can roll so I'm not sure if it was bread... Mix the following in a bowl: 3 glasses of flour (plain, none of that fancy already mixed stuff for me!) 1 glass of sugar (granular cane sugar is all I had) 3 tsp of baking soda 4 tsp of salt Beat the following into a different bowl: 2 eggs 1 cup of milk Then stir the two bowls together until mostly smooth. Smear cooking oil in the bottom of a pan (mine was 1/2\" by 7\" by 9\") to prevent sticking. Pour the mixture into the pan (not really dough, eh?) Then I melted enough butter to put a tiny layer on top of everything in the pan (note there was lots of leftover \"dough\" for a second time on the pan). Baked at 200 degrees celsius for about 15 minutes then set to cool for another 10 minutes. VERDICT: Light and ok for fluffiness. Good crust. Definitely too much baking soda but with apricot jam spread tastes fine. Serves 2-3 (or me over one day). Good things: oil on the pan, butter on top, sugar is IMPORTANT. NEXT TIME I will try halving all of the ingredients, reducing the soda by yet another half tsp, and maybe some more sugar or vanilla/cinnamon. *LEGEND 1 small glass = about 200 ml ... is that a cup? (not quite 236ml...) ml = milliliter, tsp= teaspoon","tags":"puzzles","title":"First Time Baking Bread, version .116"},{"url":"https://blog.john-pfeiffer.com/css-hover-365-programming-project-day-one/","text":"365 days of programming I have been inspired by my wife's 365 day project ( https://www.flickr.com/groups/love_life_art/pool/ to do a photograph a day for 2010. I spend my working days and much of my free time installing, repairing, updating, maintaining, and administering computers, servers, routers, business software, etc. Yet I have long wanted to focus more on Programming, and rather than spending my spare time reinstalling Linux (ok Stallman, GNU/Linux) and I thought that a little structure and motivation would help me channel my energies. So every day I will write a program, \"A set of coded instructions that enables a machine, especially a computer, to perform a desired sequence of operations.\" To keep it a S.M.A.R.T. goal: Specific = 1 program, Measurable = per day, Achievable = I hope so, Relevant*, Tangible = all source code posted) I will try to utilize some fundamental good rules of programming: high level plan modular devise a test and desired outcome before you start coding clean, descriptive variable and function names with a minimum of parameters comments built into the code *Relevant = my own rules for this are that it must be code writing that makes something happen: e.g. coding html is ok, changing the color/theme on a website by clicking a menu is not. Writing a script to install something is ok, installing something by choosing menu options is not. It can be on any platform or hardware Windows, Linux, WinCE, Android, laptop, desktop, server, virtual machine, phone, etc. Building up little pieces into a larger one is ok, in fact it's not only Programming best practice but one of the goals of the whole project (shhh... to be revealed at the end!) Building little applications: first a program to display hard coded data, then a program to take hard coded data and do something to it (e.g. add up a bunch of numbers), then a program to get input from a user, then a program to integrate all of the above! Learning by copying others is ok for just learning but my code has to be modified enough to represent my work and understanding. Enough talking, let's see some code! Pseudo Code (aka high level plan) I want a web page that when a user hovers their mouse over an image it changes. I will use html and css First I need to create the original image and the \"hovered\" image. My first \"test\" is that after I upload them I can actually see them (e.g. debugging that they're at the right location) <html> <head> <!-- this is embeddeding my cascading style sheet commands directly into the HTML --> <style type= \"text/css\" > <!-- a block displays down, inline would display it to the right The width and the height of the image are important (otherwise it won't all show) We remove \"text decoration\" to prevent any funny \"anchor link\" borders --> #rollover { display: block; width: 190px; height: 80px; text-decoration: none; background: url(\"http://kittyandbear.net/images/blog/usa-image.png\") no-repeat 0 0; } #rollover:hover { width: 190px; height: 80px; text-decoration: none; background: url(\"http://kittyandbear.net/images/blog/usa-image-hover.png\") no-repeat 0 0; } </style> </head> <body> I require 2 images, the original and a \"hovered over\" image: <br \\ > <a id= \"rollover\" href= \"#\" ><span></span></a> <br \\ > Notice there was a delay sometimes, a recommended solution would be to use a single image and a tricky \"only show the half that you want to see\", e.g. normal top half, \"hover\" = lower half. </body> </html> Voila! http://kittyandbear.net/john/web-tutorials/css-image-mouseover.htm One down, many more to go!","tags":"programming","title":"CSS Hover: 365 programming project day one"},{"url":"https://blog.john-pfeiffer.com/install-virtualbox-3-on-debian-5-gui/","text":"NOTE: this is a command line installation of VirtualBox 3 proprietary driver edition... The \"open source edition\" does not include RDP into VM's nor USB drivers for VM's to access host USB's Preparation Steps uname -r //find your kernel version apt-cache search linux-headers //in the long list see if your version is there sudo apt-get install linux-headers-$(uname -r) //failed because I have \"squeeze\" 2.6.32-rc8-686 unfortunately the rc8 headers were missing from debian website apt-get install linux-headers-2.6.32-trunk-686 //manual way (instead of the fancy function returns a result) ls -ahl /usr/src //ensure the folders are there, sometimes no kernel folder export KERN_DIR=/usr/src/kernels/2.6.32-trunk-686 //sometimes instead /usr/src/2.6.32-trunk-686 Actual Installation http://www.virtualbox.org/wiki/Linux_Downloads nano /etc/apt/sources.list //add line deb http://download.virtualbox.org/virtualbox/debian lenny non-free apt-get update apt-cache search virtualbox apt-get install virtualbox-3.0 //install without verification... sudo addusers username vboxusers OR usermod -a -G vboxusers USERNAME GUI Management Setting up virtualbox-3.0 (3.0.12-54655_Debian_lenny) ... addgroup: The group \\`vboxusers' already exists as a system group. Exiting. Messages emitted during module compilation will be logged to /var/log/vbox-install.log. Success! Starting VirtualBox kernel module:done.. Setting up libsdl-ttf2.0-0 (2.0.9-1) ... type VBoxManage -v to see if everything is working (shows your VirtualBox version) VirtualBox this command starts the GUI interface where you can easily create/manage you can test it - from the command line create and start a VM, you'll see it appear in the GUI! VBoxManage createvm --name VM1 --register VBoxManage startvm VM1 VirtualBox GUI is pretty straightforward but I highly recommend that you skim the manual and especially that you learn to play with the command line VBoxManage Troubleshooting /etc/init.d/vbox_drv setup The above command often fails, and when it fails, check the install log nano /var/log/vbox-install.log no kernel headers installed/downloaded or export (so the OS knows where!) the user running VirtualBox needs to be in the /etc/group/vboxusers if you have not installed the right headers (e.g. Virtualbox is compiling a driver specific to your kernel) So you need gcc, make... also you need the kernel-headers source uname -a will show you exactly what kernel you have installed ls -ahl /usr/src will show you exactly what directories are there... see some varations: export KERN_DIR=/usr/src/2.6.32-trunk-686 export KERN_DIR=/usr/src/kernels/2.6.32-trunk-686 export KERN_DIR=/usr/src/kernels/2.6.18-128.4.1.el5-x86_64/ sudo apt-get remove virtualbox-3.0 the easiest method to uninstall do all of the fixes (e.g. in my case I had to downgrade my kernel from 2.6.32-rc8 to 2.6.32-trunk) sudo apt-get install linux-image-686 sudo apt-get install linux-headers-$(uname -r) export KERN_DIR=/usr/src/2.6.32-trunk-686 sudo apt-get install virtualbox-3.0 usermod -a -G vboxusers USERNAME after the reinstall hopefully you'll see: Setting up libcurl3 (7.19.7-1) ... Setting up virtualbox-3.0 (3.0.12-54655\\_Debian\\_lenny) ... addgroup: The group \\`vboxusers' already exists as a system group. Exiting. Messages emitted during module compilation will be logged to /var/log/vbox-install.log. Success! Starting VirtualBox kernel module:done.. Setting up libsdl-ttf2.0-0 (2.0.9-1) ... type VBoxManage -v to see if everything is working (shows your VirtualBox version)","tags":"virtualization","title":"Install Virtualbox 3 on Debian 5 gui"},{"url":"https://blog.john-pfeiffer.com/how-to-use-drupal-blocks-and-a-custom-php-hit-counter-block/","text":"Blocks control the layout of the pages, i.e. a \"footer\" block appears at the bottom of each page. Each theme might have a different layout (and blocks available), and of course you can add your own custom blocks. Using the WebUI you can modify the look of the site's layout pretty quickly: Administer -> Blocks Drag and drop the \"Powered by Drupal\" option from FOOTER into DISABLED . ADVANCED a custom php block (maybe dangerous) admin/build/block create a new block in the body (plain text!) insert (copy paste?) your code Input Format = PHP code (Core Module -> Optional -> PHP Filter must be enabled) Home -> Administer -> Site Configuration -> Input Format admin/settings/filters PHP code -> configure (the super user System Administrator ALREADY has this filter, the above only allows you to add other users which is dangerous!) IF HITFILE EXISTS: read current count IF NOT, return error <?php $filename = \"hit-counter.txt\" ; $file_pointer = fopen ( $filename , 'a+' ); //r+ = read + write, start at beginning if ( is_writable ( $filename )) { $buffer = \"test123 \\n test456 \\n test789\" ; fwrite ( $file_pointer , $buffer ); } fclose ( $file_pointer ); <? php $filename = \"hit-counter.txt\" ; $file_pointer = fopen ( $filename , 'r' ); //r = read, starts at beginning if ( $file_pointer == NULL ) { die ( \"error accessing file\" ); } fseek ( $file_pointer , 0 ); $filecontents = file_get_contents ( $filename ); $filecontents ++ ; //the file only contains an integer print $filecontents ; fclose ( $file_pointer ); $file_pointer = fopen ( $filename , 'w' ); //r = read, starts at beginning if ( $file_pointer == NULL ) { die ( \"error accessing file\" ); } if ( is_writable ( $filename ) ) { fseek ( $file_pointer , 0 ); fwrite ( $file_pointer , $filecontents ); } fclose ( $file_pointer ); ?>","tags":"programming","title":"How to use Drupal Blocks and a custom php hit counter block"},{"url":"https://blog.john-pfeiffer.com/drupal-site-offline-how-to-login/","text":"Drupal: \"Site is offline for maintenance\" message I turned my website offline for maintenance and then logged out (or wanted to work on it from another computer). All I got was the \"Site is offline for maintenance message\". The Solution Type the following address into your browser: http://your-drupal-website.com/user or http://your-drupal-website.com/?=user This will take you to the user login prompt - if you are the Administrator user you can then turn the site back online.","tags":"programming","title":"Drupal Site Offline - how to login"},{"url":"https://blog.john-pfeiffer.com/how-to-install-virtualbox-3-on-centos-5-minimal/","text":"From the install DVD: choose no packages and customize all to deselect all packages BEWARE: this is a very empty configuration so you will have to install A LOT from yum... yum install wget elinks sudo which IF you need to do anything with \"compiling\" yum install make gcc yum install kernel ? maybe necessary? yum install kernel-devel-$(uname -r) yum install kernel-headers-$(uname -r) export KERN_DIR=/usr/src/kernels/$(uname -r)-x86_64 to add a repository, e.g. VirtualBox So create the following file (in the directory) /etc/yum.repos.d/virtualbox.repo [virtualbox] name = RHEL/CentOS-$releasever / $basearch - VirtualBox baseurl = http://download.virtualbox.org/virtualbox/rpm/rhel/$releasever/$basearch enabled = 1 gpgcheck = 1 gpgkey = http://download.virtualbox.org/virtualbox/debian/sun_vbox.asc then try yum install virtualbox download locations http://www.centos.org http://download.virtualbox.org/virtualbox/rpm/rhel/ http://download.virtualbox.org/virtualbox/debian/sun_vbox.asc","tags":"virtualization","title":"How To Install Virtualbox 3 on CentOS 5 Minimal"},{"url":"https://blog.john-pfeiffer.com/systeminfo-hardware-diagnostic-listing-ram-processor-ubuntu-version-ps-top-iotop-lshw-dmidecode/","text":"It is often critical to know exactly what is going on with your system. Linux is the defacto winner of the server world in large part because there are so many good tools. Especially since open source often needs to detect and reverse engineer drivers and compatibility for manufacturer hardware. higher level utilities for process ps processes with their virtual memory size top processes and cpu usage, type q to quit q = quit s = change interval of refresh (default is 3 seconds) u = type a User and only show processes from that User > = choose to sort by the next column < = choose to sort by the previous ps -a tree of all processes ps -ef full output of all processes including start time, uptime, cmd ps -eo pid,ppid,rss,vsize,pcpu,pmem,cmd -ww --sort=pid ps -eo pid,ppid,rss,vsize,pcpu,pmem,cmd -ww --sort=rss rss = ram usage vsize = virtual memory include swap as well ps -eo rss,vsz,pid,cmd,cputime | sort -n | tail -20 (this will show you possible memory leaks) ps guxca so the free command is funny ( http://kbase.redhat.com/faq/docs/DOC-1139 ) the amount +/- buffers/cache is the \"real\" amount available to the system (if the \"lazy\" kernel decided to free up something that's been buffered previously in order for a new app which needs the memory) <Physically Used Memory> = <Actual used memory> + <buffers> + <cache> <Physically Free Memory> = <Total Physical Memory> - <Actual used memory> - <buffers> - <cache> <Memory free for Applications> = <Total Physical Memory> - <Actual used memory> <Memory used by Applications> = <Actual used memory> - <buffers> - <cache> OS Version cat /etc/issue just the version number of the OS, e.g. ubuntu shows 16.04 cat /etc/debian_version only on debian/ubuntu... and shows squeeze/sid for natty 11.04, wheezy/sid for 11.10, jessie/sid - https://www.debian.org/releases/ cat /etc/lsb-release full ubuntu version lsb_release -a a command to shows all of the ubuntu version information, e.g. cat /etc/lsb-release lsb_release -c Codename: natty a command to just show the codename /proc is realtime info about the system cat /proc/version cat /proc/cpuinfo note: if there's \"lm\" (aka long mode) in the flags: fpu vme etc. area then you have 64bit capability... cat /proc/meminfo head -1 /proc/meminfo ls -ld /proc/somepid get some info about a specific process running via its pid number kernel information uname -a kernel version, x64 or 32 bit, machine name , multi-processor SMP, etc. getconf -a another way to get info about your kernel Memory and disk space free -m free memory in megabytes including swap/buffer usage) vmstat /virtual memory stats df -h free disk space in human readable numbers du disk usage for a directory and subdirectories - needs parameters , like du -s (summary) stat -f / -c \"%a * %s / 1024\" | bc get the specific amount of free space available from the root file system, stat --help bc is a handy \"built in calculator\" hardware listing fdisk -l shows all of the hard disk devices available - what os, bootable, etc. lshw exhaustive info about the hardware (tree by CLASS) lshw -class network hardware listing focused identification of Physical Network Adapter to logical name like eth0/wlan0 lshw -c video show the video hardware lspci lists devices connected to the PCI bus requires pciutils.deb depends on libpci3 , all the pci hardware including usb bridges,agp cards common usages are \"lspci | grep vga\" or \"lspci | grep eth\" lsusb will show all the usb devices like mice, etc dmidecode listing of system hardware according to the BIOS (so not always reliable) http://www.nongnu.org/dmidecode/ lsmod installed driver modules xdpyinfo /xserver info ls -l /lib/libc-*.so /lib/libc.so* ldd --version version of linux glibc dmesg kernel messages = all the devices the kernel has found like hard disks,cdroms,etc dmesg | grep CPU dmesg | grep mem dmesg | tail shows the last 10 lines of the hardware boot up /etc/modprobe.conf (kernel 2.6) fedora/redhat /etc/modules.conf (kernel 2.4) /etc/conf.modules (or for older systems) disk IO iotop like top but focused on I/O , http://linux.die.net/man/1/iotop network ifconfig -a shows all of the ethernet devices available ls -l /sys/class/net/eth0 apt-get install bcm43xx-fwcutter mkdir -p /lib/hotplug/firmware; cp /lib/firmware/*.fw /lib/hotplug/firmware software inventory and listing dpkg --list dpkg -l | grep foo dpkg --get-selections debian/ubuntu listing of installed software packages pip freeze pip freeze | grep foo python packaging manager listing of packages rpm -qa red hat installed software tail -f /var/log/secure //login logs tail -f /var/log/maillog //mail logs last last -f btmp last logins and then the last bad logins cat /proc/meminfo MemTotal = Total amount of physical RAM, in kilobytes. MemFree The amount of physical RAM, in kilobytes, left unused by the system. note that MemTotal - MemFree should match what free and top show you... Slab The total amount of memory, in kilobytes, used by the kernel to cache data structures for its own use. Mapped The total amount of memory, in kilobytes, which have been used to map devices, files, or libraries using the mmap command. So all of your processes from TOP + Slab + Mapped free total used free shared buffers cached Mem: 262364 78424 183940 0 2412 37756 -/+ buffers/cache: 38256 224108 Swap: 524280 0 524280 used + buffer + cached = 118,592 143,772 top - 20:39:34 up 2:19, 1 user, load average: 0.00, 0.00, 0.00 Tasks: 55 total, 1 running, 54 sleeping, 0 stopped, 0 zombie Cpu(s): 0.0%us, 0.0%sy, 0.0%ni, 99.2%id, 0.0%wa, 0.0%hi, 0.0%si, 0.8%st Mem: 262364k total, 78408k used, 183956k free, 2428k buffers Swap: 524280k total, 0k used, 524280k free, 37756k cached 3899 root 20 0 68112 2984 2316 S 0 1.1 0:00.42 sshd 3575 klog 20 0 5492 2260 420 S 0 0.9 0:00.06 klogd 3903 root 20 0 17544 1796 1328 S 0 0.7 0:00.16 bash 4490 root 20 0 18860 1200 932 R 0 0.5 0:00.04 top 3596 root 20 0 50916 1164 680 S 0 0.4 0:00.00 sshd 3856 Debian-e 20 0 43432 1000 616 S 0 0.4 0:00.00 exim4 1 root 20 0 4020 944 656 S 0 0.4 0:00.36 init 2300 root 16 -4 16832 932 372 S 0 0.4 0:00.46 udevd 3874 root 20 0 18616 860 668 S 0 0.3 0:00.04 cron 3550 syslog 20 0 12296 752 564 S 0 0.3 0:00.10 syslogd 3510 root 20 0 3864 588 492 S 0 0.2 0:00.00 getty 3513 root 20 0 3864 588 492 S 0 0.2 0:00.00 getty 3516 root 20 0 3864 588 492 S 0 0.2 0:00.00 getty 3572 root 20 0 8132 588 476 S 0 0.2 0:00.04 dd 3509 root 20 0 3864 584 492 S 0 0.2 0:00.00 getty 3512 root 20 0 3864 584 492 S 0 0.2 0:00.00 getty 3898 root 20 0 3864 584 492 S 0 0.2 0:00.00 getty 17,996 + 15,832 + 3,552 = 37,380... so still short of 78,424 TOP + Slab + Mapped http://www.hpl.hp.com/personal/Jean_Tourrilhes/Linux/ http://backtrack.offensive-security.com/index.php/HCL:Wireless#Linksys_WPC54G_v3 Linksys WPC54G v3 * Driver : bcm43xx/b43 * Chipset : Broadcom Corporation BCM4318 [AirForce One 54g] 802.11g Wireless LAN Controller (rev 02) * Subsystem: Linksys WPC54G-EU version 3 [Wireless-G Notebook Adapter] Monitor mode currently supported but injection may or may not work with bcm43xx. Apparently a new driver is coming out dubbed as b43 and is only available in either kernel >=2.6.24 and/or wireless-2.6 git. Injection will work after patching b43 via mac80211 stack. bcm43xx driver will soon be deprecated and for this chipset it will not indicate PWR levels with airodump-ng. http://linuxwireless.org/en/users/Drivers/b43 lspci -vnn | grep 14e4 0001:01:01.0 Network controller [0280]: Broadcom Corporation BCM4318 [AirForce One 54g] 802.11g Wireless LAN Controller [14e4:4318] (rev 02) cat /proc/interrupts a file listing of all the interrupt IRQs used by your system e.g. CPU0 0: 2707402473 XT-PIC timer 1: 67 XT-PIC i8042 2: 0 XT-PIC cascade 5: 411342 XT-PIC eth1 8: 1 XT-PIC rtc 10: 1898752 XT-PIC eth0 11: 0 XT-PIC uhci_hcd 12: 58 XT-PIC i8042 14: 5075806 XT-PIC ide0 15: 506 XT-PIC ide1 NMI: 0 ERR: 43 if two devices try to use the same interrupts or memory access address they will be in conflict (won't work) dmidecode in depth example DMI DECODE for how much physical ram you COULD have... dmidecode > hw.txt dump the whole thing to a text file less hw.txt lots of info! System Information Manufacturer: HP Product Name: ProLiant ML310 G5 Version: Not Specified Serial Number: UUID: Wake-up Type: Power Switch SKU Number: Family: ProLiant Processor Information Socket Designation: Proc 1 Type: Central Processor Family: Xeon Manufacturer: Intel ID: 77 06 01 00 FF FB EB BF Signature: Type 0, Family 6, Model 23, Stepping 7 External Clock: 1333 MHz Max Speed: 4800 MHz Current Speed: 2500 MHz Status: Populated, Enabled Upgrade: ZIF Socket L1 Cache Handle: 0x0710 L2 Cache Handle: 0x0720 L3 Cache Handle: 0x0730 Serial Number: Not Specified Asset Tag: Not Specified Part Number: Not Specified Core Count: 4 Core Enabled: 4 Thread Count: 4 Characteristics: 64-bit capable BECAUSE THERE IS SO MUCH INFORMATION PEOPLE USE TRICKS TO GET ONLY A CERTAIN PART dmidecode | perl -ne '$memory += $1 if /&#94;\\t+Size: (\\d+)/ ; END { print \"$memory\\n\" }' dmidecode | perl -ne '$num_procs += 1 if /&#94;\\t+Type: Central Processor/ ; END { print \"$num_procs\\n\"}' decode , biosdecode , and vpddecode // alternates to dmidecode ? DMI SHORT CODES MAKE IT EASIER TO GET A SPECIFIC CHUNK dmidecode --type 0 get info on the bios dmidecode --type 16 # dmidecode 2.9 SMBIOS 2.4 present. Handle 0x1000, DMI type 16, 15 bytes Physical Memory Array Location: System Board Or Motherboard Use: System Memory Error Correction Type: Single-bit ECC Maximum Capacity: 8 GB Error Information Handle: Not Provided Number Of Devices: 4 # Type Short Description 0 BIOS 1 System 2 Base Board 3 Chassis 4 Processor 5 Memory Controller 6 Memory Module 7 Cache 8 Port Connector 9 System Slots 10 On Board Devices 11 OEM Strings 12 System Configuration Options 13 BIOS Language 14 Group Associations 15 System Event Log 16 Physical Memory Array 17 Memory Device 18 32-bit Memory Error 19 Memory Array Mapped Address 20 Memory Device Mapped Address 21 Built-in Pointing Device 22 Portable Battery 23 System Reset 24 Hardware Security 25 System Power Controls 26 Voltage Probe 27 Cooling Device 28 Temperature Probe 29 Electrical Current Probe 30 Out-of-band Remote Access 31 Boot Integrity Services 32 System Boot 33 64-bit Memory Error 34 Management Device 35 Management Device Component 36 Management Device Threshold Data 37 Memory Channel 38 IPMI Device 39 Power Supply","tags":"linux","title":"Systeminfo hardware diagnostic listing ram processor ubuntu version ps top iotop lshw dmidecode"},{"url":"https://blog.john-pfeiffer.com/firewall-iptables-ufw-ssh-https-nat-forwarding-redirect/","text":"iptables is the tool to create a firewall in linux (manipulate the tables provided by the kernel firewall aka the netfilter) https://en.wikipedia.org/wiki/Netfilter which iptables sudo /sbin/iptables --version Most common commands iptables -nvL output the rules in the default \"FILTER\" table (INPUT, OUTPUT) in numeric , verbose, List all rules http://ipset.netfilter.org/iptables.man.html iptables -nvL --line-numbers numeric so no hostname lookups, verbose, List the rules in the chain Interactive commands iptables -D INPUT 5 delete the 5th line don't forget chmod +x firewall-script-filename.sh and /sbin/service iptables save iptables allow ping with a ratelimit iptables -A INPUT -p icmp -m limit --limit 10/second -j ACCEPT iptables -A OUPUT -p icmp -j ACCEPT > allow 10 inbound icmp packets (not tcp nor udp) per second and allow all icmp traffic outbound iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT > echo-request = 8 in numeric iptables -A OUTPUT -p icmp --icmp-type echo-reply -j ACCEPT > Allow outgoing ping replies, echo reply = 0 in numeric Clean out the old iptables - very insecure settings iptables -F iptables --delete-chain iptables -t nat -F iptables -t mangle -F iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT Flush out all of the iptables and delete all of the chains (including the nat and mangle tables) Set the default policies to accept all packets Allow SSH, ping, and Established but block all by default #!/bin/sh iptables -I INPUT 1 -i lo -j ACCEPT iptables -I OUTPUT 1 -o lo -j ACCEPT > Always allow the loopback device iptables -A INPUT -p tcp --dport 22 -j ACCEPT iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT > allow SSH server to accept connections iptables -A INPUT -i eth1 -p tcp --dport 22 -j ACCEPT iptables -A OUTPUT -o eth1 -p tcp --sport 22 -j ACCEPT > ssh server on eth1 on port 22 iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT DROP > default to block all traffic iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT > Accept packets belonging to established and related connections iptables -A INPUT -i eth1 -p icmp --icmp-type echo-request -j ACCEPT > Allow incoming ping requests from eth1 , echo-request = 8 in numeric iptables -A OUTPUT -o eth1 -p icmp --icmp-type echo-reply -j ACCEPT > Allow outgoing ping replies to eth1 , echo reply = 0 in numeric cat /proc/sys/net/ipv4/ip_forward echo 1 > /proc/sys/net/ipv4/ip_forward enable packet forwarding by the kernel, required to enable routing (especially with dual nics) bash script to set iptables during init.d #!/bin/bash # script to set the initial firewall state as very restrictive # chmod +x SCRIPTNAME.sh # cd /etc/init.d # sudo update-rc.d SCRIPTNAME.sh defaults # sudo update-rc.d -f SCRIPTNAME.sh remove # or add it to /etc/rc.d/rc.local (which runs once after all other scripts) # clear any existing firewall iptables -F iptables -X iptables -F -t mangle iptables -F -t nat iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT DROP # Protect against SYN flood attacks (see http://cr.yp.to/syncookies.html). echo 1 > /proc/sys/net/ipv4/tcp_syncookies # Allow loopback iptables -A INPUT -i lo -j ACCEPT iptables -A OUTPUT -o lo -j ACCEPT # Allow DNS queries iptables -A OUTPUT -p udp --dport 53 -m state --state NEW -j ACCEPT iptables -A INPUT -p udp --sport 53 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT # Allow NTP (query time server) iptables -A INPUT -p udp --dport 123 -j ACCEPT iptables -A OUTPUT -p udp --sport 123 -j ACCEPT # Allow SSH on port 22 iptables -A INPUT -p tcp --dport 22 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp --sport 22 -m state --state NEW,ESTABLISHED -j ACCEPT # Allow incoming HTTPS iptables -A INPUT -p tcp -s 0/0 --sport 1024:65535 --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp -s 0/0 --sport 443 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT # Allow outgoing HTTPS (note state for INPUT is only ESTABLISHED) iptables -A OUTPUT -p tcp --dport 443 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A INPUT -p tcp -s 0/0 --sport 443 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT Web Server #!/bin/sh iptables -F iptables --delete-chain iptables -t nat -F iptables -t mangle -F iptables -P INPUT ACCEPT iptables -P FORWARD ACCEPT iptables -P OUTPUT ACCEPT # LOOPBACK 127.0.0.1 iptables -I INPUT 1 -i lo -j ACCEPT iptables -I OUTPUT 1 -o lo -j ACCEPT # SSH iptables -A INPUT -p tcp --dport 22 -j ACCEPT iptables -A OUTPUT -p tcp --sport 22 -j ACCEPT # NTP iptables -A INPUT -p udp --dport 123 -j ACCEPT iptables -A OUTPUT -p udp --sport 123 -j ACCEPT # DNS iptables -A OUTPUT -p udp --dport 53 -m state --state NEW -j ACCEPT iptables -A INPUT -p udp --sport 53 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT # HTTP iptables -A INPUT -p tcp --dport 80 -j ACCEPT iptables -A OUTPUT -p tcp --sport 80 -j ACCEPT # DROP ALL UNDEFINED PACKETS iptables -P INPUT DROP iptables -P FORWARD DROP iptables -P OUTPUT DROP # PING # iptables -A INPUT -p icmp -m limit --limit 6/second -j ACCEPT # iptables -A OUPUT -p icmp -j ACCEPT Web and XMPP Server vi /etc/iptables.rules.xmpp *filter :INPUT DROP [ 3:572 ] :FORWARD DROP [ 0:0 ] :OUTPUT ACCEPT [ 10:1744 ] -A INPUT -i lo -j ACCEPT -A INPUT -p tcp -m tcp ! --tcp-flags FIN,SYN,RST,ACK SYN -m state --state NEW -j DROP -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p tcp -m tcp --dport 22 -m state --state NEW -j ACCEPT -A INPUT -p tcp -m tcp --dport 80 -m state --state NEW -j ACCEPT -A INPUT -p udp -m udp --dport 161 -m state --state NEW -j ACCEPT -A INPUT -p tcp -m tcp --dport 443 -m state --state NEW -j ACCEPT -A INPUT -p tcp -m tcp --dport 5222 -m state --state NEW -j ACCEPT -A INPUT -p tcp -m tcp --dport 5223 -m state --state NEW -j ACCEPT -A INPUT -p udp -m udp --dport 137 -j ACCEPT -A INPUT -p udp -m udp --dport 138 -j ACCEPT -A INPUT -p icmp -m icmp --icmp-type 8 -j ACCEPT -A INPUT -p icmp -m icmp --icmp-type 11 -j ACCEPT -A OUTPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A OUTPUT -o lo -j ACCEPT -A OUTPUT -p tcp -m tcp --dport 22 -m state --state NEW -j ACCEPT -A OUTPUT -p udp -m udp --dport 53 -m state --state NEW -j ACCEPT -A OUTPUT -p udp -m udp --sport 161 -m state --state NEW -j ACCEPT -A OUTPUT -m state --state NEW -j LOG COMMIT iptables-restore < /etc/iptables.rules.xmpp NTP Getting the time and date synchronized through a restricted firewall 1 2 3 4 5 6 7 8 #!/bin/bash # ntpdate port 123 iptables -A INPUT -p udp --dport 123 -j ACCEPT iptables -A OUTPUT -p udp --sport 123 -j ACCEPT # variation where rules are inserted as first items in the Tables # iptables -I INPUT 1 -p udp --dport 123 -j ACCEPT # iptables -I OUTPUT 1 -p udp --sport 123 -j ACCEPT Installing NTP... sudo apt-get install ntp nano /etc/ntp.conf server ntp.ubuntu.com server pool.ntp.org /etc/init.d/ntp restart ls -ahl /etc/cron.daily > verify that ntp is +x executable ntpq -p > verify the service is working > MANUAL = ntpdate pool.ntp.org will now return socket 123 is in use NAT forwarding iptables -A FORWARD -i eth1 -o eth0 -j ACCEPT iptables -A FORWARD -i eth0 -o eth1 -j ACCEPT iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE assuming that eth0 is your WAN and eth1 is your LAN accept all forwarding from eth1 to eth0 and back enable \"nat\" so that packets are addressed properly CIFS iptables -A INPUT -p tcp -s 10.10.10.250 --sport 445 -d 0/0 -j ACCEPT iptables -A OUTPUT -p tcp -s 0/0 --sport 1024:65535 -d 10.10.10.250 --dport 445 -m state --state NEW,ESTABLISHED -j ACCEPT > CIFS has been simplified to just use 445 TCP first... netbios-ns - 137/tcp # NETBIOS Name Service netbios-dgm - 138/tcp # NETBIOS Datagram Service netbios-ssn - 139/tcp # NETBIOS session service microsoft-ds - 445/tcp # if you are using Active Directory Allow AD Lookups LDAP/LDAPS iptables -A INPUT -p tcp -s 0/0 --sport 1024:65535 --dport 389 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp -s 0/0 --sport 389 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp --dport 389 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A INPUT -p tcp -s 0/0 --sport 389 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT iptables -A INPUT -p tcp -s 0/0 --sport 1024:65535 --dport 636 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp -s 0/0 --sport 636 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT iptables -A OUTPUT -p tcp --dport 636 -m state --state NEW,ESTABLISHED -j ACCEPT iptables -A INPUT -p tcp -s 0/0 --sport 636 --dport 1024:65535 -m state --state ESTABLISHED -j ACCEPT DMZ Setup with dual nic ATMOS = 10.10.254.195 LAN (router?) = 10.10.254.1 eth0 = LAN 10.10.254.254 eth1 = WAN 172.16.255.254 eth2 = DMZ 192.168.50.1 Router 172.16.255.1 forward traffic between DMZ and LAN iptables -A FORWARD -i eth0 -o eth2 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -i eth2 -o eth0 -m state --state ESTABLISHED,RELATED -j ACCEPT forward traffic between DMZ and WAN servers SMTP, Mail etc iptables -A FORWARD -i eth2 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT iptables -A FORWARD -i eth1 -o eth2 -m state --state NEW,ESTABLISHED,RELATED -j ACCEPT Route incoming SMTP (port 25 ) traffic to DMZ server 192.168.2.2 iptables -t nat -A PREROUTING -p tcp -i eth1 -d 202.54.1.1 --dport 25 -j DNAT --to-destination 192.168.2.2 Route incoming HTTP (port 80 ) traffic to DMZ server load balancer IP 192.168.2.3 iptables -t nat -A PREROUTING -p tcp -i eth1 -d 202.54.1.1 --dport 80 -j DNAT --to-destination 192.168.2.3 Route incoming HTTPS (port 443 ) traffic to DMZ server reverse load balancer IP 192.168.2.4 iptables -t nat -A PREROUTING -p tcp -i eth1 -d 202.54.1.1 --dport 443 -j DNAT --to-destination 192.168.2.4 End DMZ .. Add other rules Uncomplicated Firewall UFW The uncomplicated firewall is a much simpler way to configure some basic rules and enable the firewall sudo su ufw status verbose ufw allow 22 ufw allow 443 ufw default deny incoming ufw default deny outgoing ufw enable ufw status verbose Allowing 22 (SSH) and 443 (HTTPS) and denying all other incoming and outgoing traffic ufw delete allow 443 ufw show raw ufw disable removing a rule is as simple as prefixing the allow or deny command with delete disabling the firewall allows all traffic Most unfortunately there are some basic gaps that make it not very production ready (i.e. if you know what you are doing just keep using iptables) 1. ping, also known as icmp, packets (even just outbound) have to be handled in a very complex way, really not much better than iptables 1. established connection traffic is not just easily allowed 1. attempting to do something more complex very quickly requires very complex commands including just using iptables (lolwut) 1. iptables -nvL becomes almost unreadable with the extra layer https://wiki.ubuntu.com/UncomplicatedFirewall","tags":"linux","title":"firewall iptables ufw ssh https nat forwarding redirect"}]}